{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":""},{"location":"index.html#ep-stats","title":"Ep-Stats","text":"<p>Statistical package for experimentation platform.</p> <p>It provides a general python package and REST API that can be used to evaluate any metric in AB test experiment.</p>"},{"location":"index.html#features","title":"Features","text":"<ul> <li>Robust two-tailed t-test implementation with multiple p-value corrections and delta-methods applied.</li> <li>Sequential evaluations allowing to stop experiments early.</li> <li>Connect it to any data source to either get pre-aggregated or per randomization unit data.</li> <li>Simple expression language to define arbitrary metrics.</li> <li>REST API to integrate it as a service in experimentation portal with score cards.</li> </ul> <p>We encourage all readers to get familiar with basic EP Principles and then follow Quick Start.</p>"},{"location":"index.html#architecture","title":"Architecture","text":"<p>In regular experimentation platform, client data and telemetry have to pass through several components before experimenters can see results in metrics in scorecards. Ep-stats solves the statistical part of the pipeline as is described on following image.</p> <p></p> <p>Client data and telemetry collection are specific to the company, we do not strive to provide any support for this part. Aggregator is optional part between raw data and ep-stats that can help to unify and pre-aggregate data consumed by ep-stats. Scorecards represent user interface in some kind of experimentation portal or knowledge base that lists experiments and displays scorecards with experiment results and statistics.</p> <p>Ep-stats offers following components:</p> <ol> <li>DAO (data access object) interfacing underlying data source with a way how to compile required metric data into SQL or anything else in use.</li> <li>Stats computing experiment evaluation with statistics.</li> <li>REST API a web app that makes it easy to integrate experiment evaluations in scorecards.</li> </ol>"},{"location":"index.html#known-limitations-and-suggestion-for-future-work","title":"Known Limitations and Suggestion for Future Work","text":"<p>Field of online experimentation is developing as well as data, metrics, methods, statistics. We strive to provide correct experiment evaluation. Its development takes time.</p> <p>Current main and known limitations.</p> <ol> <li>Metrics that are not based on (randomization) unit type (e.g. Views per User if session is our unit type) require application of delta method and bootstrapping<sup>1</sup>. This is not implemented yet.</li> <li>Drill-down into dimensions is not implemented yet.</li> <li>Experiment data are aggregated as whole, cumulative evaluation or data for timeline graphs are not yet implemented.</li> </ol>"},{"location":"index.html#origin","title":"Origin","text":"<p>Ep-stats originated as a part of experimentation platform implementation in Avast. While there are many books on experimentation and statistics, there are few or none good implementations of it. We aim to fill in this gap between theory and practice by open-sourcing ep-stats package.</p> <p>We have been using EP with this implementation of ep-stats to run and evaluate hundreds of experiments in Avast. We will be adding new stuff here as we improve it and test it in Avast.</p>"},{"location":"index.html#inspiration","title":"Inspiration","text":"<p>Software engineering practices of this package have been heavily inspired by marvelous calmcode.io site managed by Vincent D. Warmerdam.</p> <ol> <li> <p>A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas \u21a9</p> </li> </ol>"},{"location":"architecture.html","title":"Architecture","text":"<p>Ep-Stats provides package and REST app that can be used in any experimentation portal. We do following main abstractions to allow this.</p>"},{"location":"architecture.html#experimentation-portal","title":"Experimentation Portal","text":"<p>While we built Experimentation Portal in Avast, we are far from being able to open source it because it contains too much of Avast proprietary information. We abstracted this by having json rest API part of Ep-Stats. Json request contains definition of experiment metrics to evaluate which is later translated into implementation-specific query to retrieve goals aggregated per variant.</p> <p></p>"},{"location":"architecture.html#data-and-goals","title":"Data and Goals","text":"<p>Event data, goals, metrics, database, storage, access those are all implementation-specific and heavily proprietary. Ep-Stats does not require any particular database or data format. Ep-Stats provides abstract experiment definition including definitions of goals required for experiment evaluation. It is left to implementors to transform this abstract experiment definition to SQL queries, Spark queries, or anything that has been in place. It is also left to implementators if they support all Ep-Stats features in these queries e.g. filtering results by domains, having goals with parameters, etc.</p> <p>Ep-Stats only requires goals aggregated per variant in <code>Experiment.evaluate_agg</code> method input. See this example for details.</p> <p>Ep-Stats abstracts access to date using proprietary implementations of <code>Dao</code> and <code>DaoFactory</code> classes that are passed via dependency injection into the rest app.</p>"},{"location":"architecture.html#open-vs-proprietary","title":"Open vs. Proprietary","text":"<p>We are using ClickHouse database in Avast to store experiment data. We also deploy Ep-Stats into Kubernetes and we use Sentry for alerts. This is all proprietary and left to implementors of this project to do by their own means and standards.</p> <p></p>"},{"location":"architecture.html#program-flow","title":"Program Flow","text":"<p>Program flow is simple and linear. There is a cycle for each variant that compares all variant metrics against values in the control variant in one vectorized step making the numpy code little harder to follow, but speeding up evaluation of experiments with many metrics significantly.</p> <p></p>"},{"location":"architecture.html#testing","title":"Testing","text":"<p>We prepared many artificial testing data we use in different kinds of unit tests. Testing data contains experiment metric evaluation including complete statistical output done manually in Excel. We use this to assert evaluation results computed by Ep-Stats against.</p> <p>See Test Data for details. We unit test on following levels:</p> <ol> <li>Simulating experiment evaluation from Python <code>Experiment</code> API.</li> <li>Evaluating experiments as called via json rest API.</li> <li>Unit testing all Python code snippets in docstrings ensuring code examples works.</li> <li>In Avast proprietary implementation, we also simulated ClickHouse experiment data using SQLite database to be able to unit test our proprietary translation from experiment definition into SQL code.</li> </ol> <p></p>"},{"location":"architecture.html#cicd","title":"CI/CD","text":"<p>Development workflow is started with simple <code>make install-dev</code> call that sets development environment up. We develop Ep-Stats agains unit tests. Rest app could also be launched locally. We employ pre-commit hooks to fix and check our Python code style.</p> <p>CI/CD is done using GitHub actions where we run all unit tests, deploy documentation, and release to PyPI.</p> <p></p>"},{"location":"principles.html","title":"Basic Principles","text":"<p>We describe basic principles of online controlled experiments and a way how we implement them as events, goals, metrics, and exposures. This section also gives some implementation advice how to effectively set telemetry in the app to collect events.</p>"},{"location":"principles.html#online-controlled-experiments","title":"Online Controlled Experiments","text":"<p>When executed properly, online controlled experiments (OCE) are a way to establish causal relationship between some introduced change (in treatment variant) and its impact on some metric of interest.</p> <p>OCEs randomly distribute Randomization Units (RU) (e.g. users) into multiple groups called variants. Randomization units are also known as experimental units or observations in experimentation literature. We will be using simple term unit to refer to randomization unit in this documentation. We compare the Control variant where there is no treatment or change present with one or more Treatment variant(s) where there is treatment or change present. We are interested in which variant is performing how compared to the control variant. Units' interactions are recorded during the period of the experiment and later analyzed statistically during the experiment and at the end of it. We are interested in:</p> <ol> <li>Determining the presence of a difference in one or more metrics between the control and treatment variants</li> <li>Determining the size of the difference</li> <li>Determining if we can reason that measured difference was caused by the tested change or by pure chance</li> <li>If we are confident (up to some level) that the measured difference was caused by the tested change we consider the diff to be Statistically Significant (with some confidence level).</li> </ol> <p></p> <p>Source: Ron Kohavi, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing.</p> <p>Colloquially put, to evaluate some metric in the experiment, we need a way how to calculate the metric from goals that we collect in form of events from units exposed to the experiment.</p> <p>We will continue by describing these terms and explaining how they play together.</p>"},{"location":"principles.html#exposures","title":"Exposures","text":"<p>We distribute units randomly (independently and identically \u2013 IID<sup>1</sup>) into experiment variants. We can imagine that we assign units to some experiment and variant. We say that the unit (e.g. user, license, ...) has been exposed to the experiment E and variant V at the moment when the unit experienced the control or treatment variants.</p> <p>For example, we expose the unit to the green variant of some experiment when the unit sees the green screen. Or we expose some other unit to the yellow variant of the same experiment when the unit sees the yellow screen.</p> <p>The concept of explicit exposure is crucial in ep-stats. It decouples assignment of the unit into some experiment and variant from rather explicit exposure. The assignment has no impact on the unit's customer experience, it is merely just a technical thing. While the moment of exposure has direct impact on the unit's customer experience because the unit sees a green or yellow screen.</p> <p>The following graph illustrates the importance of the exposure event. It is the moment we start recording events attributed with the exposed unit (<code>unit-1</code> and later <code>unit-2</code> in this case) per experiment and variant.</p> <p></p> <p>By making the exposure explicit, we later limit our analysis only to those units that experienced the change in the experiment. This drastically improves sensitivity of the analysis (we are able to detect smaller differences or the same diffs but with lower unit count).</p> <p>Since we know the explicit moment of exposure, we do not need to add any artificial test information to events we collect from products. We simply start recording all events from all units that have already been exposed to experiment E and its variant V. One unit can certainly be exposed into an arbitrary number of experiments at the same time. We record all events from such unit and attribute them to all experiments and variants the unit has been exposed to.</p>"},{"location":"principles.html#events-and-goals","title":"Events and Goals","text":"<p>After the unit has been exposed to the experiment (control or treatment variant), we are interested in unit's events such as clicks, conversions etc. These events are called events in ep-stats. ep-stats listens to the stream of these events coming from units and we attribute (record) events of interest as goals to some experiment E and variant V if that event happened on some unit that has been exposed to the experiment E and variant V before. ep-stats uses goals to compute all metrics.</p> <p>The exposure is a (special) goal in the ep-stats as well.</p>"},{"location":"principles.html#goal-attribution","title":"Goal Attribution","text":"<p>We attribute goals to experiments and variants based on units exposures. This means that every event that should make a goal in ep-stats must reference some unit. Without that, ep-stats is not able to recognize what exposure to look up to determine which experiment(s) and variant(s) the goal should be attributed to.</p> <p>See Supported Unit Types.</p>"},{"location":"principles.html#metrics","title":"Metrics","text":"<p>We calculate metrics from goals. We calculate metrics per experiment variant. Metrics are usually some average number (or value, e.g., value of conversion in USD) of positive goals over number of exposures. For example, the metric Average Bookings is defined as USD value of all conversions of all users exposed to the experiment divided by number of users exposed to the experiment.</p> <p>We define metrics from goals in form of nominator and denominator. For example, we define metric Average Bookings as <code>value(test_unit_type.unit.conversion) / count(test_unit_type.global.exposure)</code> where</p> <ul> <li><code>value</code> determines we need the value of goals recorded, e.g., USD bookings in this case (<code>count</code> then means number of <code>conversion</code> goals)</li> <li><code>test_unit_type</code> is a type of unit</li> <li><code>unit</code>, <code>global</code> are types of aggregation</li> <li><code>conversion</code>, <code>exposure</code> are goals</li> </ul> <p>See Aggregation for details about how we process data and about specifics of metric definitions in ep-stats.</p>"},{"location":"principles.html#some-potential-caveats","title":"Some Potential Caveats","text":"<p>While ep-stats handles most of the known problems transparently for the experimenter, it is important to be mindful at least about the following potential problems.</p> <ol> <li>Due to randomization of units, every metric is a random variable. ep-stats helps to provide \"always valid\" metric values and statistics to allow you to \"always make a good decision\". This relates to a problem of \"peeking at results\" which we solve using sequential analysis.</li> <li>In some metrics, a unit is not the same as analysis unit. This violates IID assumption and requires ep-stats to perform delta method.</li> </ol> <p>We describe these problems in detail in Statistics.</p> <ol> <li> <p>Independent and identically distributed random variables - IID \u21a9</p> </li> </ol>"},{"location":"resources.html","title":"Resources","text":"<p>Following are selected lists of publications that influenced us the most in making ep-stats design decisions and development. The most important being Ron Kohavi, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing that we used for inspiration on soft topics like how to build experimentation culture and protocol and as a checklist to make sure we haven't forgot anything in implementation.</p> <ul> <li>Ron Kohavi, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing</li> <li>Georgi Z. Georgiev, Statistical Methods in Online A/B Testing: Statistics for data-driven business decisions and risk management in e-commerce</li> <li>ExP Experimentation Platform</li> <li>A. Fabijan et al., Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale</li> <li>R. Kohavi, Sample Ratio Mismatch</li> <li>A. Deng et al., Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data</li> <li>Yandex, Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics</li> <li>R. Kohavi et al., Seven Rules of Thumb for Web Site Experiments</li> <li>H. Hohnhold et al., Focusing on the Long-term: It's Good for Users and Business</li> <li>Yandex, Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics</li> <li>Various authors, Top Challenges from the first Practical Online Controlled Experiments Summit</li> <li>David L. DeMents, K. K. Gordon Lan, Interim analysis: The alpha spending function approach</li> <li>P. C. O\u2019Brien, T.R. Fleming - A Multiple Testing Procedure for Clinical Trials</li> <li>A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas</li> </ul>"},{"location":"api/check.html","title":"Check","text":"<p>Perform data quality check that accompanies metric evaluation in the experiment.</p> <p>See Data Quality Checks for details about data quality checks and <code>Evaluation</code> for description of output.</p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>class Check:\n    \"\"\"\n    Perform data quality check that accompanies metric evaluation in the experiment.\n\n    See [Data Quality Checks](../stats/basics.md#data-quality-checks) for details about\n    data quality checks and [`Evaluation`][epstats.toolkit.experiment.Evaluation] for description of output.\n    \"\"\"\n\n    def __init__(self, id: int, name: str, denominator: str, **unused_kwargs):\n        self.id = id\n        self.name = name\n        self.denominator = denominator\n        self._denominator_parser = Parser(denominator, denominator)\n        self._goals = self._denominator_parser.get_goals()\n\n    def get_goals(self) -&gt; List:\n        \"\"\"\n        List of all goals needed to evaluate the check in the experiment.\n\n        Returns:\n            list of parsed structured goals\n        \"\"\"\n        return self._goals\n\n    def evaluate_agg(\n        self, goals: pd.DataFrame, default_exp_variant_id: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Evaluate this check from pre-aggregated goals.\n\n        Arguments:\n            goals: one row per experiment variant\n            default_exp_variant_id: default variant\n\n        See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for details\n        on `goals` at input.\n\n        Returns:\n            `checks` dataframe with columns:\n\n        `checks` dataframe with columns:\n\n        1. `timestamp` - timestamp of evaluation\n        1. `exp_id` - experiment id\n        1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n        1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`,\n        `test_stat`, `confidence_level`\n        1. `value` - value of the variable\n        \"\"\"\n        raise NotImplementedError()\n\n    def evaluate_by_unit(\n        self, goals: pd.DataFrame, default_exp_variant_id: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Evaluate this check from goals aggregated by unit.\n\n        Arguments:\n            goals: ne row per experiment variant\n            default_exp_variant_id: default variant\n\n        See [`Experiment.evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for details\n        on `goals` at input.\n\n        Returns:\n            `checks` dataframe with columns:\n\n        `checks` dataframe with columns:\n\n        1. `timestamp` - timestamp of evaluation\n        1. `exp_id` - experiment id\n        1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n        1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`,\n        `test_stat`, `confidence_level`\n        1. `value` - value of the variable\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.Check.evaluate_agg","title":"<code>evaluate_agg(goals, default_exp_variant_id)</code>","text":"<p>Evaluate this check from pre-aggregated goals.</p> <p>Parameters:</p> Name Type Description Default <code>goals</code> <code>DataFrame</code> <p>one row per experiment variant</p> required <code>default_exp_variant_id</code> <code>str</code> <p>default variant</p> required <p>See <code>Experiment.evaluate_agg</code> for details on <code>goals</code> at input.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>checks</code> dataframe with columns:</p> <p><code>checks</code> dataframe with columns:</p> <ol> <li><code>timestamp</code> - timestamp of evaluation</li> <li><code>exp_id</code> - experiment id</li> <li><code>check_id</code> - check id as in <code>Experiment</code> definition</li> <li><code>variable_id</code> - name of the variable in check evaluation, SRM check has following variables <code>p_value</code>, <code>test_stat</code>, <code>confidence_level</code></li> <li><code>value</code> - value of the variable</li> </ol> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def evaluate_agg(\n    self, goals: pd.DataFrame, default_exp_variant_id: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Evaluate this check from pre-aggregated goals.\n\n    Arguments:\n        goals: one row per experiment variant\n        default_exp_variant_id: default variant\n\n    See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for details\n    on `goals` at input.\n\n    Returns:\n        `checks` dataframe with columns:\n\n    `checks` dataframe with columns:\n\n    1. `timestamp` - timestamp of evaluation\n    1. `exp_id` - experiment id\n    1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n    1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`,\n    `test_stat`, `confidence_level`\n    1. `value` - value of the variable\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.Check.evaluate_by_unit","title":"<code>evaluate_by_unit(goals, default_exp_variant_id)</code>","text":"<p>Evaluate this check from goals aggregated by unit.</p> <p>Parameters:</p> Name Type Description Default <code>goals</code> <code>DataFrame</code> <p>ne row per experiment variant</p> required <code>default_exp_variant_id</code> <code>str</code> <p>default variant</p> required <p>See <code>Experiment.evaluate_by_unit</code> for details on <code>goals</code> at input.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>checks</code> dataframe with columns:</p> <p><code>checks</code> dataframe with columns:</p> <ol> <li><code>timestamp</code> - timestamp of evaluation</li> <li><code>exp_id</code> - experiment id</li> <li><code>check_id</code> - check id as in <code>Experiment</code> definition</li> <li><code>variable_id</code> - name of the variable in check evaluation, SRM check has following variables <code>p_value</code>, <code>test_stat</code>, <code>confidence_level</code></li> <li><code>value</code> - value of the variable</li> </ol> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def evaluate_by_unit(\n    self, goals: pd.DataFrame, default_exp_variant_id: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Evaluate this check from goals aggregated by unit.\n\n    Arguments:\n        goals: ne row per experiment variant\n        default_exp_variant_id: default variant\n\n    See [`Experiment.evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for details\n    on `goals` at input.\n\n    Returns:\n        `checks` dataframe with columns:\n\n    `checks` dataframe with columns:\n\n    1. `timestamp` - timestamp of evaluation\n    1. `exp_id` - experiment id\n    1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n    1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`,\n    `test_stat`, `confidence_level`\n    1. `value` - value of the variable\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.Check.get_goals","title":"<code>get_goals()</code>","text":"<p>List of all goals needed to evaluate the check in the experiment.</p> <p>Returns:</p> Type Description <code>List</code> <p>list of parsed structured goals</p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def get_goals(self) -&gt; List:\n    \"\"\"\n    List of all goals needed to evaluate the check in the experiment.\n\n    Returns:\n        list of parsed structured goals\n    \"\"\"\n    return self._goals\n</code></pre>"},{"location":"api/check.html#srm-check","title":"SRM Check","text":"<p>               Bases: <code>Check</code></p> <p>Sample ratio mismatch check checking randomization of units to variants using Chi-square test.</p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>class SrmCheck(Check):\n    \"\"\"\n    [Sample ratio mismatch check](../stats/basics.md#sample-ratio-mismatch-check) checking randomization\n    of units to variants using [Chi-square test](https://en.wikipedia.org/wiki/Chi-squared_test).\n    \"\"\"\n\n    def __init__(\n        self,\n        id: int,\n        name: str,\n        denominator: str,\n        confidence_level: float = 0.999,\n        **unused_kwargs,\n    ):\n        \"\"\"\n        Constructor of the SRM check.\n\n        Arguments:\n            id: check (order) id\n            name: check name\n            denominator: values to check\n            confidence_level: confidence level of the statistical test\n\n        Usage:\n        ```python\n        SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')\n        ```\n        \"\"\"\n        super().__init__(id, name, denominator)\n        self.confidence_level = confidence_level\n\n    def evaluate_agg(\n        self, goals: pd.DataFrame, default_exp_variant_id: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        See [`Check.evaluate_agg`][epstats.toolkit.check.Check.evaluate_agg].\n        \"\"\"\n        # input example:\n        # test - srm, a, global.exposure, 10000, 10010, 10010, 0.0, 0.0\n        # test - srm, b, global.exposure, 10010, 10010, 10010, 0.0, 0.0\n        # test - srm, c, global.exposure, 10040, 10040, 10040, 0.0, 0.0\n\n        # output example:\n        # test - srm, 1, SRM, p_value, 0.20438\n        # test - srm, 1, SRM, test_stat, 3.17552\n        # test - srm, 1, SRM, confidence_level, 0.999\n\n        # prepare data - we only need exposures\n        exposures, _, _ = self._denominator_parser.evaluate_agg(goals)\n\n        # chi-square test\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            # we fill in zeros, when goal data are missing for some variant.\n            # There could be division by zero here which is expected as we return\n            # nan or inf values to the caller.\n            stat, pval = chisquare(exposures)\n\n        r = pd.DataFrame(\n            {\n                \"check_id\": [self.id, self.id, self.id],\n                \"check_name\": [self.name, self.name, self.name],\n                \"variable_id\": [\"p_value\", \"test_stat\", \"confidence_level\"],\n                \"value\": [pval, stat, self.confidence_level],\n            }\n        )\n        return r\n\n    def evaluate_by_unit(\n        self, goals: pd.DataFrame, default_exp_variant_id: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        See [`Check.evaluate_by_unit`][epstats.toolkit.check.Check.evaluate_by_unit].\n        \"\"\"\n\n        exposures, _, _ = self._denominator_parser.evaluate_by_unit(goals)\n\n        # chi-square test\n        stat, pval = chisquare(exposures)\n\n        r = pd.DataFrame(\n            {\n                \"check_id\": [self.id, self.id, self.id],\n                \"check_name\": [self.name, self.name, self.name],\n                \"variable_id\": [\"p_value\", \"test_stat\", \"confidence_level\"],\n                \"value\": [pval, stat, self.confidence_level],\n            }\n        )\n        return r\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.SrmCheck.__init__","title":"<code>__init__(id, name, denominator, confidence_level=0.999, **unused_kwargs)</code>","text":"<p>Constructor of the SRM check.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>check (order) id</p> required <code>name</code> <code>str</code> <p>check name</p> required <code>denominator</code> <code>str</code> <p>values to check</p> required <code>confidence_level</code> <code>float</code> <p>confidence level of the statistical test</p> <code>0.999</code> <p>Usage: <pre><code>SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')\n</code></pre></p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def __init__(\n    self,\n    id: int,\n    name: str,\n    denominator: str,\n    confidence_level: float = 0.999,\n    **unused_kwargs,\n):\n    \"\"\"\n    Constructor of the SRM check.\n\n    Arguments:\n        id: check (order) id\n        name: check name\n        denominator: values to check\n        confidence_level: confidence level of the statistical test\n\n    Usage:\n    ```python\n    SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')\n    ```\n    \"\"\"\n    super().__init__(id, name, denominator)\n    self.confidence_level = confidence_level\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.SrmCheck.evaluate_agg","title":"<code>evaluate_agg(goals, default_exp_variant_id)</code>","text":"<p>See <code>Check.evaluate_agg</code>.</p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def evaluate_agg(\n    self, goals: pd.DataFrame, default_exp_variant_id: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    See [`Check.evaluate_agg`][epstats.toolkit.check.Check.evaluate_agg].\n    \"\"\"\n    # input example:\n    # test - srm, a, global.exposure, 10000, 10010, 10010, 0.0, 0.0\n    # test - srm, b, global.exposure, 10010, 10010, 10010, 0.0, 0.0\n    # test - srm, c, global.exposure, 10040, 10040, 10040, 0.0, 0.0\n\n    # output example:\n    # test - srm, 1, SRM, p_value, 0.20438\n    # test - srm, 1, SRM, test_stat, 3.17552\n    # test - srm, 1, SRM, confidence_level, 0.999\n\n    # prepare data - we only need exposures\n    exposures, _, _ = self._denominator_parser.evaluate_agg(goals)\n\n    # chi-square test\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        # we fill in zeros, when goal data are missing for some variant.\n        # There could be division by zero here which is expected as we return\n        # nan or inf values to the caller.\n        stat, pval = chisquare(exposures)\n\n    r = pd.DataFrame(\n        {\n            \"check_id\": [self.id, self.id, self.id],\n            \"check_name\": [self.name, self.name, self.name],\n            \"variable_id\": [\"p_value\", \"test_stat\", \"confidence_level\"],\n            \"value\": [pval, stat, self.confidence_level],\n        }\n    )\n    return r\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.SrmCheck.evaluate_by_unit","title":"<code>evaluate_by_unit(goals, default_exp_variant_id)</code>","text":"<p>See <code>Check.evaluate_by_unit</code>.</p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def evaluate_by_unit(\n    self, goals: pd.DataFrame, default_exp_variant_id: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    See [`Check.evaluate_by_unit`][epstats.toolkit.check.Check.evaluate_by_unit].\n    \"\"\"\n\n    exposures, _, _ = self._denominator_parser.evaluate_by_unit(goals)\n\n    # chi-square test\n    stat, pval = chisquare(exposures)\n\n    r = pd.DataFrame(\n        {\n            \"check_id\": [self.id, self.id, self.id],\n            \"check_name\": [self.name, self.name, self.name],\n            \"variable_id\": [\"p_value\", \"test_stat\", \"confidence_level\"],\n            \"value\": [pval, stat, self.confidence_level],\n        }\n    )\n    return r\n</code></pre>"},{"location":"api/check.html#simple-srm-check","title":"Simple SRM Check","text":"<p>               Bases: <code>SrmCheck</code></p> <p>Simplified definition of SRM check.</p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>class SimpleSrmCheck(SrmCheck):\n    \"\"\"Simplified definition of SRM check.\"\"\"\n\n    def __init__(\n        self,\n        id: int,\n        name: str,\n        denominator: str,\n        confidence_level: float = 0.999,\n        unit_type: str = \"test_unit_type\",\n    ):\n        \"\"\"\n        Constructor of the simplified SRM check.\n\n        It modifies parameter denominator in a way that it is in line with general SRM Check definition. It adds all\n        the niceties necessary for proper SrmCheck format. Finaly it calls constructor of the parent SrmCheck class.\n\n        Arguments:\n            id: check (order) id\n            name: check name\n            denominator: value (column) of the denominator\n            confidence_level: confidence level of the statistical test\n            unit_type: unit type\n\n        Usage:\n        ```python\n        SimpleSrmCheck(1, 'SRM', 'exposures')\n        ```\n        \"\"\"\n        agg_type = \"global\"\n        den = \"value\" + \"(\" + unit_type + \".\" + agg_type + \".\" + denominator + \")\"\n        super().__init__(id, name, den, confidence_level)\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.SimpleSrmCheck.__init__","title":"<code>__init__(id, name, denominator, confidence_level=0.999, unit_type='test_unit_type')</code>","text":"<p>Constructor of the simplified SRM check.</p> <p>It modifies parameter denominator in a way that it is in line with general SRM Check definition. It adds all the niceties necessary for proper SrmCheck format. Finaly it calls constructor of the parent SrmCheck class.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>check (order) id</p> required <code>name</code> <code>str</code> <p>check name</p> required <code>denominator</code> <code>str</code> <p>value (column) of the denominator</p> required <code>confidence_level</code> <code>float</code> <p>confidence level of the statistical test</p> <code>0.999</code> <code>unit_type</code> <code>str</code> <p>unit type</p> <code>'test_unit_type'</code> <p>Usage: <pre><code>SimpleSrmCheck(1, 'SRM', 'exposures')\n</code></pre></p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def __init__(\n    self,\n    id: int,\n    name: str,\n    denominator: str,\n    confidence_level: float = 0.999,\n    unit_type: str = \"test_unit_type\",\n):\n    \"\"\"\n    Constructor of the simplified SRM check.\n\n    It modifies parameter denominator in a way that it is in line with general SRM Check definition. It adds all\n    the niceties necessary for proper SrmCheck format. Finaly it calls constructor of the parent SrmCheck class.\n\n    Arguments:\n        id: check (order) id\n        name: check name\n        denominator: value (column) of the denominator\n        confidence_level: confidence level of the statistical test\n        unit_type: unit type\n\n    Usage:\n    ```python\n    SimpleSrmCheck(1, 'SRM', 'exposures')\n    ```\n    \"\"\"\n    agg_type = \"global\"\n    den = \"value\" + \"(\" + unit_type + \".\" + agg_type + \".\" + denominator + \")\"\n    super().__init__(id, name, den, confidence_level)\n</code></pre>"},{"location":"api/check.html#sumratio-check","title":"SumRatio Check","text":"<p>               Bases: <code>Check</code></p> <p>Computes the ratio of <code>nominator</code>, <code>denominator</code> goal counts summed across all variants.</p> <p>Max ratio check.</p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>class SumRatioCheck(Check):\n    \"\"\"\n    Computes the ratio of `nominator`, `denominator` goal counts summed across all variants.\n\n    [Max ratio check](../stats/basics.md#sum-ratio-check).\n    \"\"\"\n\n    def __init__(\n        self,\n        id: int,\n        name: str,\n        nominator: str,\n        denominator: str,\n        max_sum_ratio: float = 0.01,\n        confidence_level: float = 0.999,\n        **unused_kwargs,\n    ):\n        \"\"\"\n        Constructor of the check.\n\n        Arguments:\n            id: check (order) id\n            name: check name\n            nominator:  goal in the ratio numerator\n            denominator: goal in the ratio denominitaor\n            max_sum_ratio: maximum allowed sum_ratio value\n            confidence_level: confidence level of the statistical test\n\n        Usage:\n        ```python\n        SumRatioCheck(\n            1,\n            \"SumRatio\",\n            \"count(test_unit_type.global.inconsistent_exposure)\",\n            \"count(test_unit_type.global.exposure)\"\n        )\n        ```\n        \"\"\"\n        super().__init__(id, name, denominator)\n        self.max_sum_ratio = max_sum_ratio\n        self.confidence_level = confidence_level\n        self.nominator = nominator\n        self._nominator_parser = Parser(nominator, nominator)\n        self._goals = self._goals.union(self._nominator_parser.get_goals())\n\n    def evaluate_agg(\n        self, goals: pd.DataFrame, default_exp_variant_id: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        See [`Check.evaluate_agg`][epstats.toolkit.check.Check.evaluate_agg].\n        \"\"\"\n\n        denominator_counts, _, _ = self._denominator_parser.evaluate_agg(goals)\n        nominator_counts, _, _ = self._nominator_parser.evaluate_agg(goals)\n\n        # chi-square test\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            sum_ratio = nominator_counts.sum() / (\n                denominator_counts.sum() + nominator_counts.sum()\n            )\n\n            stat, pval = chisquare(\n                [\n                    denominator_counts.sum(),\n                    np.abs(denominator_counts.sum() - nominator_counts.sum()),\n                ]\n            )\n\n        r = pd.DataFrame(\n            {\n                \"check_id\": self.id,\n                \"check_name\": self.name,\n                \"variable_id\": [\n                    \"sum_ratio\",\n                    \"max_sum_ratio\",\n                    \"p_value\",\n                    \"test_stat\",\n                    \"confidence_level\",\n                ],\n                \"value\": [\n                    sum_ratio,\n                    self.max_sum_ratio,\n                    pval,\n                    stat,\n                    self.confidence_level,\n                ],\n            }\n        )\n        return r\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.SumRatioCheck.__init__","title":"<code>__init__(id, name, nominator, denominator, max_sum_ratio=0.01, confidence_level=0.999, **unused_kwargs)</code>","text":"<p>Constructor of the check.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>check (order) id</p> required <code>name</code> <code>str</code> <p>check name</p> required <code>nominator</code> <code>str</code> <p>goal in the ratio numerator</p> required <code>denominator</code> <code>str</code> <p>goal in the ratio denominitaor</p> required <code>max_sum_ratio</code> <code>float</code> <p>maximum allowed sum_ratio value</p> <code>0.01</code> <code>confidence_level</code> <code>float</code> <p>confidence level of the statistical test</p> <code>0.999</code> <p>Usage: <pre><code>SumRatioCheck(\n    1,\n    \"SumRatio\",\n    \"count(test_unit_type.global.inconsistent_exposure)\",\n    \"count(test_unit_type.global.exposure)\"\n)\n</code></pre></p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def __init__(\n    self,\n    id: int,\n    name: str,\n    nominator: str,\n    denominator: str,\n    max_sum_ratio: float = 0.01,\n    confidence_level: float = 0.999,\n    **unused_kwargs,\n):\n    \"\"\"\n    Constructor of the check.\n\n    Arguments:\n        id: check (order) id\n        name: check name\n        nominator:  goal in the ratio numerator\n        denominator: goal in the ratio denominitaor\n        max_sum_ratio: maximum allowed sum_ratio value\n        confidence_level: confidence level of the statistical test\n\n    Usage:\n    ```python\n    SumRatioCheck(\n        1,\n        \"SumRatio\",\n        \"count(test_unit_type.global.inconsistent_exposure)\",\n        \"count(test_unit_type.global.exposure)\"\n    )\n    ```\n    \"\"\"\n    super().__init__(id, name, denominator)\n    self.max_sum_ratio = max_sum_ratio\n    self.confidence_level = confidence_level\n    self.nominator = nominator\n    self._nominator_parser = Parser(nominator, nominator)\n    self._goals = self._goals.union(self._nominator_parser.get_goals())\n</code></pre>"},{"location":"api/check.html#epstats.toolkit.SumRatioCheck.evaluate_agg","title":"<code>evaluate_agg(goals, default_exp_variant_id)</code>","text":"<p>See <code>Check.evaluate_agg</code>.</p> Source code in <code>src/epstats/toolkit/check.py</code> <pre><code>def evaluate_agg(\n    self, goals: pd.DataFrame, default_exp_variant_id: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    See [`Check.evaluate_agg`][epstats.toolkit.check.Check.evaluate_agg].\n    \"\"\"\n\n    denominator_counts, _, _ = self._denominator_parser.evaluate_agg(goals)\n    nominator_counts, _, _ = self._nominator_parser.evaluate_agg(goals)\n\n    # chi-square test\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        sum_ratio = nominator_counts.sum() / (\n            denominator_counts.sum() + nominator_counts.sum()\n        )\n\n        stat, pval = chisquare(\n            [\n                denominator_counts.sum(),\n                np.abs(denominator_counts.sum() - nominator_counts.sum()),\n            ]\n        )\n\n    r = pd.DataFrame(\n        {\n            \"check_id\": self.id,\n            \"check_name\": self.name,\n            \"variable_id\": [\n                \"sum_ratio\",\n                \"max_sum_ratio\",\n                \"p_value\",\n                \"test_stat\",\n                \"confidence_level\",\n            ],\n            \"value\": [\n                sum_ratio,\n                self.max_sum_ratio,\n                pval,\n                stat,\n                self.confidence_level,\n            ],\n        }\n    )\n    return r\n</code></pre>"},{"location":"api/dao.html","title":"Dao","text":"<p>Abstract class interfacing any kind of underlying data source.</p> Source code in <code>src/epstats/toolkit/dao.py</code> <pre><code>class Dao:\n    \"\"\"\n    Abstract class interfacing any kind of underlying data source.\n    \"\"\"\n\n    def get_unit_goals(self, experiment: Experiment) -&gt; pd.DataFrame:\n        \"\"\"\n        Get goals data pre-aggregated by `exp_variant_id`, `unit_type`, `agg_type`, `goal`,\n        `unit_id` and any dimension columns (in case of dimensional metrics).\n\n        See [`Experiment.evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for column\n        descriptions and example result.\n        \"\"\"\n        pass\n\n    def get_agg_goals(self, experiment: Experiment) -&gt; pd.DataFrame:\n        \"\"\"\n        Get goals data pre-aggregated by `exp_variant_id`, `unit_type`, `agg_type`, `goal`,\n        `unit_id` and any dimension columns (in case of dimensional metrics)\n\n        See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for column\n        descriptions and example result.\n        \"\"\"\n        pass\n\n    def close(self) -&gt; None:\n        \"\"\"\n        Close underlying data source connection and frees resources (if any).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/dao.html#epstats.toolkit.Dao.close","title":"<code>close()</code>","text":"<p>Close underlying data source connection and frees resources (if any).</p> Source code in <code>src/epstats/toolkit/dao.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Close underlying data source connection and frees resources (if any).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/dao.html#epstats.toolkit.Dao.get_agg_goals","title":"<code>get_agg_goals(experiment)</code>","text":"<p>Get goals data pre-aggregated by <code>exp_variant_id</code>, <code>unit_type</code>, <code>agg_type</code>, <code>goal</code>, <code>unit_id</code> and any dimension columns (in case of dimensional metrics)</p> <p>See <code>Experiment.evaluate_agg</code> for column descriptions and example result.</p> Source code in <code>src/epstats/toolkit/dao.py</code> <pre><code>def get_agg_goals(self, experiment: Experiment) -&gt; pd.DataFrame:\n    \"\"\"\n    Get goals data pre-aggregated by `exp_variant_id`, `unit_type`, `agg_type`, `goal`,\n    `unit_id` and any dimension columns (in case of dimensional metrics)\n\n    See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for column\n    descriptions and example result.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/dao.html#epstats.toolkit.Dao.get_unit_goals","title":"<code>get_unit_goals(experiment)</code>","text":"<p>Get goals data pre-aggregated by <code>exp_variant_id</code>, <code>unit_type</code>, <code>agg_type</code>, <code>goal</code>, <code>unit_id</code> and any dimension columns (in case of dimensional metrics).</p> <p>See <code>Experiment.evaluate_by_unit</code> for column descriptions and example result.</p> Source code in <code>src/epstats/toolkit/dao.py</code> <pre><code>def get_unit_goals(self, experiment: Experiment) -&gt; pd.DataFrame:\n    \"\"\"\n    Get goals data pre-aggregated by `exp_variant_id`, `unit_type`, `agg_type`, `goal`,\n    `unit_id` and any dimension columns (in case of dimensional metrics).\n\n    See [`Experiment.evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for column\n    descriptions and example result.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/dao_factory.html","title":"DaoFactory","text":"<p>Factory creating instances of <code>Dao</code> classes.</p> <p>It is used in API server to get dao for every request.</p> Source code in <code>src/epstats/toolkit/dao.py</code> <pre><code>class DaoFactory:\n    \"\"\"\n    Factory creating instances of [`Dao`][epstats.toolkit.dao.Dao] classes.\n\n    It is used in API server to get dao for every request.\n    \"\"\"\n\n    def get_dao(self) -&gt; Dao:\n        \"\"\"\n        Create new instance of [`Dao`][epstats.toolkit.dao.Dao] to serve the request.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/dao_factory.html#epstats.toolkit.DaoFactory.get_dao","title":"<code>get_dao()</code>","text":"<p>Create new instance of <code>Dao</code> to serve the request.</p> Source code in <code>src/epstats/toolkit/dao.py</code> <pre><code>def get_dao(self) -&gt; Dao:\n    \"\"\"\n    Create new instance of [`Dao`][epstats.toolkit.dao.Dao] to serve the request.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/evaluation.html","title":"Evaluation","text":"<p>Results of an experiment evaluation.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>class Evaluation:\n    \"\"\"\n    Results of an experiment evaluation.\n    \"\"\"\n\n    def __init__(self, metrics: pd.DataFrame, checks: pd.DataFrame, exposures: pd.DataFrame):\n        self.metrics = metrics\n        self.checks = checks\n        self.exposures = exposures\n\n    @classmethod\n    def metric_columns(cls) -&gt; List[str]:\n        \"\"\"\n        `metrics` dataframe with columns:\n\n        1. `timestamp` - timestamp of evaluation\n        1. `exp_id` - experiment id\n        1. `metric_id` - metric id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n        1. `metric_name` - metric name as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n        1. `exp_variant_id` - variant id\n        1. `count` - number of exposures, value of metric denominator\n        1. `mean` - `sum_value` / `count`\n        1. `std` - sample standard deviation\n        1. `sum_value` - value of goals, value of metric nominator\n        1. `confidence_level` - current confidence level used to calculate `p_value` and `confidence_interval`\n        1. `diff` - relative diff between sample means of this and control variant\n        1. `test_stat` - value of test statistic of the relative difference in means\n        1. `p_value` - p-value of the test statistic under current `confidence_level`\n        1. `confidence_interval` - confidence interval of the `diff` under current `confidence_level`\n        1. `standard_error` - standard error of the `diff`\n        1. `degrees_of_freedom` - degrees of freedom of this variant mean\n        1. `sample_size` - current sample size\n        1. `required_sample_size` - size of the sample required to reach the required power\n        1. `power` - power based on the collected `sample_size`\n        1. `false_positive_risk` - false positive risk of a significant metric\n        \"\"\"\n        return [\n            \"timestamp\",\n            \"exp_id\",\n            \"metric_id\",\n            \"metric_name\",\n            \"exp_variant_id\",\n            \"count\",\n            \"mean\",\n            \"std\",\n            \"sum_value\",\n            \"confidence_level\",\n            \"diff\",\n            \"test_stat\",\n            \"p_value\",\n            \"confidence_interval\",\n            \"standard_error\",\n            \"degrees_of_freedom\",\n            \"minimum_effect\",\n            \"sample_size\",\n            \"required_sample_size\",\n            \"power\",\n            \"false_positive_risk\",\n        ]\n\n    @classmethod\n    def check_columns(cls) -&gt; List[str]:\n        \"\"\"\n        `checks` dataframe with columns:\n\n        1. `timestamp` - timestamp of evaluation\n        1. `exp_id` - experiment id\n        1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n        1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`,\n        `test_stat`, `confidence_level`\n        1. `value` - value of the variable\n        \"\"\"\n        return [\"timestamp\", \"exp_id\", \"check_id\", \"check_name\", \"variable_id\", \"value\"]\n\n    @classmethod\n    def exposure_columns(cls) -&gt; List[str]:\n        \"\"\"\n        `exposures` dataframe with columns:\n\n        1. `timestamp` - timestamp of evaluation\n        1. `exp_id` - experiment id\n        1. `exp_variant_id` - variant id\n        1. `exposures` - number of exposures of this variant\n        \"\"\"\n        return [\"exp_variant_id\", \"exposures\"]\n</code></pre>"},{"location":"api/evaluation.html#epstats.toolkit.Evaluation.check_columns","title":"<code>check_columns()</code>  <code>classmethod</code>","text":"<p><code>checks</code> dataframe with columns:</p> <ol> <li><code>timestamp</code> - timestamp of evaluation</li> <li><code>exp_id</code> - experiment id</li> <li><code>check_id</code> - check id as in <code>Experiment</code> definition</li> <li><code>variable_id</code> - name of the variable in check evaluation, SRM check has following variables <code>p_value</code>, <code>test_stat</code>, <code>confidence_level</code></li> <li><code>value</code> - value of the variable</li> </ol> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@classmethod\ndef check_columns(cls) -&gt; List[str]:\n    \"\"\"\n    `checks` dataframe with columns:\n\n    1. `timestamp` - timestamp of evaluation\n    1. `exp_id` - experiment id\n    1. `check_id` - check id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n    1. `variable_id` - name of the variable in check evaluation, SRM check has following variables `p_value`,\n    `test_stat`, `confidence_level`\n    1. `value` - value of the variable\n    \"\"\"\n    return [\"timestamp\", \"exp_id\", \"check_id\", \"check_name\", \"variable_id\", \"value\"]\n</code></pre>"},{"location":"api/evaluation.html#epstats.toolkit.Evaluation.exposure_columns","title":"<code>exposure_columns()</code>  <code>classmethod</code>","text":"<p><code>exposures</code> dataframe with columns:</p> <ol> <li><code>timestamp</code> - timestamp of evaluation</li> <li><code>exp_id</code> - experiment id</li> <li><code>exp_variant_id</code> - variant id</li> <li><code>exposures</code> - number of exposures of this variant</li> </ol> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@classmethod\ndef exposure_columns(cls) -&gt; List[str]:\n    \"\"\"\n    `exposures` dataframe with columns:\n\n    1. `timestamp` - timestamp of evaluation\n    1. `exp_id` - experiment id\n    1. `exp_variant_id` - variant id\n    1. `exposures` - number of exposures of this variant\n    \"\"\"\n    return [\"exp_variant_id\", \"exposures\"]\n</code></pre>"},{"location":"api/evaluation.html#epstats.toolkit.Evaluation.metric_columns","title":"<code>metric_columns()</code>  <code>classmethod</code>","text":"<p><code>metrics</code> dataframe with columns:</p> <ol> <li><code>timestamp</code> - timestamp of evaluation</li> <li><code>exp_id</code> - experiment id</li> <li><code>metric_id</code> - metric id as in <code>Experiment</code> definition</li> <li><code>metric_name</code> - metric name as in <code>Experiment</code> definition</li> <li><code>exp_variant_id</code> - variant id</li> <li><code>count</code> - number of exposures, value of metric denominator</li> <li><code>mean</code> - <code>sum_value</code> / <code>count</code></li> <li><code>std</code> - sample standard deviation</li> <li><code>sum_value</code> - value of goals, value of metric nominator</li> <li><code>confidence_level</code> - current confidence level used to calculate <code>p_value</code> and <code>confidence_interval</code></li> <li><code>diff</code> - relative diff between sample means of this and control variant</li> <li><code>test_stat</code> - value of test statistic of the relative difference in means</li> <li><code>p_value</code> - p-value of the test statistic under current <code>confidence_level</code></li> <li><code>confidence_interval</code> - confidence interval of the <code>diff</code> under current <code>confidence_level</code></li> <li><code>standard_error</code> - standard error of the <code>diff</code></li> <li><code>degrees_of_freedom</code> - degrees of freedom of this variant mean</li> <li><code>sample_size</code> - current sample size</li> <li><code>required_sample_size</code> - size of the sample required to reach the required power</li> <li><code>power</code> - power based on the collected <code>sample_size</code></li> <li><code>false_positive_risk</code> - false positive risk of a significant metric</li> </ol> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@classmethod\ndef metric_columns(cls) -&gt; List[str]:\n    \"\"\"\n    `metrics` dataframe with columns:\n\n    1. `timestamp` - timestamp of evaluation\n    1. `exp_id` - experiment id\n    1. `metric_id` - metric id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n    1. `metric_name` - metric name as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n    1. `exp_variant_id` - variant id\n    1. `count` - number of exposures, value of metric denominator\n    1. `mean` - `sum_value` / `count`\n    1. `std` - sample standard deviation\n    1. `sum_value` - value of goals, value of metric nominator\n    1. `confidence_level` - current confidence level used to calculate `p_value` and `confidence_interval`\n    1. `diff` - relative diff between sample means of this and control variant\n    1. `test_stat` - value of test statistic of the relative difference in means\n    1. `p_value` - p-value of the test statistic under current `confidence_level`\n    1. `confidence_interval` - confidence interval of the `diff` under current `confidence_level`\n    1. `standard_error` - standard error of the `diff`\n    1. `degrees_of_freedom` - degrees of freedom of this variant mean\n    1. `sample_size` - current sample size\n    1. `required_sample_size` - size of the sample required to reach the required power\n    1. `power` - power based on the collected `sample_size`\n    1. `false_positive_risk` - false positive risk of a significant metric\n    \"\"\"\n    return [\n        \"timestamp\",\n        \"exp_id\",\n        \"metric_id\",\n        \"metric_name\",\n        \"exp_variant_id\",\n        \"count\",\n        \"mean\",\n        \"std\",\n        \"sum_value\",\n        \"confidence_level\",\n        \"diff\",\n        \"test_stat\",\n        \"p_value\",\n        \"confidence_interval\",\n        \"standard_error\",\n        \"degrees_of_freedom\",\n        \"minimum_effect\",\n        \"sample_size\",\n        \"required_sample_size\",\n        \"power\",\n        \"false_positive_risk\",\n    ]\n</code></pre>"},{"location":"api/experiment.html","title":"Experiment","text":"<p>Evaluate one experiment described as a list of metrics and checks.</p> <p>See Statistics for details about statistical method used and <code>Evaluation</code> for description of output.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>class Experiment:\n    \"\"\"\n    Evaluate one experiment described as a list of metrics and checks.\n\n    See [Statistics](../stats/basics.md) for details about statistical method used\n    and [`Evaluation`][epstats.toolkit.experiment.Evaluation] for description of output.\n    \"\"\"\n\n    def __init__(\n        self,\n        id: str,\n        control_variant: str,\n        metrics: List[Metric],\n        checks: List[Check],\n        unit_type: str,\n        date_from: Optional[str] = None,\n        date_to: Optional[str] = None,\n        date_for: Optional[str] = None,\n        confidence_level: float = DEFAULT_CONFIDENCE_LEVEL,\n        variants: Optional[List[str]] = None,\n        filters: Optional[List[Filter]] = None,\n        null_hypothesis_rate: Optional[float] = None,\n        query_parameters: dict = {},\n    ):\n        self._logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n        self.id = id\n        self.control_variant = control_variant\n        self.unit_type = unit_type\n        self.metrics = metrics\n        self._check_metric_ids_unique()\n        self.checks = checks\n        self.date_from = datetime.strptime(date_from, \"%Y-%m-%d\").date() if date_from is not None else None\n        self.date_to = datetime.strptime(date_to, \"%Y-%m-%d\").date() if date_to is not None else None\n        self.date_for = (\n            datetime.strptime(date_for, \"%Y-%m-%d\").date() if date_for is not None else datetime.today().date()\n        )\n        self.confidence_level = confidence_level\n        self.variants = variants\n        self._exposure_goals = [\n            EpGoal(\n                [\n                    \"count\",\n                    \"(\",\n                    UnitType([unit_type]),\n                    \".\",\n                    AggType([\"global\"]),\n                    \".\",\n                    Goal([\"exposure\"]),\n                    \")\",\n                ]\n            )\n        ]\n        self._update_dimension_to_value()\n        self.filters = filters if filters is not None else []\n        self.query_parameters = query_parameters\n        self.null_hypothesis_rate = null_hypothesis_rate\n\n    def _check_metric_ids_unique(self):\n        \"\"\"\n        Raises an exception if `metrics` contain duplicated ids.\n        \"\"\"\n        id_counts = Counter(metric.id for metric in self.metrics)\n        for id_, count in id_counts.items():\n            if count &gt; 1:\n                raise ValueError(f\"Metric ids must be unique. Id={id_} found more than once.\")\n\n    def _update_dimension_to_value(self):\n        \"\"\"\n        To every `EpGoal` across all metrics, we need to add missing dimensions\n        that are present in every other `EpGoal` instances so the row masking\n        can work properly.\n        \"\"\"\n\n        all_goals = []\n        for metric_or_check in self.metrics + self.checks:\n            for attr in metric_or_check.__dict__.values():\n                if isinstance(attr, Parser):\n                    all_goals.append(attr._nominator_expr.get_goals())\n                    all_goals.append(attr._denominator_expr.get_goals())\n\n        all_goals = chain(*all_goals, self._exposure_goals)\n\n        for goal in all_goals:\n            for dimension in self.get_dimension_columns():\n                if dimension not in goal.dimension_to_value:\n                    goal.dimension_to_value[dimension] = \"\"\n\n    def evaluate_agg(self, goals: pd.DataFrame) -&gt; Evaluation:\n        \"\"\"\n        Evaluate all metrics and checks in the experiment from already pre-aggregated goals.\n\n        This method is usefull when there are too many units in the experiment to evaluate it\n        using [`evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit].\n\n        Does best effort to fill in missing goals and variants with zeros.\n\n        Arguments:\n            goals: dataframe with one row per goal and aggregated data in columns\n\n        `goals` dataframe columns:\n\n        1. `exp_id` - experiment id\n        1. `exp_variant_id` - variant\n        1. `unit_type` - randomization unit type\n        1. `agg_type` - level of aggregation\n        1. `goal` - goal name\n        1. any number of dimensional columns, e.g. column `product` containing values `p_1`\n        1. `count` - number of observed goals (e.g. conversions)\n        1. `sum_sqr_count` - summed squared number of observed goals per unit, it is similar\n        to `sum_sqr_value`\n        1. `sum_value` - value of observed goals\n        1. `sum_sqr_value` - summed squared value per unit.  This is used to calculate\n        sample standard deviation from pre-aggregated data (it is a term $\\\\sum x^2$\n        in $\\\\hat{\\\\sigma}^2 = \\\\frac{\\\\sum x^2 - \\\\frac{(\\\\sum x)^2}{n}}{n-1}$).\n        1. `count_unique` - number of units with at least 1 observed goal\n\n        Returns:\n            set of dataframes with evaluation\n\n        Usage:\n\n        ```python\n        from epstats.toolkit import Experiment, Metric, SrmCheck\n        experiment = Experiment(\n            'test-conversion',\n            'a',\n            [Metric(\n                1,\n                'Click-through Rate',\n                'count(test_unit_type.unit.click)',\n                'count(test_unit_type.global.exposure)'),\n            ],\n            [SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')],\n            unit_type='test_unit_type')\n\n        # This gets testing data, use other Dao or get aggregated goals in some other way.\n        from epstats.toolkit.testing import TestData\n        goals = TestData.load_goals_agg(experiment.id)\n\n        # evaluate experiment\n        ev = experiment.evaluate_agg(goals)\n\n        # work with results\n        print(ev.exposures)\n        print(ev.metrics[ev.metrics == 1])\n        print(ev.checks[ev.checks == 1])\n\n        # this is to assert that this code sample works correctly\n        from epstats.toolkit.testing import TestDao\n        assert_experiment(experiment, ev, TestDao(TestData()))\n        ```\n\n        Input data frame example:\n\n        ```\n        exp_id      exp_variant_id  unit_type           agg_type    goal            product             count   sum_sqr_count   sum_value   sum_sqr_value   count_unique\n        test-srm    a               test_unit_type      global      exposure                            100000  100000          100000      100000          100000\n        test-srm    b               test_unit_type      global      exposure                            100100  100100          100100      100100          100100\n        test-srm    a               test_unit_type      unit        conversion                          1200    1800            32000       66528           900\n        test-srm    a               test_unit_type_2    global      conversion      product_1           1000    1700            31000       55000           850\n        ```\n        \"\"\"\n        g = self._fix_missing_agg(goals)\n        return self._evaluate(\n            g,\n            Experiment._metrics_column_fce_agg,\n            Experiment._checks_fce_agg,\n            Experiment._exposures_fce_agg,\n        )\n\n    def evaluate_wide_agg(self, goals: pd.DataFrame) -&gt; Evaluation:\n        \"\"\"\n        This is a simplified version of the method [`evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg].\n\n        It consumes simple input `goals` dataframe, transfers it into suitable dataframe format and evaluate it using general method [`evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg].\n\n        It assumes that the first two columns are name of the experiment and variants. Than follows columns with data.\n\n        See usage of the method in the notebook [Ad-hoc A/B test evaluation using Ep-Stats][ad-hoc-ab-test-evaluation-using-ep-stats].\n\n        Arguments:\n            goals: dataframe with one row per variant and aggregated data in columns\n\n        Possible `goals` dataframe columns (check the input dataframe example):\n\n        1. `exp_id` - experiment id\n        1. `exp_variant_id` - variant\n        1. `clicks` - sum of clicks\n        1. `views` - sum of views\n        1. `bookings` - sum of bookings\n        1. `bookings_squared` - sum of bookings squared\n\n        Returns:\n            set of dataframes with evaluation\n\n        Usage:\n\n        ```python\n        from epstats.toolkit import Experiment, SimpleMetric, SimpleSrmCheck\n        from epstats.toolkit.results import results_long_to_wide, format_results\n        from epstats.toolkit.testing import TestData\n\n        # Load Test Data\n        goals = TestData.load_goals_simple_agg()\n\n        # Define the experiment\n        unit_type = 'test_unit_type'\n        experiment = Experiment(\n            'test-simple-metric',\n            'a',\n            [\n                SimpleMetric(1, 'Click-through Rate (CTR)', 'clicks', 'views', unit_type),\n                SimpleMetric(2, 'Conversion Rate', 'conversions', 'views', unit_type),\n                SimpleMetric(3, 'Revenue per Mille (RPM)', 'bookings', 'views', unit_type, metric_format='${:,.2f}', metric_value_multiplier=1000),\n            ],\n            [SimpleSrmCheck(1, 'SRM', 'views')],\n            unit_type=unit_type)\n\n        # Evaluate the experiment\n        ev = experiment.evaluate_wide_agg(goals)\n\n        # Work with results\n        print(ev.exposures)\n        print(ev.metrics)\n        print(ev.checks)\n\n        # Possible formatting of metrics\n        ev.metrics.pipe(results_long_to_wide).pipe(format_results, experiment, format_pct='{:.1%}', format_pval='{:.3f}')\n        ```\n\n        Input dataframe example:\n        ```\n        experiment_id   variant_id  views   clicks  conversions     bookings    bookings_squared\n        my-exp          a           473661  48194   413             17152       803105\n        my-exp          b           471485  47184   360             14503       677178\n        my-exp          c           477159  48841   406             15892       711661\n        my-exp          d           474934  49090   289             11995       566700\n        ```\n        \"\"\"\n        g = goals_wide_to_long(goals, self.unit_type)\n        return self.evaluate_agg(g)\n\n    def evaluate_by_unit(self, goals: pd.DataFrame) -&gt; Evaluation:\n        \"\"\"\n        Evaluate all metrics and checks in the experiment from goals grouped by `unit_id`.\n\n        This method is useful when there are not many (&lt;1M) units in the experiment to evaluate it.\n        If there many units exposed to the experiment, pre-aggregate data and use [`evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg].\n\n        Does best effort to fill in missing goals and variants with zeros.\n\n        Arguments:\n            goals: dataframe with one row per goal and aggregated data in columns\n\n        `goals` dataframe columns:\n\n        1. `exp_id` - experiment id\n        1. `exp_variant_id` - variant\n        1. `unit_type` - randomization unit type\n        1. `unit_id` - (randomization) unit id\n        1. `agg_type` - level of aggregation\n        1. `goal` - goal name\n        1. any number of dimensional columns, e.g. column `product` containing values `p_1`\n        1. `count` - number of observed goals\n        1. `sum_value` - value of observed goals\n\n        Returns:\n            set of dataframes with evaluation\n\n        Usage:\n\n        ```python\n        from epstats.toolkit import Experiment, Metric, SrmCheck\n        experiment = Experiment(\n            'test-real-valued',\n            'a',\n            [Metric(\n                2,\n                'Average Bookings',\n                'value(test_unit_type.unit.conversion)',\n                'count(test_unit_type.unit.exposure)')\n            ],\n            [],\n            unit_type='test_unit_type')\n\n        # This gets testing data, use other Dao or get aggregated goals in some other way.\n        from epstats.toolkit.testing import TestData\n        goals = TestData.load_goals_by_unit(experiment.id)\n\n        # evaluate experiment\n        ev = experiment.evaluate_by_unit(goals)\n\n        # work with results\n        print(ev.exposures)\n        print(ev.metrics[ev.metrics == 1])\n        print(ev.checks[ev.checks == 1])\n\n        # this is to assert that this code sample works correctly\n        from epstats.toolkit.testing import TestDao\n        assert_experiment(experiment, ev, TestDao(TestData()))\n        ```\n\n        Input data frame example:\n\n        ```\n        exp_id      exp_variant_id  unit_type       unit_id             agg_type    goal              product             count   sum_value\n        test-srm    a               test_unit_type  test_unit_type_1    unit        exposure                              1       1\n        test-srm    a               test_unit_type  test_unit_type_1    unit        conversion        product_1           2       75\n        test-srm    b               test_unit_type  test_unit_type_2    unit        exposure                              1       1\n        test-srm    b               test_unit_type  test_unit_type_3    unit        exposure                              1       1\n        test-srm    b               test_unit_type  test_unit_type_3    unit        conversion        product_2           1       1\n        ```\n        \"\"\"\n        g = self._fix_missing_by_unit(goals)\n\n        # We need to pivot table to get all goals per `unit_id` on the same row in the data frame.\n        # This is needed to be able to vector-evaluate compound metrics\n        # eg. `value(test_unit_type.unit.conversion) - value(test_unit_type.unit.refund)`\n        g = (\n            pd.pivot_table(\n                g,\n                values=[\"count\", \"sum_value\"],\n                index=[\n                    \"exp_id\",\n                    \"exp_variant_id\",\n                    \"unit_type\",\n                    \"agg_type\",\n                    \"unit_id\",\n                ]\n                + self.get_dimension_columns(),\n                columns=\"goal\",\n                aggfunc=\"sum\",\n                fill_value=0,\n            )\n            .swaplevel(axis=1)\n            .reset_index()\n        )\n\n        return self._evaluate(\n            g,\n            Experiment._metrics_column_fce_by_unit,\n            Experiment._checks_fce_by_unit,\n            Experiment._exposures_fce_by_unit,\n        )\n\n    def get_goals(self) -&gt; List[EpGoal]:\n        \"\"\"\n        List of all goals needed to evaluate all metrics and checks in the experiment.\n\n        Returns:\n            list of parsed structured goals\n        \"\"\"\n        res = set()\n        for m in self.metrics:\n            res = res.union(m.get_goals())\n        for c in self.checks:\n            res = res.union(c.get_goals())\n        res = res.union(self._exposure_goals)\n        return list(res)\n\n    @staticmethod\n    def _metrics_column_fce_agg(m: Metric, goals: pd.DataFrame):\n        \"\"\"\n        Gets count, sum_value, sum_sqr_value columns by expression from already aggregated goals.\n        \"\"\"\n        return m.get_evaluate_columns_agg(goals)\n\n    @staticmethod\n    def _metrics_column_fce_by_unit(m: Metric, goals: pd.DataFrame):\n        \"\"\"\n        Gets count, sum_value, sum_sqr_value columns by expression from goals grouped by `unit_id`.\n        \"\"\"\n        return m.get_evaluate_columns_by_unit(goals)\n\n    @staticmethod\n    def _checks_fce_agg(c: Check, goals: pd.DataFrame, control_variant: str):\n        \"\"\"\n        Evaluates checks from already aggregated goals.\n        \"\"\"\n        return c.evaluate_agg(goals, control_variant)\n\n    @staticmethod\n    def _checks_fce_by_unit(c: Check, goals: pd.DataFrame, control_variant: str):\n        \"\"\"\n        Evaluates checks from goals grouped by `unit_id`.\n        \"\"\"\n        return c.evaluate_by_unit(goals, control_variant)\n\n    @staticmethod\n    def _exposures_fce_agg(goals: pd.DataFrame, exp_id: str, unit_type: str):\n        \"\"\"\n        Evaluates checks from already aggregated goals.\n        \"\"\"\n        checks_df = (\n            goals[(goals[\"unit_type\"] == unit_type) &amp; (goals[\"agg_type\"] == \"global\") &amp; (goals[\"goal\"] == \"exposure\")]\n            .groupby(\"exp_variant_id\")\n            .agg(exposures=(\"count\", \"sum\"))\n            .reset_index()\n        )\n        checks_df[\"exp_id\"] = exp_id\n        return checks_df\n\n    @staticmethod\n    def _exposures_fce_by_unit(goals: pd.DataFrame, exp_id: str, unit_type: str):\n        \"\"\"\n        Evaluates checks from already aggregated goals.\n        \"\"\"\n        checks_df = goals[(goals[\"unit_type\"] == unit_type) &amp; (goals[\"agg_type\"] == \"unit\")][\n            [(\"exp_variant_id\", \"\"), (\"exposure\", \"count\")]\n        ]\n        checks_df = checks_df.droplevel(0, axis=1)\n        checks_df.columns = [\"exp_variant_id\", \"exposures\"]\n        d = checks_df.groupby(\"exp_variant_id\").agg(exposures=(\"exposures\", \"sum\")).reset_index()\n        d[\"exp_id\"] = exp_id\n        return d\n\n    def _evaluate(self, goals: pd.DataFrame, metrics_column_fce, checks_fce, exposures_fce):\n        metrics = self._evaluate_metrics(goals, metrics_column_fce)\n        checks = self._evaluate_checks(goals, checks_fce)\n        exposures = self._evaluate_exposures(goals, exposures_fce)\n        return Evaluation(metrics, checks, exposures)\n\n    def _evaluate_exposures(self, goals: pd.DataFrame, exposures_fce) -&gt; pd.DataFrame:\n        return exposures_fce(goals, self.id, self.unit_type)\n\n    def _evaluate_checks(self, goals: pd.DataFrame, check_fce) -&gt; pd.DataFrame:\n        res = []\n        for c in self.checks:\n            try:\n                r = check_fce(c, goals, self.control_variant)\n                r[\"exp_id\"] = self.id\n                res.append(r)\n            except Exception as e:\n                self._logger.warning(f\"Cannot evaluate check [{c.id} in experiment [{self.id}] because of {e}\")\n                check_evaluation_errors_metric.inc()\n\n        c = pd.concat(res, axis=0) if res != [] else pd.DataFrame([], columns=Evaluation.check_columns())\n        c[\"timestamp\"] = round(get_utc_timestamp(datetime.now()).timestamp())\n        return c[Evaluation.check_columns()]\n\n    def get_dimension_columns(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all dimensions used in all metrics in the experiment.\n        \"\"\"\n        return list({d for g in self.get_goals() for d in g.dimension_to_value.keys()})\n\n    def _set_variants(self, goals):\n        # what variants and goals there should be from all the goals needed to evaluate all metrics\n        self.variants = (\n            self.variants\n            if self.variants is not None\n            else np.unique(np.append(goals[\"exp_variant_id\"], self.control_variant))\n        )\n\n    def _fix_missing_agg(self, goals: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Adds zero values for missing goals and variants that are needed for metric evaluation.\n\n        Does that in the best effort - fills in `count`, `sum_sqr_count`, `sum_value`, `sum_sqr_value` and `count_unique` with zeros.\n        \"\"\"\n        # what variants and goals there should be from all the goals needed to evaluate all metrics\n        self._set_variants(goals)\n        g = goals[goals.exp_variant_id.isin(self.variants)]\n        nvs = self.variants\n        ngs = self.get_goals()\n\n        # variants * goals is the number of variant x goals combinations we expect in the data\n        lnvs = len(nvs)\n        lngs = len(ngs)\n\n        # create zero data frame for all variants and goals\n        empty_df = pd.DataFrame(\n            {\n                \"exp_id\": self.id,\n                \"exp_variant_id\": np.tile(nvs, lngs),\n                \"unit_type\": np.repeat([g.unit_type for g in ngs], lnvs),\n                \"agg_type\": np.repeat([g.agg_type for g in ngs], lnvs),\n                \"goal\": np.repeat([g.goal for g in ngs], lnvs),\n                \"count\": 0,\n                \"sum_sqr_count\": 0,\n                \"sum_value\": 0,\n                \"sum_sqr_value\": 0,\n                \"count_unique\": 0,\n            }\n        )\n\n        for dimension in self.get_dimension_columns():\n            empty_df[dimension] = np.repeat([g.dimension_to_value.get(dimension, \"\") for g in ngs], lnvs)\n\n        # join to existing data and use zeros for only missing variants and goals\n        m = (\n            pd.concat([g, empty_df], axis=0)\n            .fillna({d: \"\" for d in self.get_dimension_columns()})\n            .groupby(\n                [\n                    \"exp_id\",\n                    \"exp_variant_id\",\n                    \"unit_type\",\n                    \"agg_type\",\n                    \"goal\",\n                ]\n                + self.get_dimension_columns(),\n                # dropna=False,\n            )\n            .sum()\n            .reset_index()\n        )\n        return m\n\n    def _fix_missing_by_unit(self, goals: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Adds zero values for missing goals and variants that are needed for metric evaluation.\n\n        Does that in the best effort - fills in `count`, `sum_value` with zeros.\n        \"\"\"\n        # what variants and goals there should be from all the goals needed to evaluate all metrics\n        self._set_variants(goals)\n        g = goals[goals.exp_variant_id.isin(self.variants)]\n        nvs = self.variants\n        ngs = self.get_goals()\n\n        # variants * goals is the number of variant x goals combinations we expect in the data\n        lnvs = len(nvs)\n        lngs = len(ngs)\n\n        # create zero data frame for all variants and goals\n        empty_df = pd.DataFrame(\n            {\n                \"exp_id\": self.id,\n                \"exp_variant_id\": np.tile(nvs, lngs),\n                \"unit_type\": np.repeat([g.unit_type for g in ngs], lnvs),\n                \"agg_type\": np.repeat([g.agg_type for g in ngs], lnvs),\n                \"goal\": np.repeat([g.goal for g in ngs], lnvs),\n                \"unit_id\": np.nan,\n                \"count\": 0,\n                \"sum_value\": 0,\n            }\n        )\n\n        for dimension in self.get_dimension_columns():\n            empty_df[dimension] = np.repeat([g.dimension_to_value.get(dimension, \"\") for g in ngs], lnvs)\n\n        # join to existing data and use zeros for only missing variants and goals\n        m = pd.concat([g, empty_df], axis=0).fillna({d: \"\" for d in self.get_dimension_columns()})\n        return m[\n            [\n                \"exp_id\",\n                \"exp_variant_id\",\n                \"unit_type\",\n                \"agg_type\",\n                \"goal\",\n                \"unit_id\",\n                \"count\",\n                \"sum_value\",\n            ]\n            + self.get_dimension_columns()\n        ]\n\n    def _get_required_sample_size(\n        self,\n        metric_row: pd.Series,\n        controls: dict,\n        minimum_effects: dict,\n        metrics_with_value_denominator: set,\n        n_variants: int,\n    ) -&gt; pd.Series:\n        metric_id = metric_row[\"metric_id\"]\n        minimum_effect = minimum_effects[metric_id]\n        index = [\"minimum_effect\", \"sample_size\", \"required_sample_size\"]\n\n        # Right now, metric with value() denominator would return count that is not equal\n        # to the sample size. In such case we do not evaluate the required sample size.\n        # TODO: add suport for value() denominator metrics,\n        # parser will return an additional column equal to count or count_unique.\n        sample_size = metric_row[\"count\"] if metric_id not in metrics_with_value_denominator else np.nan\n\n        if metric_row[\"exp_variant_id\"] == self.control_variant or pd.isna(minimum_effect):\n            return pd.Series([np.nan, sample_size, np.nan], index)\n\n        metric_id = metric_row[\"metric_id\"]\n        return pd.Series(\n            [\n                minimum_effect,\n                sample_size,\n                Statistics.required_sample_size_per_variant(\n                    n_variants=n_variants,\n                    minimum_effect=minimum_effect,\n                    mean=controls[metric_id][\"mean\"],\n                    std=controls[metric_id][\"std\"],\n                    std_2=metric_row[\"std\"],\n                    confidence_level=metric_row[\"confidence_level\"],\n                    power=DEFAULT_POWER,\n                ),\n            ],\n            index,\n        )\n\n    def _get_required_sample_sizes(self, metrics: pd.DataFrame, n_variants: int) -&gt; pd.DataFrame:\n        controls = {\n            r[\"metric_id\"]: {\"mean\": r[\"mean\"], \"std\": r[\"std\"]}\n            for _, r in metrics.iterrows()\n            if r[\"exp_variant_id\"] == self.control_variant\n        }\n\n        minimum_effects = {m.id: m.minimum_effect for m in self.metrics}\n        metrics_with_value_denominator = {\n            m.id for m in self.metrics if m.denominator.startswith(\"value(\") and not isinstance(m, SimpleMetric)\n        }\n\n        return metrics.apply(\n            lambda metric_row: self._get_required_sample_size(\n                metric_row=metric_row,\n                controls=controls,\n                minimum_effects=minimum_effects,\n                metrics_with_value_denominator=metrics_with_value_denominator,\n                n_variants=n_variants,\n            ),\n            axis=1,\n        )\n\n    def _get_power_from_required_sample_sizes(self, metrics: pd.DataFrame, n_variants: int) -&gt; pd.Series:\n        return metrics.apply(\n            lambda metric_row: Statistics.power_from_required_sample_size_per_variant(\n                n_variants=n_variants,\n                sample_size_per_variant=metric_row[\"sample_size\"],\n                required_sample_size_per_variant=metric_row[\"required_sample_size\"],\n                required_confidence_level=metric_row[\"confidence_level\"],\n                required_power=DEFAULT_POWER,\n            ),\n            axis=1,\n        )\n\n    def _get_false_positive_risk(self, metric_row: pd.Series) -&gt; float:\n        if self.null_hypothesis_rate is None:\n            return np.nan\n\n        if metric_row[\"p_value\"] &gt;= (1 - metric_row[\"confidence_level\"]):\n            return np.nan\n\n        return Statistics.false_positive_risk(\n            null_hypothesis_rate=self.null_hypothesis_rate,\n            power=metric_row[\"power\"],\n            p_value=metric_row[\"p_value\"],\n        )\n\n    def _get_false_positive_risks(self, metrics: pd.DataFrame) -&gt; pd.Series:\n        return metrics.apply(self._get_false_positive_risk, axis=1)\n\n    def _evaluate_metrics(self, goals: pd.DataFrame, column_fce) -&gt; pd.DataFrame:\n        if not self.metrics:\n            return pd.DataFrame([], columns=Evaluation.metric_columns())\n\n        sts = []\n        for m in self.metrics:\n            count, sum_value, sum_sqr_value = column_fce(m, goals)\n            sts.append([count, sum_value, sum_sqr_value])\n        stats = np.array(sts).transpose(0, 2, 1)\n        metrics = stats.shape[0]\n        n_variants = stats.shape[1]\n\n        count = stats[:, :, 0]\n        sum_value = stats[:, :, 1]\n        sum_sqr_value = stats[:, :, 2]\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            # We fill in zeros, when goal data are missing for some variant.\n            # There could be division by zero here which is expected as we return\n            # nan or inf values to the caller.\n            mean = sum_value / count\n            std = np.sqrt((sum_sqr_value - sum_value * sum_value / count) / (count - 1))\n\n        # sequential testing correction\n        if self.date_from is not None and self.date_to is not None:\n            # Parameters\n            test_length = (self.date_to - self.date_from).days + 1  # test length in days\n            actual_day = (self.date_for - self.date_from).days + 1  # day(s) since beginning of the test\n            actual_day = min(actual_day, test_length)  # actual day of evaluation must be in interval [1, test_length]\n\n            # confidence level adjustment - applied when actual_day &lt; test_length (test is still running)\n            confidence_level = Statistics.obf_alpha_spending_function(self.confidence_level, test_length, actual_day)\n        else:\n            confidence_level = self.confidence_level  # no change\n\n        stats = np.dstack((count, mean, std, sum_value, np.ones(count.shape) * confidence_level))\n        stats = np.dstack(\n            (\n                np.repeat([m.id for m in self.metrics], n_variants).reshape(metrics, n_variants, -1),\n                np.repeat([m.name for m in self.metrics], n_variants).reshape(metrics, n_variants, -1),\n                np.tile(goals[\"exp_variant_id\"].unique(), metrics).reshape(metrics, n_variants, -1),\n                stats,\n            )\n        )\n\n        # dimensions of `stats` array: (metrics, variants, stats)\n        # elements of `stats` array: metrics_id, exp_variant_id, count, mean, std, sum_value, confidence_level\n        # hypothesis evaluation (standard way using t-test)\n        c = Statistics.ttest_evaluation(stats, self.control_variant)\n\n        # multiple variants (comparisons) correction - applied when we have multiple treatment variants\n        if n_variants &gt; 2:\n            c = Statistics.multiple_comparisons_correction(c, n_variants, metrics, confidence_level)\n\n        c[\"exp_id\"] = self.id\n        c[\"timestamp\"] = round(get_utc_timestamp(datetime.now()).timestamp())\n        c[[\"minimum_effect\", \"sample_size\", \"required_sample_size\"]] = self._get_required_sample_sizes(c, n_variants)\n        c[\"power\"] = self._get_power_from_required_sample_sizes(c, n_variants)\n        c[\"false_positive_risk\"] = self._get_false_positive_risks(c)\n        return c[Evaluation.metric_columns()]\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._check_metric_ids_unique","title":"<code>_check_metric_ids_unique()</code>","text":"<p>Raises an exception if <code>metrics</code> contain duplicated ids.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def _check_metric_ids_unique(self):\n    \"\"\"\n    Raises an exception if `metrics` contain duplicated ids.\n    \"\"\"\n    id_counts = Counter(metric.id for metric in self.metrics)\n    for id_, count in id_counts.items():\n        if count &gt; 1:\n            raise ValueError(f\"Metric ids must be unique. Id={id_} found more than once.\")\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._checks_fce_agg","title":"<code>_checks_fce_agg(c, goals, control_variant)</code>  <code>staticmethod</code>","text":"<p>Evaluates checks from already aggregated goals.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@staticmethod\ndef _checks_fce_agg(c: Check, goals: pd.DataFrame, control_variant: str):\n    \"\"\"\n    Evaluates checks from already aggregated goals.\n    \"\"\"\n    return c.evaluate_agg(goals, control_variant)\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._checks_fce_by_unit","title":"<code>_checks_fce_by_unit(c, goals, control_variant)</code>  <code>staticmethod</code>","text":"<p>Evaluates checks from goals grouped by <code>unit_id</code>.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@staticmethod\ndef _checks_fce_by_unit(c: Check, goals: pd.DataFrame, control_variant: str):\n    \"\"\"\n    Evaluates checks from goals grouped by `unit_id`.\n    \"\"\"\n    return c.evaluate_by_unit(goals, control_variant)\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._exposures_fce_agg","title":"<code>_exposures_fce_agg(goals, exp_id, unit_type)</code>  <code>staticmethod</code>","text":"<p>Evaluates checks from already aggregated goals.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@staticmethod\ndef _exposures_fce_agg(goals: pd.DataFrame, exp_id: str, unit_type: str):\n    \"\"\"\n    Evaluates checks from already aggregated goals.\n    \"\"\"\n    checks_df = (\n        goals[(goals[\"unit_type\"] == unit_type) &amp; (goals[\"agg_type\"] == \"global\") &amp; (goals[\"goal\"] == \"exposure\")]\n        .groupby(\"exp_variant_id\")\n        .agg(exposures=(\"count\", \"sum\"))\n        .reset_index()\n    )\n    checks_df[\"exp_id\"] = exp_id\n    return checks_df\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._exposures_fce_by_unit","title":"<code>_exposures_fce_by_unit(goals, exp_id, unit_type)</code>  <code>staticmethod</code>","text":"<p>Evaluates checks from already aggregated goals.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@staticmethod\ndef _exposures_fce_by_unit(goals: pd.DataFrame, exp_id: str, unit_type: str):\n    \"\"\"\n    Evaluates checks from already aggregated goals.\n    \"\"\"\n    checks_df = goals[(goals[\"unit_type\"] == unit_type) &amp; (goals[\"agg_type\"] == \"unit\")][\n        [(\"exp_variant_id\", \"\"), (\"exposure\", \"count\")]\n    ]\n    checks_df = checks_df.droplevel(0, axis=1)\n    checks_df.columns = [\"exp_variant_id\", \"exposures\"]\n    d = checks_df.groupby(\"exp_variant_id\").agg(exposures=(\"exposures\", \"sum\")).reset_index()\n    d[\"exp_id\"] = exp_id\n    return d\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._fix_missing_agg","title":"<code>_fix_missing_agg(goals)</code>","text":"<p>Adds zero values for missing goals and variants that are needed for metric evaluation.</p> <p>Does that in the best effort - fills in <code>count</code>, <code>sum_sqr_count</code>, <code>sum_value</code>, <code>sum_sqr_value</code> and <code>count_unique</code> with zeros.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def _fix_missing_agg(self, goals: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Adds zero values for missing goals and variants that are needed for metric evaluation.\n\n    Does that in the best effort - fills in `count`, `sum_sqr_count`, `sum_value`, `sum_sqr_value` and `count_unique` with zeros.\n    \"\"\"\n    # what variants and goals there should be from all the goals needed to evaluate all metrics\n    self._set_variants(goals)\n    g = goals[goals.exp_variant_id.isin(self.variants)]\n    nvs = self.variants\n    ngs = self.get_goals()\n\n    # variants * goals is the number of variant x goals combinations we expect in the data\n    lnvs = len(nvs)\n    lngs = len(ngs)\n\n    # create zero data frame for all variants and goals\n    empty_df = pd.DataFrame(\n        {\n            \"exp_id\": self.id,\n            \"exp_variant_id\": np.tile(nvs, lngs),\n            \"unit_type\": np.repeat([g.unit_type for g in ngs], lnvs),\n            \"agg_type\": np.repeat([g.agg_type for g in ngs], lnvs),\n            \"goal\": np.repeat([g.goal for g in ngs], lnvs),\n            \"count\": 0,\n            \"sum_sqr_count\": 0,\n            \"sum_value\": 0,\n            \"sum_sqr_value\": 0,\n            \"count_unique\": 0,\n        }\n    )\n\n    for dimension in self.get_dimension_columns():\n        empty_df[dimension] = np.repeat([g.dimension_to_value.get(dimension, \"\") for g in ngs], lnvs)\n\n    # join to existing data and use zeros for only missing variants and goals\n    m = (\n        pd.concat([g, empty_df], axis=0)\n        .fillna({d: \"\" for d in self.get_dimension_columns()})\n        .groupby(\n            [\n                \"exp_id\",\n                \"exp_variant_id\",\n                \"unit_type\",\n                \"agg_type\",\n                \"goal\",\n            ]\n            + self.get_dimension_columns(),\n            # dropna=False,\n        )\n        .sum()\n        .reset_index()\n    )\n    return m\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._fix_missing_by_unit","title":"<code>_fix_missing_by_unit(goals)</code>","text":"<p>Adds zero values for missing goals and variants that are needed for metric evaluation.</p> <p>Does that in the best effort - fills in <code>count</code>, <code>sum_value</code> with zeros.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def _fix_missing_by_unit(self, goals: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Adds zero values for missing goals and variants that are needed for metric evaluation.\n\n    Does that in the best effort - fills in `count`, `sum_value` with zeros.\n    \"\"\"\n    # what variants and goals there should be from all the goals needed to evaluate all metrics\n    self._set_variants(goals)\n    g = goals[goals.exp_variant_id.isin(self.variants)]\n    nvs = self.variants\n    ngs = self.get_goals()\n\n    # variants * goals is the number of variant x goals combinations we expect in the data\n    lnvs = len(nvs)\n    lngs = len(ngs)\n\n    # create zero data frame for all variants and goals\n    empty_df = pd.DataFrame(\n        {\n            \"exp_id\": self.id,\n            \"exp_variant_id\": np.tile(nvs, lngs),\n            \"unit_type\": np.repeat([g.unit_type for g in ngs], lnvs),\n            \"agg_type\": np.repeat([g.agg_type for g in ngs], lnvs),\n            \"goal\": np.repeat([g.goal for g in ngs], lnvs),\n            \"unit_id\": np.nan,\n            \"count\": 0,\n            \"sum_value\": 0,\n        }\n    )\n\n    for dimension in self.get_dimension_columns():\n        empty_df[dimension] = np.repeat([g.dimension_to_value.get(dimension, \"\") for g in ngs], lnvs)\n\n    # join to existing data and use zeros for only missing variants and goals\n    m = pd.concat([g, empty_df], axis=0).fillna({d: \"\" for d in self.get_dimension_columns()})\n    return m[\n        [\n            \"exp_id\",\n            \"exp_variant_id\",\n            \"unit_type\",\n            \"agg_type\",\n            \"goal\",\n            \"unit_id\",\n            \"count\",\n            \"sum_value\",\n        ]\n        + self.get_dimension_columns()\n    ]\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._metrics_column_fce_agg","title":"<code>_metrics_column_fce_agg(m, goals)</code>  <code>staticmethod</code>","text":"<p>Gets count, sum_value, sum_sqr_value columns by expression from already aggregated goals.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@staticmethod\ndef _metrics_column_fce_agg(m: Metric, goals: pd.DataFrame):\n    \"\"\"\n    Gets count, sum_value, sum_sqr_value columns by expression from already aggregated goals.\n    \"\"\"\n    return m.get_evaluate_columns_agg(goals)\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._metrics_column_fce_by_unit","title":"<code>_metrics_column_fce_by_unit(m, goals)</code>  <code>staticmethod</code>","text":"<p>Gets count, sum_value, sum_sqr_value columns by expression from goals grouped by <code>unit_id</code>.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>@staticmethod\ndef _metrics_column_fce_by_unit(m: Metric, goals: pd.DataFrame):\n    \"\"\"\n    Gets count, sum_value, sum_sqr_value columns by expression from goals grouped by `unit_id`.\n    \"\"\"\n    return m.get_evaluate_columns_by_unit(goals)\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment._update_dimension_to_value","title":"<code>_update_dimension_to_value()</code>","text":"<p>To every <code>EpGoal</code> across all metrics, we need to add missing dimensions that are present in every other <code>EpGoal</code> instances so the row masking can work properly.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def _update_dimension_to_value(self):\n    \"\"\"\n    To every `EpGoal` across all metrics, we need to add missing dimensions\n    that are present in every other `EpGoal` instances so the row masking\n    can work properly.\n    \"\"\"\n\n    all_goals = []\n    for metric_or_check in self.metrics + self.checks:\n        for attr in metric_or_check.__dict__.values():\n            if isinstance(attr, Parser):\n                all_goals.append(attr._nominator_expr.get_goals())\n                all_goals.append(attr._denominator_expr.get_goals())\n\n    all_goals = chain(*all_goals, self._exposure_goals)\n\n    for goal in all_goals:\n        for dimension in self.get_dimension_columns():\n            if dimension not in goal.dimension_to_value:\n                goal.dimension_to_value[dimension] = \"\"\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment.evaluate_agg","title":"<code>evaluate_agg(goals)</code>","text":"<p>Evaluate all metrics and checks in the experiment from already pre-aggregated goals.</p> <p>This method is usefull when there are too many units in the experiment to evaluate it using <code>evaluate_by_unit</code>.</p> <p>Does best effort to fill in missing goals and variants with zeros.</p> <p>Parameters:</p> Name Type Description Default <code>goals</code> <code>DataFrame</code> <p>dataframe with one row per goal and aggregated data in columns</p> required <p><code>goals</code> dataframe columns:</p> <ol> <li><code>exp_id</code> - experiment id</li> <li><code>exp_variant_id</code> - variant</li> <li><code>unit_type</code> - randomization unit type</li> <li><code>agg_type</code> - level of aggregation</li> <li><code>goal</code> - goal name</li> <li>any number of dimensional columns, e.g. column <code>product</code> containing values <code>p_1</code></li> <li><code>count</code> - number of observed goals (e.g. conversions)</li> <li><code>sum_sqr_count</code> - summed squared number of observed goals per unit, it is similar to <code>sum_sqr_value</code></li> <li><code>sum_value</code> - value of observed goals</li> <li><code>sum_sqr_value</code> - summed squared value per unit.  This is used to calculate sample standard deviation from pre-aggregated data (it is a term \\(\\sum x^2\\) in \\(\\hat{\\sigma}^2 = \\frac{\\sum x^2 - \\frac{(\\sum x)^2}{n}}{n-1}\\)).</li> <li><code>count_unique</code> - number of units with at least 1 observed goal</li> </ol> <p>Returns:</p> Type Description <code>Evaluation</code> <p>set of dataframes with evaluation</p> <p>Usage:</p> <pre><code>from epstats.toolkit import Experiment, Metric, SrmCheck\nexperiment = Experiment(\n    'test-conversion',\n    'a',\n    [Metric(\n        1,\n        'Click-through Rate',\n        'count(test_unit_type.unit.click)',\n        'count(test_unit_type.global.exposure)'),\n    ],\n    [SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')],\n    unit_type='test_unit_type')\n\n# This gets testing data, use other Dao or get aggregated goals in some other way.\nfrom epstats.toolkit.testing import TestData\ngoals = TestData.load_goals_agg(experiment.id)\n\n# evaluate experiment\nev = experiment.evaluate_agg(goals)\n\n# work with results\nprint(ev.exposures)\nprint(ev.metrics[ev.metrics == 1])\nprint(ev.checks[ev.checks == 1])\n\n# this is to assert that this code sample works correctly\nfrom epstats.toolkit.testing import TestDao\nassert_experiment(experiment, ev, TestDao(TestData()))\n</code></pre> <p>Input data frame example:</p> <pre><code>exp_id      exp_variant_id  unit_type           agg_type    goal            product             count   sum_sqr_count   sum_value   sum_sqr_value   count_unique\ntest-srm    a               test_unit_type      global      exposure                            100000  100000          100000      100000          100000\ntest-srm    b               test_unit_type      global      exposure                            100100  100100          100100      100100          100100\ntest-srm    a               test_unit_type      unit        conversion                          1200    1800            32000       66528           900\ntest-srm    a               test_unit_type_2    global      conversion      product_1           1000    1700            31000       55000           850\n</code></pre> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def evaluate_agg(self, goals: pd.DataFrame) -&gt; Evaluation:\n    \"\"\"\n    Evaluate all metrics and checks in the experiment from already pre-aggregated goals.\n\n    This method is usefull when there are too many units in the experiment to evaluate it\n    using [`evaluate_by_unit`][epstats.toolkit.experiment.Experiment.evaluate_by_unit].\n\n    Does best effort to fill in missing goals and variants with zeros.\n\n    Arguments:\n        goals: dataframe with one row per goal and aggregated data in columns\n\n    `goals` dataframe columns:\n\n    1. `exp_id` - experiment id\n    1. `exp_variant_id` - variant\n    1. `unit_type` - randomization unit type\n    1. `agg_type` - level of aggregation\n    1. `goal` - goal name\n    1. any number of dimensional columns, e.g. column `product` containing values `p_1`\n    1. `count` - number of observed goals (e.g. conversions)\n    1. `sum_sqr_count` - summed squared number of observed goals per unit, it is similar\n    to `sum_sqr_value`\n    1. `sum_value` - value of observed goals\n    1. `sum_sqr_value` - summed squared value per unit.  This is used to calculate\n    sample standard deviation from pre-aggregated data (it is a term $\\\\sum x^2$\n    in $\\\\hat{\\\\sigma}^2 = \\\\frac{\\\\sum x^2 - \\\\frac{(\\\\sum x)^2}{n}}{n-1}$).\n    1. `count_unique` - number of units with at least 1 observed goal\n\n    Returns:\n        set of dataframes with evaluation\n\n    Usage:\n\n    ```python\n    from epstats.toolkit import Experiment, Metric, SrmCheck\n    experiment = Experiment(\n        'test-conversion',\n        'a',\n        [Metric(\n            1,\n            'Click-through Rate',\n            'count(test_unit_type.unit.click)',\n            'count(test_unit_type.global.exposure)'),\n        ],\n        [SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')],\n        unit_type='test_unit_type')\n\n    # This gets testing data, use other Dao or get aggregated goals in some other way.\n    from epstats.toolkit.testing import TestData\n    goals = TestData.load_goals_agg(experiment.id)\n\n    # evaluate experiment\n    ev = experiment.evaluate_agg(goals)\n\n    # work with results\n    print(ev.exposures)\n    print(ev.metrics[ev.metrics == 1])\n    print(ev.checks[ev.checks == 1])\n\n    # this is to assert that this code sample works correctly\n    from epstats.toolkit.testing import TestDao\n    assert_experiment(experiment, ev, TestDao(TestData()))\n    ```\n\n    Input data frame example:\n\n    ```\n    exp_id      exp_variant_id  unit_type           agg_type    goal            product             count   sum_sqr_count   sum_value   sum_sqr_value   count_unique\n    test-srm    a               test_unit_type      global      exposure                            100000  100000          100000      100000          100000\n    test-srm    b               test_unit_type      global      exposure                            100100  100100          100100      100100          100100\n    test-srm    a               test_unit_type      unit        conversion                          1200    1800            32000       66528           900\n    test-srm    a               test_unit_type_2    global      conversion      product_1           1000    1700            31000       55000           850\n    ```\n    \"\"\"\n    g = self._fix_missing_agg(goals)\n    return self._evaluate(\n        g,\n        Experiment._metrics_column_fce_agg,\n        Experiment._checks_fce_agg,\n        Experiment._exposures_fce_agg,\n    )\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment.evaluate_by_unit","title":"<code>evaluate_by_unit(goals)</code>","text":"<p>Evaluate all metrics and checks in the experiment from goals grouped by <code>unit_id</code>.</p> <p>This method is useful when there are not many (&lt;1M) units in the experiment to evaluate it. If there many units exposed to the experiment, pre-aggregate data and use <code>evaluate_agg</code>.</p> <p>Does best effort to fill in missing goals and variants with zeros.</p> <p>Parameters:</p> Name Type Description Default <code>goals</code> <code>DataFrame</code> <p>dataframe with one row per goal and aggregated data in columns</p> required <p><code>goals</code> dataframe columns:</p> <ol> <li><code>exp_id</code> - experiment id</li> <li><code>exp_variant_id</code> - variant</li> <li><code>unit_type</code> - randomization unit type</li> <li><code>unit_id</code> - (randomization) unit id</li> <li><code>agg_type</code> - level of aggregation</li> <li><code>goal</code> - goal name</li> <li>any number of dimensional columns, e.g. column <code>product</code> containing values <code>p_1</code></li> <li><code>count</code> - number of observed goals</li> <li><code>sum_value</code> - value of observed goals</li> </ol> <p>Returns:</p> Type Description <code>Evaluation</code> <p>set of dataframes with evaluation</p> <p>Usage:</p> <pre><code>from epstats.toolkit import Experiment, Metric, SrmCheck\nexperiment = Experiment(\n    'test-real-valued',\n    'a',\n    [Metric(\n        2,\n        'Average Bookings',\n        'value(test_unit_type.unit.conversion)',\n        'count(test_unit_type.unit.exposure)')\n    ],\n    [],\n    unit_type='test_unit_type')\n\n# This gets testing data, use other Dao or get aggregated goals in some other way.\nfrom epstats.toolkit.testing import TestData\ngoals = TestData.load_goals_by_unit(experiment.id)\n\n# evaluate experiment\nev = experiment.evaluate_by_unit(goals)\n\n# work with results\nprint(ev.exposures)\nprint(ev.metrics[ev.metrics == 1])\nprint(ev.checks[ev.checks == 1])\n\n# this is to assert that this code sample works correctly\nfrom epstats.toolkit.testing import TestDao\nassert_experiment(experiment, ev, TestDao(TestData()))\n</code></pre> <p>Input data frame example:</p> <pre><code>exp_id      exp_variant_id  unit_type       unit_id             agg_type    goal              product             count   sum_value\ntest-srm    a               test_unit_type  test_unit_type_1    unit        exposure                              1       1\ntest-srm    a               test_unit_type  test_unit_type_1    unit        conversion        product_1           2       75\ntest-srm    b               test_unit_type  test_unit_type_2    unit        exposure                              1       1\ntest-srm    b               test_unit_type  test_unit_type_3    unit        exposure                              1       1\ntest-srm    b               test_unit_type  test_unit_type_3    unit        conversion        product_2           1       1\n</code></pre> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def evaluate_by_unit(self, goals: pd.DataFrame) -&gt; Evaluation:\n    \"\"\"\n    Evaluate all metrics and checks in the experiment from goals grouped by `unit_id`.\n\n    This method is useful when there are not many (&lt;1M) units in the experiment to evaluate it.\n    If there many units exposed to the experiment, pre-aggregate data and use [`evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg].\n\n    Does best effort to fill in missing goals and variants with zeros.\n\n    Arguments:\n        goals: dataframe with one row per goal and aggregated data in columns\n\n    `goals` dataframe columns:\n\n    1. `exp_id` - experiment id\n    1. `exp_variant_id` - variant\n    1. `unit_type` - randomization unit type\n    1. `unit_id` - (randomization) unit id\n    1. `agg_type` - level of aggregation\n    1. `goal` - goal name\n    1. any number of dimensional columns, e.g. column `product` containing values `p_1`\n    1. `count` - number of observed goals\n    1. `sum_value` - value of observed goals\n\n    Returns:\n        set of dataframes with evaluation\n\n    Usage:\n\n    ```python\n    from epstats.toolkit import Experiment, Metric, SrmCheck\n    experiment = Experiment(\n        'test-real-valued',\n        'a',\n        [Metric(\n            2,\n            'Average Bookings',\n            'value(test_unit_type.unit.conversion)',\n            'count(test_unit_type.unit.exposure)')\n        ],\n        [],\n        unit_type='test_unit_type')\n\n    # This gets testing data, use other Dao or get aggregated goals in some other way.\n    from epstats.toolkit.testing import TestData\n    goals = TestData.load_goals_by_unit(experiment.id)\n\n    # evaluate experiment\n    ev = experiment.evaluate_by_unit(goals)\n\n    # work with results\n    print(ev.exposures)\n    print(ev.metrics[ev.metrics == 1])\n    print(ev.checks[ev.checks == 1])\n\n    # this is to assert that this code sample works correctly\n    from epstats.toolkit.testing import TestDao\n    assert_experiment(experiment, ev, TestDao(TestData()))\n    ```\n\n    Input data frame example:\n\n    ```\n    exp_id      exp_variant_id  unit_type       unit_id             agg_type    goal              product             count   sum_value\n    test-srm    a               test_unit_type  test_unit_type_1    unit        exposure                              1       1\n    test-srm    a               test_unit_type  test_unit_type_1    unit        conversion        product_1           2       75\n    test-srm    b               test_unit_type  test_unit_type_2    unit        exposure                              1       1\n    test-srm    b               test_unit_type  test_unit_type_3    unit        exposure                              1       1\n    test-srm    b               test_unit_type  test_unit_type_3    unit        conversion        product_2           1       1\n    ```\n    \"\"\"\n    g = self._fix_missing_by_unit(goals)\n\n    # We need to pivot table to get all goals per `unit_id` on the same row in the data frame.\n    # This is needed to be able to vector-evaluate compound metrics\n    # eg. `value(test_unit_type.unit.conversion) - value(test_unit_type.unit.refund)`\n    g = (\n        pd.pivot_table(\n            g,\n            values=[\"count\", \"sum_value\"],\n            index=[\n                \"exp_id\",\n                \"exp_variant_id\",\n                \"unit_type\",\n                \"agg_type\",\n                \"unit_id\",\n            ]\n            + self.get_dimension_columns(),\n            columns=\"goal\",\n            aggfunc=\"sum\",\n            fill_value=0,\n        )\n        .swaplevel(axis=1)\n        .reset_index()\n    )\n\n    return self._evaluate(\n        g,\n        Experiment._metrics_column_fce_by_unit,\n        Experiment._checks_fce_by_unit,\n        Experiment._exposures_fce_by_unit,\n    )\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment.evaluate_wide_agg","title":"<code>evaluate_wide_agg(goals)</code>","text":"<p>This is a simplified version of the method <code>evaluate_agg</code>.</p> <p>It consumes simple input <code>goals</code> dataframe, transfers it into suitable dataframe format and evaluate it using general method <code>evaluate_agg</code>.</p> <p>It assumes that the first two columns are name of the experiment and variants. Than follows columns with data.</p> <p>See usage of the method in the notebook Ad-hoc A/B test evaluation using Ep-Stats.</p> <p>Parameters:</p> Name Type Description Default <code>goals</code> <code>DataFrame</code> <p>dataframe with one row per variant and aggregated data in columns</p> required <p>Possible <code>goals</code> dataframe columns (check the input dataframe example):</p> <ol> <li><code>exp_id</code> - experiment id</li> <li><code>exp_variant_id</code> - variant</li> <li><code>clicks</code> - sum of clicks</li> <li><code>views</code> - sum of views</li> <li><code>bookings</code> - sum of bookings</li> <li><code>bookings_squared</code> - sum of bookings squared</li> </ol> <p>Returns:</p> Type Description <code>Evaluation</code> <p>set of dataframes with evaluation</p> <p>Usage:</p> <pre><code>from epstats.toolkit import Experiment, SimpleMetric, SimpleSrmCheck\nfrom epstats.toolkit.results import results_long_to_wide, format_results\nfrom epstats.toolkit.testing import TestData\n\n# Load Test Data\ngoals = TestData.load_goals_simple_agg()\n\n# Define the experiment\nunit_type = 'test_unit_type'\nexperiment = Experiment(\n    'test-simple-metric',\n    'a',\n    [\n        SimpleMetric(1, 'Click-through Rate (CTR)', 'clicks', 'views', unit_type),\n        SimpleMetric(2, 'Conversion Rate', 'conversions', 'views', unit_type),\n        SimpleMetric(3, 'Revenue per Mille (RPM)', 'bookings', 'views', unit_type, metric_format='${:,.2f}', metric_value_multiplier=1000),\n    ],\n    [SimpleSrmCheck(1, 'SRM', 'views')],\n    unit_type=unit_type)\n\n# Evaluate the experiment\nev = experiment.evaluate_wide_agg(goals)\n\n# Work with results\nprint(ev.exposures)\nprint(ev.metrics)\nprint(ev.checks)\n\n# Possible formatting of metrics\nev.metrics.pipe(results_long_to_wide).pipe(format_results, experiment, format_pct='{:.1%}', format_pval='{:.3f}')\n</code></pre> <p>Input dataframe example: <pre><code>experiment_id   variant_id  views   clicks  conversions     bookings    bookings_squared\nmy-exp          a           473661  48194   413             17152       803105\nmy-exp          b           471485  47184   360             14503       677178\nmy-exp          c           477159  48841   406             15892       711661\nmy-exp          d           474934  49090   289             11995       566700\n</code></pre></p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def evaluate_wide_agg(self, goals: pd.DataFrame) -&gt; Evaluation:\n    \"\"\"\n    This is a simplified version of the method [`evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg].\n\n    It consumes simple input `goals` dataframe, transfers it into suitable dataframe format and evaluate it using general method [`evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg].\n\n    It assumes that the first two columns are name of the experiment and variants. Than follows columns with data.\n\n    See usage of the method in the notebook [Ad-hoc A/B test evaluation using Ep-Stats][ad-hoc-ab-test-evaluation-using-ep-stats].\n\n    Arguments:\n        goals: dataframe with one row per variant and aggregated data in columns\n\n    Possible `goals` dataframe columns (check the input dataframe example):\n\n    1. `exp_id` - experiment id\n    1. `exp_variant_id` - variant\n    1. `clicks` - sum of clicks\n    1. `views` - sum of views\n    1. `bookings` - sum of bookings\n    1. `bookings_squared` - sum of bookings squared\n\n    Returns:\n        set of dataframes with evaluation\n\n    Usage:\n\n    ```python\n    from epstats.toolkit import Experiment, SimpleMetric, SimpleSrmCheck\n    from epstats.toolkit.results import results_long_to_wide, format_results\n    from epstats.toolkit.testing import TestData\n\n    # Load Test Data\n    goals = TestData.load_goals_simple_agg()\n\n    # Define the experiment\n    unit_type = 'test_unit_type'\n    experiment = Experiment(\n        'test-simple-metric',\n        'a',\n        [\n            SimpleMetric(1, 'Click-through Rate (CTR)', 'clicks', 'views', unit_type),\n            SimpleMetric(2, 'Conversion Rate', 'conversions', 'views', unit_type),\n            SimpleMetric(3, 'Revenue per Mille (RPM)', 'bookings', 'views', unit_type, metric_format='${:,.2f}', metric_value_multiplier=1000),\n        ],\n        [SimpleSrmCheck(1, 'SRM', 'views')],\n        unit_type=unit_type)\n\n    # Evaluate the experiment\n    ev = experiment.evaluate_wide_agg(goals)\n\n    # Work with results\n    print(ev.exposures)\n    print(ev.metrics)\n    print(ev.checks)\n\n    # Possible formatting of metrics\n    ev.metrics.pipe(results_long_to_wide).pipe(format_results, experiment, format_pct='{:.1%}', format_pval='{:.3f}')\n    ```\n\n    Input dataframe example:\n    ```\n    experiment_id   variant_id  views   clicks  conversions     bookings    bookings_squared\n    my-exp          a           473661  48194   413             17152       803105\n    my-exp          b           471485  47184   360             14503       677178\n    my-exp          c           477159  48841   406             15892       711661\n    my-exp          d           474934  49090   289             11995       566700\n    ```\n    \"\"\"\n    g = goals_wide_to_long(goals, self.unit_type)\n    return self.evaluate_agg(g)\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment.get_dimension_columns","title":"<code>get_dimension_columns()</code>","text":"<p>Returns a list of all dimensions used in all metrics in the experiment.</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def get_dimension_columns(self) -&gt; List[str]:\n    \"\"\"\n    Returns a list of all dimensions used in all metrics in the experiment.\n    \"\"\"\n    return list({d for g in self.get_goals() for d in g.dimension_to_value.keys()})\n</code></pre>"},{"location":"api/experiment.html#epstats.toolkit.Experiment.get_goals","title":"<code>get_goals()</code>","text":"<p>List of all goals needed to evaluate all metrics and checks in the experiment.</p> <p>Returns:</p> Type Description <code>List[EpGoal]</code> <p>list of parsed structured goals</p> Source code in <code>src/epstats/toolkit/experiment.py</code> <pre><code>def get_goals(self) -&gt; List[EpGoal]:\n    \"\"\"\n    List of all goals needed to evaluate all metrics and checks in the experiment.\n\n    Returns:\n        list of parsed structured goals\n    \"\"\"\n    res = set()\n    for m in self.metrics:\n        res = res.union(m.get_goals())\n    for c in self.checks:\n        res = res.union(c.get_goals())\n    res = res.union(self._exposure_goals)\n    return list(res)\n</code></pre>"},{"location":"api/metric.html","title":"Metric","text":"<p>Definition of a metric to evaluate in an experiment.</p> Source code in <code>src/epstats/toolkit/metric.py</code> <pre><code>class Metric:\n    \"\"\"\n    Definition of a metric to evaluate in an experiment.\n    \"\"\"\n\n    def __init__(\n        self,\n        id: int,\n        name: str,\n        nominator: str,\n        denominator: str,\n        metric_format: str = \"{:.2%}\",\n        metric_value_multiplier: int = 1,\n        minimum_effect: Optional[float] = None,\n    ):\n        \"\"\"\n        Constructor of the general metric definition.\n\n        Parameters `nominator` and `denominator` specify exactly type of data and aggregation of the metric nominator\n        and denominator.\n\n        Parameters `format` and `multiplier` does not play any role in metric evaluation. They are used independently\n        after the metric evaluation.\n\n        Arguments:\n            id: metric (order) id\n            name: metric name\n            nominator: definition of nominator\n            denominator: definition of denominator\n            metric_format: specify format of the metric, e.g. '${:,.1f}' for RPM\n            metric_value_multiplier: specify multiplier, e.g. 1000 for RPM\n\n        Usage:\n\n        ```python\n        Metric(\n            1,\n            'Click-through Rate',\n            'count(test_unit_type.unit.click)',\n            'count(test_unit_type.global.exposure)')\n        ```\n        \"\"\"\n\n        self.id = id\n        self.name = name\n        self.nominator = nominator\n        self.denominator = denominator\n        self._parser = Parser(nominator, denominator)\n        self._goals = self._parser.get_goals()\n        self.metric_format = metric_format\n        self.metric_value_multiplier = metric_value_multiplier\n        self.minimum_effect = minimum_effect\n\n    def get_goals(self) -&gt; Set:\n        \"\"\"\n        Get all goals needed to evaluate the metric.\n        \"\"\"\n        return self._goals\n\n    def get_evaluate_columns_agg(self, goals: pd.DataFrame) -&gt; np.array:\n        \"\"\"\n        Get `count`, `sum_value`, `sum_value_sqr` numpy array of shape (variants, metrics) after\n        evaluating nominator and denominator expressions from pre-aggregated goals.\n\n        Arguments:\n            goals: one row per experiment variant\n\n        See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for details\n        on `goals` at input.\n\n        Returns:\n            numpy array of shape (variants, metrics) where metrics are in order of\n            (count, sum_value, sum_sqr_value)\n        \"\"\"\n        return self._parser.evaluate_agg(goals)\n\n    def get_evaluate_columns_by_unit(self, goals: pd.DataFrame) -&gt; np.array:\n        \"\"\"\n        Get `count`, `sum_value`, `sum_value_sqr` numpy array of shape (variants, metrics) after\n        evaluating nominator and denominator expressions from goals aggregated by unit.\n\n        Arguments:\n            goals: one row per experiment variant\n\n        See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for details\n        on `goals` at input.\n\n        Returns:\n            numpy array of shape (variants, metrics) where metrics are in order of\n            (count, sum_value, sum_sqr_value)\n        \"\"\"\n        return self._parser.evaluate_by_unit(goals)\n</code></pre>"},{"location":"api/metric.html#epstats.toolkit.Metric.__init__","title":"<code>__init__(id, name, nominator, denominator, metric_format='{:.2%}', metric_value_multiplier=1, minimum_effect=None)</code>","text":"<p>Constructor of the general metric definition.</p> <p>Parameters <code>nominator</code> and <code>denominator</code> specify exactly type of data and aggregation of the metric nominator and denominator.</p> <p>Parameters <code>format</code> and <code>multiplier</code> does not play any role in metric evaluation. They are used independently after the metric evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>metric (order) id</p> required <code>name</code> <code>str</code> <p>metric name</p> required <code>nominator</code> <code>str</code> <p>definition of nominator</p> required <code>denominator</code> <code>str</code> <p>definition of denominator</p> required <code>metric_format</code> <code>str</code> <p>specify format of the metric, e.g. '${:,.1f}' for RPM</p> <code>'{:.2%}'</code> <code>metric_value_multiplier</code> <code>int</code> <p>specify multiplier, e.g. 1000 for RPM</p> <code>1</code> <p>Usage:</p> <pre><code>Metric(\n    1,\n    'Click-through Rate',\n    'count(test_unit_type.unit.click)',\n    'count(test_unit_type.global.exposure)')\n</code></pre> Source code in <code>src/epstats/toolkit/metric.py</code> <pre><code>def __init__(\n    self,\n    id: int,\n    name: str,\n    nominator: str,\n    denominator: str,\n    metric_format: str = \"{:.2%}\",\n    metric_value_multiplier: int = 1,\n    minimum_effect: Optional[float] = None,\n):\n    \"\"\"\n    Constructor of the general metric definition.\n\n    Parameters `nominator` and `denominator` specify exactly type of data and aggregation of the metric nominator\n    and denominator.\n\n    Parameters `format` and `multiplier` does not play any role in metric evaluation. They are used independently\n    after the metric evaluation.\n\n    Arguments:\n        id: metric (order) id\n        name: metric name\n        nominator: definition of nominator\n        denominator: definition of denominator\n        metric_format: specify format of the metric, e.g. '${:,.1f}' for RPM\n        metric_value_multiplier: specify multiplier, e.g. 1000 for RPM\n\n    Usage:\n\n    ```python\n    Metric(\n        1,\n        'Click-through Rate',\n        'count(test_unit_type.unit.click)',\n        'count(test_unit_type.global.exposure)')\n    ```\n    \"\"\"\n\n    self.id = id\n    self.name = name\n    self.nominator = nominator\n    self.denominator = denominator\n    self._parser = Parser(nominator, denominator)\n    self._goals = self._parser.get_goals()\n    self.metric_format = metric_format\n    self.metric_value_multiplier = metric_value_multiplier\n    self.minimum_effect = minimum_effect\n</code></pre>"},{"location":"api/metric.html#epstats.toolkit.Metric.get_evaluate_columns_agg","title":"<code>get_evaluate_columns_agg(goals)</code>","text":"<p>Get <code>count</code>, <code>sum_value</code>, <code>sum_value_sqr</code> numpy array of shape (variants, metrics) after evaluating nominator and denominator expressions from pre-aggregated goals.</p> <p>Parameters:</p> Name Type Description Default <code>goals</code> <code>DataFrame</code> <p>one row per experiment variant</p> required <p>See <code>Experiment.evaluate_agg</code> for details on <code>goals</code> at input.</p> <p>Returns:</p> Type Description <code>array</code> <p>numpy array of shape (variants, metrics) where metrics are in order of</p> <code>array</code> <p>(count, sum_value, sum_sqr_value)</p> Source code in <code>src/epstats/toolkit/metric.py</code> <pre><code>def get_evaluate_columns_agg(self, goals: pd.DataFrame) -&gt; np.array:\n    \"\"\"\n    Get `count`, `sum_value`, `sum_value_sqr` numpy array of shape (variants, metrics) after\n    evaluating nominator and denominator expressions from pre-aggregated goals.\n\n    Arguments:\n        goals: one row per experiment variant\n\n    See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_agg] for details\n    on `goals` at input.\n\n    Returns:\n        numpy array of shape (variants, metrics) where metrics are in order of\n        (count, sum_value, sum_sqr_value)\n    \"\"\"\n    return self._parser.evaluate_agg(goals)\n</code></pre>"},{"location":"api/metric.html#epstats.toolkit.Metric.get_evaluate_columns_by_unit","title":"<code>get_evaluate_columns_by_unit(goals)</code>","text":"<p>Get <code>count</code>, <code>sum_value</code>, <code>sum_value_sqr</code> numpy array of shape (variants, metrics) after evaluating nominator and denominator expressions from goals aggregated by unit.</p> <p>Parameters:</p> Name Type Description Default <code>goals</code> <code>DataFrame</code> <p>one row per experiment variant</p> required <p>See <code>Experiment.evaluate_agg</code> for details on <code>goals</code> at input.</p> <p>Returns:</p> Type Description <code>array</code> <p>numpy array of shape (variants, metrics) where metrics are in order of</p> <code>array</code> <p>(count, sum_value, sum_sqr_value)</p> Source code in <code>src/epstats/toolkit/metric.py</code> <pre><code>def get_evaluate_columns_by_unit(self, goals: pd.DataFrame) -&gt; np.array:\n    \"\"\"\n    Get `count`, `sum_value`, `sum_value_sqr` numpy array of shape (variants, metrics) after\n    evaluating nominator and denominator expressions from goals aggregated by unit.\n\n    Arguments:\n        goals: one row per experiment variant\n\n    See [`Experiment.evaluate_agg`][epstats.toolkit.experiment.Experiment.evaluate_by_unit] for details\n    on `goals` at input.\n\n    Returns:\n        numpy array of shape (variants, metrics) where metrics are in order of\n        (count, sum_value, sum_sqr_value)\n    \"\"\"\n    return self._parser.evaluate_by_unit(goals)\n</code></pre>"},{"location":"api/metric.html#epstats.toolkit.Metric.get_goals","title":"<code>get_goals()</code>","text":"<p>Get all goals needed to evaluate the metric.</p> Source code in <code>src/epstats/toolkit/metric.py</code> <pre><code>def get_goals(self) -&gt; Set:\n    \"\"\"\n    Get all goals needed to evaluate the metric.\n    \"\"\"\n    return self._goals\n</code></pre>"},{"location":"api/metric.html#simple-metric","title":"Simple Metric","text":"<p>               Bases: <code>Metric</code></p> <p>Simplified metric definition to evaluate in an experiment.</p> Source code in <code>src/epstats/toolkit/metric.py</code> <pre><code>class SimpleMetric(Metric):\n    \"\"\"\n    Simplified metric definition to evaluate in an experiment.\n    \"\"\"\n\n    def __init__(\n        self,\n        id: int,\n        name: str,\n        numerator: str,\n        denominator: str,\n        unit_type: str = \"test_unit_type\",\n        metric_format: str = \"{:.2%}\",\n        metric_value_multiplier: int = 1,\n        minimum_effect: Optional[float] = None,\n    ):\n        \"\"\"\n        Constructor of the simplified metric definition.\n\n        It modifies parameters numerator and denominator in a way that it is in line with general Metric definition.\n        It adds all the niceties necessary for proper Metric format. Finally it calls constructor of the parent Metric\n        class.\n\n        Arguments:\n            id: metric (order) id\n            name: metric name\n            numerator: value (column) of the numerator\n            denominator: value (column) of the denominator\n            unit_type: unit type\n            metric_format: specify format of the metric, e.g. '${:,.1f}' for RPM\n            metric_value_multiplier: specify multiplier, e.g. 1000 for RPM\n\n        Usage:\n\n        ```python\n        SimpleMetric(\n            1,\n            'Click-through Rate',\n            'clicks',\n            'views',\n            unit_type='test_unit_type',\n            metric_format='{:.2%}',\n            metric_value_multiplier=1)\n        ```\n        \"\"\"\n        agg_type = \"global\"  # technical parameter; it has no impact\n        num = \"value\" + \"(\" + unit_type + \".\" + agg_type + \".\" + numerator + \")\"\n        den = \"value\" + \"(\" + unit_type + \".\" + agg_type + \".\" + denominator + \")\"\n\n        super().__init__(id, name, num, den, metric_format, metric_value_multiplier, minimum_effect)\n</code></pre>"},{"location":"api/metric.html#epstats.toolkit.SimpleMetric.__init__","title":"<code>__init__(id, name, numerator, denominator, unit_type='test_unit_type', metric_format='{:.2%}', metric_value_multiplier=1, minimum_effect=None)</code>","text":"<p>Constructor of the simplified metric definition.</p> <p>It modifies parameters numerator and denominator in a way that it is in line with general Metric definition. It adds all the niceties necessary for proper Metric format. Finally it calls constructor of the parent Metric class.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>metric (order) id</p> required <code>name</code> <code>str</code> <p>metric name</p> required <code>numerator</code> <code>str</code> <p>value (column) of the numerator</p> required <code>denominator</code> <code>str</code> <p>value (column) of the denominator</p> required <code>unit_type</code> <code>str</code> <p>unit type</p> <code>'test_unit_type'</code> <code>metric_format</code> <code>str</code> <p>specify format of the metric, e.g. '${:,.1f}' for RPM</p> <code>'{:.2%}'</code> <code>metric_value_multiplier</code> <code>int</code> <p>specify multiplier, e.g. 1000 for RPM</p> <code>1</code> <p>Usage:</p> <pre><code>SimpleMetric(\n    1,\n    'Click-through Rate',\n    'clicks',\n    'views',\n    unit_type='test_unit_type',\n    metric_format='{:.2%}',\n    metric_value_multiplier=1)\n</code></pre> Source code in <code>src/epstats/toolkit/metric.py</code> <pre><code>def __init__(\n    self,\n    id: int,\n    name: str,\n    numerator: str,\n    denominator: str,\n    unit_type: str = \"test_unit_type\",\n    metric_format: str = \"{:.2%}\",\n    metric_value_multiplier: int = 1,\n    minimum_effect: Optional[float] = None,\n):\n    \"\"\"\n    Constructor of the simplified metric definition.\n\n    It modifies parameters numerator and denominator in a way that it is in line with general Metric definition.\n    It adds all the niceties necessary for proper Metric format. Finally it calls constructor of the parent Metric\n    class.\n\n    Arguments:\n        id: metric (order) id\n        name: metric name\n        numerator: value (column) of the numerator\n        denominator: value (column) of the denominator\n        unit_type: unit type\n        metric_format: specify format of the metric, e.g. '${:,.1f}' for RPM\n        metric_value_multiplier: specify multiplier, e.g. 1000 for RPM\n\n    Usage:\n\n    ```python\n    SimpleMetric(\n        1,\n        'Click-through Rate',\n        'clicks',\n        'views',\n        unit_type='test_unit_type',\n        metric_format='{:.2%}',\n        metric_value_multiplier=1)\n    ```\n    \"\"\"\n    agg_type = \"global\"  # technical parameter; it has no impact\n    num = \"value\" + \"(\" + unit_type + \".\" + agg_type + \".\" + numerator + \")\"\n    den = \"value\" + \"(\" + unit_type + \".\" + agg_type + \".\" + denominator + \")\"\n\n    super().__init__(id, name, num, den, metric_format, metric_value_multiplier, minimum_effect)\n</code></pre>"},{"location":"api/statistics.html","title":"Statistics","text":"<p>Various methods needed to evaluate experiment.</p> Source code in <code>src/epstats/toolkit/statistics.py</code> <pre><code>class Statistics:\n    \"\"\"\n    Various methods needed to evaluate experiment.\n    \"\"\"\n\n    @classmethod\n    def ttest_evaluation(cls, stats: np.array, control_variant: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Testing statistical significance of relative difference in means of treatment and control variant.\n\n        This is inspired by [scipy.stats.ttest_ind_from_stats](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html)\n        method that returns many more statistics than p-value and test statistic.\n\n        Statistics used:\n\n        1. [Welch's t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test)\n        1. [Welch\u2013Satterthwaite equation](https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation)\n        approximation of degrees of freedom.\n\n        Arguments:\n            stats: array with dimensions (metrics, variants, stats)\n            control_variant: string with the name of control variant\n\n        `stats` array values:\n\n        0. `metric_id`\n        1. `metric_name`\n        1. `exp_variant_id`\n        1. `count`\n        1. `mean`\n        1. `std`\n        1. `sum_value`\n        1. `sum_sqr_value`\n\n        Returns:\n            dataframe containing statistics from the t-test\n\n        Schema of returned dataframe:\n\n        1. `metric_id` - metric id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n        1. `metric_name` - metric name as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n        1. `exp_variant_id` - variant id\n        1. `count` - number of exposures, value of metric denominator\n        1. `mean` - `sum_value` / `count`\n        1. `std` - sample standard deviation\n        1. `sum_value` - value of goals, value of metric nominator\n        1. `confidence_level` - current confidence level used to calculate `p_value` and `confidence_interval`\n        1. `diff` - relative diff between sample means of this and control variant\n        1. `test_stat` - value of test statistic of the relative difference in means\n        1. `p_value` - p-value of the test statistic under current `confidence_level`\n        1. `confidence_interval` - confidence interval of the `diff` under current `confidence_level`\n        1. `standard_error` - standard error of the `diff`\n        1. `degrees_of_freedom` - degrees of freedom of this variant mean\n        \"\"\"\n        stats = stats.transpose(1, 2, 0)\n\n        stat_res = []  # semiresults\n        variants_count = stats.shape[0]  # number of variants\n\n        # get only stats (not metric_id, metric_name, exp_variant_id) from the stats array as floats\n        stats_values = stats[:, 3:8, :].astype(float)\n\n        # select stats data for control variant\n        for s in stats:\n            if s[2][0] == control_variant:\n                stats_values_control_variant = s[3:8, :].astype(float)\n                break\n\n        # control variant values\n        count_cont = stats_values_control_variant[0]  # number of observations\n        mean_cont = stats_values_control_variant[1]  # mean\n        std_cont = stats_values_control_variant[2]  # standard deviation\n        conf_level = stats_values_control_variant[4]  # confidence level\n\n        # this for loop goes over variants and compares one variant values against control variant values for\n        # all metrics at once. Similar to scipy.stats.ttest_ind_from_stats\n        for i in range(variants_count):\n            # treatment variant data\n            s = stats_values[i]\n            count_treat = s[0]  # number of observations\n            mean_treat = s[1]  # mean\n            std_treat = s[2]  # standard deviation\n            sum_value = s[3]  # sum of observations\n\n            # degrees of freedom\n            num = (std_cont**2 / count_cont + std_treat**2 / count_treat) ** 2\n            den = (std_cont**4 / (count_cont**2 * (count_cont - 1))) + (\n                std_treat**4 / (count_treat**2 * (count_treat - 1))\n            )\n\n            with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n                # We fill in zeros, when goal data are missing for some variant.\n                # There could be division by zero here which is expected as we return\n                # nan or inf values to the caller.\n                # np.round() in case of roundoff errors, e.g. f = 9.999999998 =&gt; trunc(round(f, 5)) = 10\n                f = np.trunc(np.round(num / den, 5))  # (rounded &amp; truncated) degrees of freedom\n\n            # t-quantile\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n                t_quantile = st.t.ppf(conf_level + (1 - conf_level) / 2, f)  # right quantile\n\n            # relative difference and test statistics\n            with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n                # We fill in zeros, when goal data are missing for some variant.\n                # There could be division by zero here which is expected as we return\n                # nan or inf values to the caller.\n                rel_diff = (mean_treat - mean_cont) / np.abs(mean_cont)\n                # standard error for relative difference\n                rel_se = (\n                    np.sqrt((mean_treat * std_cont) ** 2 / (mean_cont**2 * count_cont) + (std_treat**2 / count_treat))\n                    / mean_cont\n                )\n                test_stat = rel_diff / rel_se\n\n                # If test_stat is inf and sum of non-zero observations is low,\n                # set test_stat to 0 to prevent p-value from being 0.\n                if test_stat.shape == sum_value.shape:\n                    mask = np.isinf(test_stat) &amp; (sum_value &lt;= 10)\n                    test_stat[mask] = 0.0\n\n            # p-value\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n                pval = 2 * (1 - st.t.cdf(np.abs(test_stat), f))\n\n            # confidence interval\n            conf_int = rel_se * t_quantile\n\n            # save results\n            stat_res.append((rel_diff, test_stat, pval, conf_int, rel_se, f))\n\n        # Tune up results\n        s = np.hstack([stats, stat_res])\n        s = s.transpose(2, 0, 1)  # move back to metrics, variants, stats order\n        x, y, z = s.shape\n        arr = s.reshape(x * y, z)\n\n        # Output dataframe\n        col = [\n            \"metric_id\",\n            \"metric_name\",\n            \"exp_variant_id\",\n            \"count\",\n            \"mean\",\n            \"std\",\n            \"sum_value\",\n            \"confidence_level\",\n            \"diff\",\n            \"test_stat\",\n            \"p_value\",\n            \"confidence_interval\",\n            \"standard_error\",\n            \"degrees_of_freedom\",\n        ]\n        r = pd.DataFrame(arr, columns=col)\n        return r\n\n    @classmethod\n    def multiple_comparisons_correction(\n        cls, df: pd.DataFrame, n_variants: int, metrics: int, confidence_level: float\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        [Holm-Bonferroni correction](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method)\n        for multiple comparisons problem. It is applied when we have more than two variants,\n        i.e. we have one control variant and at least two treatment variants.\n\n        It adjusts p-value and length of confidence interval - both to be more conservative.\n        [Complete manual](../stats/multiple.md)\n\n        Algorithm:\n\n        For each metric, select (unadjusted) p-values and replace them with adjusted ones.\n        Based on adjustment ratio, compute new (adjusted) confidence intervals and replace\n        old (unadjusted) ones.\n\n        Arguments:\n            df: dataframe as output of [`ttest_evaluation`][epstats.toolkit.statistics.Statistics.ttest_evaluation]\n            n_variants: number of variants in the experiment\n            metrics: number of metrics of experiment\n            confidence_level: desired confidence level at the end of the experiment, e.g. 0.95\n\n        Returns:\n            dataframe of the same format as input with adjusted p-values and confidence intervals.\n        \"\"\"\n        alpha = 1 - confidence_level  # level of significance\n\n        for m in range(metrics):\n            # indices of rows with metric m data\n            index_from = m * n_variants + 1\n            index_to = (m + 1) * n_variants - 1\n\n            # p-value adjustment\n            pvals = df.loc[index_from:index_to, \"p_value\"].to_list()  # select old p-values\n            adj_pvals = multipletests(pvals=pvals, alpha=alpha, method=\"holm\")[1]  # compute adjusted p-values\n\n            # confidence interval adjustment\n            # we set ratio to 1 when test_stat is so big that pvals are zero, no reason to update ci\n            adj_ratio = np.nan_to_num(pvals / adj_pvals, nan=1)  # adjustment ratio\n            adj_alpha = adj_ratio * alpha  # adjusted level alpha\n\n            f = df.loc[index_from:index_to, \"degrees_of_freedom\"].to_list()  # degrees of freedom\n            se = df.loc[index_from:index_to, \"standard_error\"].to_list()  # standard error\n\n            t_quantile = st.t.ppf(np.ones(n_variants - 1) - adj_alpha + adj_alpha / 2, f)  # right t-quantile\n            adj_conf_int = se * t_quantile  # adjusted confidence interval\n\n            # replace (unadjusted) p-values and confidence intervals with new adjusted ones\n            df.loc[index_from:index_to, \"p_value\"] = adj_pvals\n            df.loc[index_from:index_to, \"confidence_interval\"] = adj_conf_int\n        return df\n\n    @classmethod\n    def obf_alpha_spending_function(cls, confidence_level: int, total_length: int, actual_day: int) -&gt; int:\n        \"\"\"\n        [O'Brien-Fleming alpha spending function](https://online.stat.psu.edu/stat509/lesson/9/9.6/).\n        We adjust confidence level in time in experiment. Confidence level in this setting is\n        a decreasing function of experiment time. See [Sequential Analysis](../stats/sequential.md) for details.\n\n        Arguments:\n            confidence_level: required confidence level at the end of the test, e.g. 0.95\n            total_length: length of the test in days, e.g. 7, 14, 21\n            actual_day: actual days in the experiment period, must be between 1 and `total_length`\n\n        Returns:\n            adjusted confidence level with respect to actual day of the experiment and total\n            length of the experiment.\n        \"\"\"\n        alpha = 1 - confidence_level\n        t = actual_day / total_length  # t in (0, 1]\n        q = st.norm.ppf(1 - alpha / 2)  # quantile of normal distribution\n        alpha_adj = 2 - 2 * st.norm.cdf(q / np.sqrt(t))\n        return np.round(1 - alpha_adj, decimals=4)\n\n    @staticmethod\n    def required_sample_size_per_variant(\n        n_variants: int,\n        minimum_effect: float,\n        mean: float,\n        std: float,\n        std_2: Optional[float] = None,\n        confidence_level: float = DEFAULT_CONFIDENCE_LEVEL,\n        power: float = DEFAULT_POWER,\n    ) -&gt; Union[int, float]:\n        \"\"\"\n        Computes the sample size required to reach the defined `confidence_level` and `power`.\n\n        Uses the following formula:\n\n        $$\n        N = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{1-\\\\beta})^2(s_1^2 + s_2^2)}{\\\\Delta^2}\n        $$\n\n        where $\\\\Delta = \\\\mathrm{MEI}\\\\mu_1$. When `std_2` is unknown,\n        we assume equal variance $s_1^2 = s_2^2$:\n\n        $$\n        N = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{1-\\\\beta})^2 2s_1^2}{\\\\Delta^2}\n        $$\n\n        For `confidence_level = 0.95` and `power = 0.8`:\n        $$\n        N = \\\\frac{7.84 * 2s_1^2}{\\\\Delta^2} = \\\\frac{15.7s_1^2}{\\\\Delta^2}\n        $$\n\n        The calculation is using Bonferroni correction when `n_variants &gt; 2`. The initial\n        $\\\\alpha$ defined by the `confidence_level` parameter is adjusted to\n\n        $$\n        \\\\alpha^{*} = \\\\alpha / m\n        $$\n\n        where $m$ is the number of treatment variants. This correction produces\n        greater total sample size than Holm-Bonferroni correction because it assigns\n        the most conservative $\\\\alpha^{*}$ to all variants.\n\n        Arguments:\n            n_variants: number of variants in the experiment\n            minimum_effect: minimum (relative) effect that we find meaningful to detect\n            mean: estimate of the current population mean, also known as rate in case of Bernoulli distribution\n            std: estimate of the current population standard deviation\n            std_2: estimate of the treatment population standard deviation\n            confidence_level: confidence level of the test\n            power: power of the test\n\n        Returns:\n            required sample size\n        \"\"\"\n\n        if minimum_effect &lt; 0:\n            raise ValueError(\"minimum_effect must be greater than zero.\")\n\n        if n_variants &lt; 2:\n            raise ValueError(\"There must be at least two variants.\")\n\n        two_vars = 2 * (std**2) if std_2 is None else (std**2 + std_2**2)\n        delta = np.float64(mean * minimum_effect)\n\n        alpha = 1 - confidence_level\n        m = n_variants - 1\n        alpha = alpha / m  # Bonferroni correction\n        # 7.84 for 80% power and 95% confidence, alpha / 2 for two-sided hypothesis\n        confidence_and_power = (st.norm.ppf(1 - alpha / 2) + st.norm.ppf(power)) ** 2\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            samples_size_per_variant = confidence_and_power * (two_vars / delta**2)\n        return np.round(samples_size_per_variant)\n\n    @classmethod\n    def required_sample_size_per_variant_bernoulli(\n        cls,\n        n_variants: int,\n        minimum_effect: float,\n        mean: float,\n        confidence_level: float = DEFAULT_CONFIDENCE_LEVEL,\n        power: float = DEFAULT_POWER,\n        **unused_kwargs,\n    ) -&gt; Union[int, float]:\n        \"\"\"\n        Computes the sample size required to reach the defined `confidence_level`\n        and `power` when the data follow Bernoulli distribution.\n\n        Uses `Statistics.required_sample_size_per_variant` with `std_2` defined as\n\n        $$\n        p_2 = p_1(1 + \\\\mathrm{MEI}) \\\\\\\\\n        s_2^2 = p_2(1 - p_2) \\\\\\\\\n        $$\n\n        Arguments:\n            n_variants: number of variants in the experiment\n            minimum_effect: minimum (relative) effect that we find meaningful to detect\n            mean: estimate of the current population mean,\n                  also known as rate in case of Bernoulli distribution\n            confidence_level: confidence level of the test\n            power: power of the test\n\n        Returns:\n            required sample size\n        \"\"\"\n\n        if mean &gt; 1 or mean &lt; 0:\n            raise ValueError(f\"mean must be between zero and one, received {mean}.\")\n\n        # if we know minimum effect, we know treatment mean and treatment variance\n        # see https://github.com/bookingcom/powercalculator/blob/master/src/js/math.js#L113\n\n        def get_std(mean):\n            return np.sqrt(mean * (1 - mean))\n\n        mean_2 = mean * (1 + minimum_effect)\n\n        return cls.required_sample_size_per_variant(\n            n_variants=n_variants,\n            minimum_effect=minimum_effect,\n            mean=mean,\n            std=get_std(mean),\n            std_2=get_std(mean_2),\n            confidence_level=confidence_level,\n            power=power,\n        )\n\n    @staticmethod\n    def power_from_required_sample_size_per_variant(\n        n_variants: int,\n        sample_size_per_variant: Union[int, float],\n        required_sample_size_per_variant: Union[int, float],\n        required_power: float = DEFAULT_POWER,\n        required_confidence_level: float = DEFAULT_CONFIDENCE_LEVEL,\n    ) -&gt; float:\n        \"\"\"\n        Computes power based on the ratio of `sample_size_per_variant`\n        and `required_sample_size_per_variant`.\n\n        How does it work? Consider the formula for computing the sample size $N$\n        for a given $\\\\alpha$ and $1-\\\\beta$:\n\n        $$\n        N = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{1-\\\\beta})^2(s_1^2 + s_2^2)}{\\\\Delta^2}\n        $$\n\n        We can define the required sample size $N_R$ to reach 80% power as\n\n        $$\n        N_r = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{0.8})^2(s_1^2 + s_2^2)}{\\\\Delta^2}\n        $$\n\n        The ratio $\\\\frac{N}{N_r}$ simplifies to\n\n        $$\n        \\\\frac{N}{N_r} = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{1-\\\\beta})^2}{(Z_{1-\\\\alpha/2} + Z_{0.8})^2}\n        $$\n\n        This means that the power can be computed as\n\n        $$\n        Z_{1-\\\\beta} = \\\\sqrt\\\\frac{N}{N_r}(Z_{1-\\\\alpha/2}+Z_{0.8})-Z_{1-\\\\alpha/2} \\\\\\\\\n        1-\\\\beta = \\\\Phi(Z_{1-\\\\beta})\n        $$\n\n        Arguments:\n            n_variants: number of variants in the experiment\n            sample_size_per_variant: number of samples in one variant\n            required_sample_size_per_variant: number of samples required to reach the\n                                              `required_power` using the `required_confidence_level`\n            required_confidence_level: confidence level used to compute the\n                                       `required_sample_size_per_variant`\n            required_power: power used to compute the `required_sample_size_per_variant`\n\n        Returns:\n            power\n        \"\"\"\n\n        if n_variants &lt; 2 or required_sample_size_per_variant == 0:\n            return np.nan\n\n        required_sample_size_ratio = sample_size_per_variant / required_sample_size_per_variant\n        alpha = (1 - required_confidence_level) / (n_variants - 1)\n\n        return st.norm.cdf(\n            np.sqrt(required_sample_size_ratio) * (st.norm.ppf(1 - alpha / 2) + st.norm.ppf(required_power))\n            - st.norm.ppf(1 - alpha / 2)\n        )\n\n    @staticmethod\n    def false_positive_risk(\n        null_hypothesis_rate: float,\n        power: float,\n        p_value: float,\n    ) -&gt; float:\n        \"\"\"\n        Computes false positive risk defined as:\n\n        $$\n        P(H_0|S) = \\\\frac{P(S|H_0)P(H_0)}{P(S)} = \\\\frac{\\\\alpha\\\\pi}{\\\\alpha\\\\pi + (1 - \\\\beta)(1 - \\\\pi)}\n        $$\n\n        where $S$ is a statisically significant outcome, $H_0$ is the null hypothesis, $1 - \\\\beta$\n        is the power of a test, and $\\\\pi$ is the global null hypothesis rate defined as the proportion\n        of all tests in an experimentation program that have not improved or degraded the primary metric.\n\n        False positive risk $P(H_0|S)$ is not the same as the false positive rate $P(S|H_0) = \\\\alpha$.\n\n        More information can be found in the paper: https://bit.ly/ABTestingIntuitionBusters.\n\n        Arguments:\n            null_hypothesis_rate: global null hypothesis rate of the experimanation program\n            current_power: power achieved in the test\n            confidence_level: confidence level of the test\n\n        Returns:\n            false positive risk\n        \"\"\"\n\n        pi = null_hypothesis_rate\n        return (p_value * pi) / (p_value * pi + power * (1 - pi))\n</code></pre>"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.false_positive_risk","title":"<code>false_positive_risk(null_hypothesis_rate, power, p_value)</code>  <code>staticmethod</code>","text":"<p>Computes false positive risk defined as:</p> \\[ P(H_0|S) = \\frac{P(S|H_0)P(H_0)}{P(S)} = \\frac{\\alpha\\pi}{\\alpha\\pi + (1 - \\beta)(1 - \\pi)} \\] <p>where \\(S\\) is a statisically significant outcome, \\(H_0\\) is the null hypothesis, \\(1 - \\beta\\) is the power of a test, and \\(\\pi\\) is the global null hypothesis rate defined as the proportion of all tests in an experimentation program that have not improved or degraded the primary metric.</p> <p>False positive risk \\(P(H_0|S)\\) is not the same as the false positive rate \\(P(S|H_0) = \\alpha\\).</p> <p>More information can be found in the paper: https://bit.ly/ABTestingIntuitionBusters.</p> <p>Parameters:</p> Name Type Description Default <code>null_hypothesis_rate</code> <code>float</code> <p>global null hypothesis rate of the experimanation program</p> required <code>current_power</code> <p>power achieved in the test</p> required <code>confidence_level</code> <p>confidence level of the test</p> required <p>Returns:</p> Type Description <code>float</code> <p>false positive risk</p> Source code in <code>src/epstats/toolkit/statistics.py</code> <pre><code>@staticmethod\ndef false_positive_risk(\n    null_hypothesis_rate: float,\n    power: float,\n    p_value: float,\n) -&gt; float:\n    \"\"\"\n    Computes false positive risk defined as:\n\n    $$\n    P(H_0|S) = \\\\frac{P(S|H_0)P(H_0)}{P(S)} = \\\\frac{\\\\alpha\\\\pi}{\\\\alpha\\\\pi + (1 - \\\\beta)(1 - \\\\pi)}\n    $$\n\n    where $S$ is a statisically significant outcome, $H_0$ is the null hypothesis, $1 - \\\\beta$\n    is the power of a test, and $\\\\pi$ is the global null hypothesis rate defined as the proportion\n    of all tests in an experimentation program that have not improved or degraded the primary metric.\n\n    False positive risk $P(H_0|S)$ is not the same as the false positive rate $P(S|H_0) = \\\\alpha$.\n\n    More information can be found in the paper: https://bit.ly/ABTestingIntuitionBusters.\n\n    Arguments:\n        null_hypothesis_rate: global null hypothesis rate of the experimanation program\n        current_power: power achieved in the test\n        confidence_level: confidence level of the test\n\n    Returns:\n        false positive risk\n    \"\"\"\n\n    pi = null_hypothesis_rate\n    return (p_value * pi) / (p_value * pi + power * (1 - pi))\n</code></pre>"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.multiple_comparisons_correction","title":"<code>multiple_comparisons_correction(df, n_variants, metrics, confidence_level)</code>  <code>classmethod</code>","text":"<p>Holm-Bonferroni correction for multiple comparisons problem. It is applied when we have more than two variants, i.e. we have one control variant and at least two treatment variants.</p> <p>It adjusts p-value and length of confidence interval - both to be more conservative. Complete manual</p> <p>Algorithm:</p> <p>For each metric, select (unadjusted) p-values and replace them with adjusted ones. Based on adjustment ratio, compute new (adjusted) confidence intervals and replace old (unadjusted) ones.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe as output of <code>ttest_evaluation</code></p> required <code>n_variants</code> <code>int</code> <p>number of variants in the experiment</p> required <code>metrics</code> <code>int</code> <p>number of metrics of experiment</p> required <code>confidence_level</code> <code>float</code> <p>desired confidence level at the end of the experiment, e.g. 0.95</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>dataframe of the same format as input with adjusted p-values and confidence intervals.</p> Source code in <code>src/epstats/toolkit/statistics.py</code> <pre><code>@classmethod\ndef multiple_comparisons_correction(\n    cls, df: pd.DataFrame, n_variants: int, metrics: int, confidence_level: float\n) -&gt; pd.DataFrame:\n    \"\"\"\n    [Holm-Bonferroni correction](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method)\n    for multiple comparisons problem. It is applied when we have more than two variants,\n    i.e. we have one control variant and at least two treatment variants.\n\n    It adjusts p-value and length of confidence interval - both to be more conservative.\n    [Complete manual](../stats/multiple.md)\n\n    Algorithm:\n\n    For each metric, select (unadjusted) p-values and replace them with adjusted ones.\n    Based on adjustment ratio, compute new (adjusted) confidence intervals and replace\n    old (unadjusted) ones.\n\n    Arguments:\n        df: dataframe as output of [`ttest_evaluation`][epstats.toolkit.statistics.Statistics.ttest_evaluation]\n        n_variants: number of variants in the experiment\n        metrics: number of metrics of experiment\n        confidence_level: desired confidence level at the end of the experiment, e.g. 0.95\n\n    Returns:\n        dataframe of the same format as input with adjusted p-values and confidence intervals.\n    \"\"\"\n    alpha = 1 - confidence_level  # level of significance\n\n    for m in range(metrics):\n        # indices of rows with metric m data\n        index_from = m * n_variants + 1\n        index_to = (m + 1) * n_variants - 1\n\n        # p-value adjustment\n        pvals = df.loc[index_from:index_to, \"p_value\"].to_list()  # select old p-values\n        adj_pvals = multipletests(pvals=pvals, alpha=alpha, method=\"holm\")[1]  # compute adjusted p-values\n\n        # confidence interval adjustment\n        # we set ratio to 1 when test_stat is so big that pvals are zero, no reason to update ci\n        adj_ratio = np.nan_to_num(pvals / adj_pvals, nan=1)  # adjustment ratio\n        adj_alpha = adj_ratio * alpha  # adjusted level alpha\n\n        f = df.loc[index_from:index_to, \"degrees_of_freedom\"].to_list()  # degrees of freedom\n        se = df.loc[index_from:index_to, \"standard_error\"].to_list()  # standard error\n\n        t_quantile = st.t.ppf(np.ones(n_variants - 1) - adj_alpha + adj_alpha / 2, f)  # right t-quantile\n        adj_conf_int = se * t_quantile  # adjusted confidence interval\n\n        # replace (unadjusted) p-values and confidence intervals with new adjusted ones\n        df.loc[index_from:index_to, \"p_value\"] = adj_pvals\n        df.loc[index_from:index_to, \"confidence_interval\"] = adj_conf_int\n    return df\n</code></pre>"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.obf_alpha_spending_function","title":"<code>obf_alpha_spending_function(confidence_level, total_length, actual_day)</code>  <code>classmethod</code>","text":"<p>O'Brien-Fleming alpha spending function. We adjust confidence level in time in experiment. Confidence level in this setting is a decreasing function of experiment time. See Sequential Analysis for details.</p> <p>Parameters:</p> Name Type Description Default <code>confidence_level</code> <code>int</code> <p>required confidence level at the end of the test, e.g. 0.95</p> required <code>total_length</code> <code>int</code> <p>length of the test in days, e.g. 7, 14, 21</p> required <code>actual_day</code> <code>int</code> <p>actual days in the experiment period, must be between 1 and <code>total_length</code></p> required <p>Returns:</p> Type Description <code>int</code> <p>adjusted confidence level with respect to actual day of the experiment and total</p> <code>int</code> <p>length of the experiment.</p> Source code in <code>src/epstats/toolkit/statistics.py</code> <pre><code>@classmethod\ndef obf_alpha_spending_function(cls, confidence_level: int, total_length: int, actual_day: int) -&gt; int:\n    \"\"\"\n    [O'Brien-Fleming alpha spending function](https://online.stat.psu.edu/stat509/lesson/9/9.6/).\n    We adjust confidence level in time in experiment. Confidence level in this setting is\n    a decreasing function of experiment time. See [Sequential Analysis](../stats/sequential.md) for details.\n\n    Arguments:\n        confidence_level: required confidence level at the end of the test, e.g. 0.95\n        total_length: length of the test in days, e.g. 7, 14, 21\n        actual_day: actual days in the experiment period, must be between 1 and `total_length`\n\n    Returns:\n        adjusted confidence level with respect to actual day of the experiment and total\n        length of the experiment.\n    \"\"\"\n    alpha = 1 - confidence_level\n    t = actual_day / total_length  # t in (0, 1]\n    q = st.norm.ppf(1 - alpha / 2)  # quantile of normal distribution\n    alpha_adj = 2 - 2 * st.norm.cdf(q / np.sqrt(t))\n    return np.round(1 - alpha_adj, decimals=4)\n</code></pre>"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.power_from_required_sample_size_per_variant","title":"<code>power_from_required_sample_size_per_variant(n_variants, sample_size_per_variant, required_sample_size_per_variant, required_power=DEFAULT_POWER, required_confidence_level=DEFAULT_CONFIDENCE_LEVEL)</code>  <code>staticmethod</code>","text":"<p>Computes power based on the ratio of <code>sample_size_per_variant</code> and <code>required_sample_size_per_variant</code>.</p> <p>How does it work? Consider the formula for computing the sample size \\(N\\) for a given \\(\\alpha\\) and \\(1-\\beta\\):</p> \\[ N = \\frac{(Z_{1-\\alpha/2} + Z_{1-\\beta})^2(s_1^2 + s_2^2)}{\\Delta^2} \\] <p>We can define the required sample size \\(N_R\\) to reach 80% power as</p> \\[ N_r = \\frac{(Z_{1-\\alpha/2} + Z_{0.8})^2(s_1^2 + s_2^2)}{\\Delta^2} \\] <p>The ratio \\(\\frac{N}{N_r}\\) simplifies to</p> \\[ \\frac{N}{N_r} = \\frac{(Z_{1-\\alpha/2} + Z_{1-\\beta})^2}{(Z_{1-\\alpha/2} + Z_{0.8})^2} \\] <p>This means that the power can be computed as</p> \\[ Z_{1-\\beta} = \\sqrt\\frac{N}{N_r}(Z_{1-\\alpha/2}+Z_{0.8})-Z_{1-\\alpha/2} \\\\ 1-\\beta = \\Phi(Z_{1-\\beta}) \\] <p>Parameters:</p> Name Type Description Default <code>n_variants</code> <code>int</code> <p>number of variants in the experiment</p> required <code>sample_size_per_variant</code> <code>Union[int, float]</code> <p>number of samples in one variant</p> required <code>required_sample_size_per_variant</code> <code>Union[int, float]</code> <p>number of samples required to reach the                               <code>required_power</code> using the <code>required_confidence_level</code></p> required <code>required_confidence_level</code> <code>float</code> <p>confidence level used to compute the                        <code>required_sample_size_per_variant</code></p> <code>DEFAULT_CONFIDENCE_LEVEL</code> <code>required_power</code> <code>float</code> <p>power used to compute the <code>required_sample_size_per_variant</code></p> <code>DEFAULT_POWER</code> <p>Returns:</p> Type Description <code>float</code> <p>power</p> Source code in <code>src/epstats/toolkit/statistics.py</code> <pre><code>@staticmethod\ndef power_from_required_sample_size_per_variant(\n    n_variants: int,\n    sample_size_per_variant: Union[int, float],\n    required_sample_size_per_variant: Union[int, float],\n    required_power: float = DEFAULT_POWER,\n    required_confidence_level: float = DEFAULT_CONFIDENCE_LEVEL,\n) -&gt; float:\n    \"\"\"\n    Computes power based on the ratio of `sample_size_per_variant`\n    and `required_sample_size_per_variant`.\n\n    How does it work? Consider the formula for computing the sample size $N$\n    for a given $\\\\alpha$ and $1-\\\\beta$:\n\n    $$\n    N = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{1-\\\\beta})^2(s_1^2 + s_2^2)}{\\\\Delta^2}\n    $$\n\n    We can define the required sample size $N_R$ to reach 80% power as\n\n    $$\n    N_r = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{0.8})^2(s_1^2 + s_2^2)}{\\\\Delta^2}\n    $$\n\n    The ratio $\\\\frac{N}{N_r}$ simplifies to\n\n    $$\n    \\\\frac{N}{N_r} = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{1-\\\\beta})^2}{(Z_{1-\\\\alpha/2} + Z_{0.8})^2}\n    $$\n\n    This means that the power can be computed as\n\n    $$\n    Z_{1-\\\\beta} = \\\\sqrt\\\\frac{N}{N_r}(Z_{1-\\\\alpha/2}+Z_{0.8})-Z_{1-\\\\alpha/2} \\\\\\\\\n    1-\\\\beta = \\\\Phi(Z_{1-\\\\beta})\n    $$\n\n    Arguments:\n        n_variants: number of variants in the experiment\n        sample_size_per_variant: number of samples in one variant\n        required_sample_size_per_variant: number of samples required to reach the\n                                          `required_power` using the `required_confidence_level`\n        required_confidence_level: confidence level used to compute the\n                                   `required_sample_size_per_variant`\n        required_power: power used to compute the `required_sample_size_per_variant`\n\n    Returns:\n        power\n    \"\"\"\n\n    if n_variants &lt; 2 or required_sample_size_per_variant == 0:\n        return np.nan\n\n    required_sample_size_ratio = sample_size_per_variant / required_sample_size_per_variant\n    alpha = (1 - required_confidence_level) / (n_variants - 1)\n\n    return st.norm.cdf(\n        np.sqrt(required_sample_size_ratio) * (st.norm.ppf(1 - alpha / 2) + st.norm.ppf(required_power))\n        - st.norm.ppf(1 - alpha / 2)\n    )\n</code></pre>"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.required_sample_size_per_variant","title":"<code>required_sample_size_per_variant(n_variants, minimum_effect, mean, std, std_2=None, confidence_level=DEFAULT_CONFIDENCE_LEVEL, power=DEFAULT_POWER)</code>  <code>staticmethod</code>","text":"<p>Computes the sample size required to reach the defined <code>confidence_level</code> and <code>power</code>.</p> <p>Uses the following formula:</p> \\[ N = \\frac{(Z_{1-\\alpha/2} + Z_{1-\\beta})^2(s_1^2 + s_2^2)}{\\Delta^2} \\] <p>where \\(\\Delta = \\mathrm{MEI}\\mu_1\\). When <code>std_2</code> is unknown, we assume equal variance \\(s_1^2 = s_2^2\\):</p> \\[ N = \\frac{(Z_{1-\\alpha/2} + Z_{1-\\beta})^2 2s_1^2}{\\Delta^2} \\] <p>For <code>confidence_level = 0.95</code> and <code>power = 0.8</code>: $$ N = \\frac{7.84 * 2s_1^2}{\\Delta^2} = \\frac{15.7s_1^2}{\\Delta^2} $$</p> <p>The calculation is using Bonferroni correction when <code>n_variants &gt; 2</code>. The initial \\(\\alpha\\) defined by the <code>confidence_level</code> parameter is adjusted to</p> \\[ \\alpha^{*} = \\alpha / m \\] <p>where \\(m\\) is the number of treatment variants. This correction produces greater total sample size than Holm-Bonferroni correction because it assigns the most conservative \\(\\alpha^{*}\\) to all variants.</p> <p>Parameters:</p> Name Type Description Default <code>n_variants</code> <code>int</code> <p>number of variants in the experiment</p> required <code>minimum_effect</code> <code>float</code> <p>minimum (relative) effect that we find meaningful to detect</p> required <code>mean</code> <code>float</code> <p>estimate of the current population mean, also known as rate in case of Bernoulli distribution</p> required <code>std</code> <code>float</code> <p>estimate of the current population standard deviation</p> required <code>std_2</code> <code>Optional[float]</code> <p>estimate of the treatment population standard deviation</p> <code>None</code> <code>confidence_level</code> <code>float</code> <p>confidence level of the test</p> <code>DEFAULT_CONFIDENCE_LEVEL</code> <code>power</code> <code>float</code> <p>power of the test</p> <code>DEFAULT_POWER</code> <p>Returns:</p> Type Description <code>Union[int, float]</code> <p>required sample size</p> Source code in <code>src/epstats/toolkit/statistics.py</code> <pre><code>@staticmethod\ndef required_sample_size_per_variant(\n    n_variants: int,\n    minimum_effect: float,\n    mean: float,\n    std: float,\n    std_2: Optional[float] = None,\n    confidence_level: float = DEFAULT_CONFIDENCE_LEVEL,\n    power: float = DEFAULT_POWER,\n) -&gt; Union[int, float]:\n    \"\"\"\n    Computes the sample size required to reach the defined `confidence_level` and `power`.\n\n    Uses the following formula:\n\n    $$\n    N = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{1-\\\\beta})^2(s_1^2 + s_2^2)}{\\\\Delta^2}\n    $$\n\n    where $\\\\Delta = \\\\mathrm{MEI}\\\\mu_1$. When `std_2` is unknown,\n    we assume equal variance $s_1^2 = s_2^2$:\n\n    $$\n    N = \\\\frac{(Z_{1-\\\\alpha/2} + Z_{1-\\\\beta})^2 2s_1^2}{\\\\Delta^2}\n    $$\n\n    For `confidence_level = 0.95` and `power = 0.8`:\n    $$\n    N = \\\\frac{7.84 * 2s_1^2}{\\\\Delta^2} = \\\\frac{15.7s_1^2}{\\\\Delta^2}\n    $$\n\n    The calculation is using Bonferroni correction when `n_variants &gt; 2`. The initial\n    $\\\\alpha$ defined by the `confidence_level` parameter is adjusted to\n\n    $$\n    \\\\alpha^{*} = \\\\alpha / m\n    $$\n\n    where $m$ is the number of treatment variants. This correction produces\n    greater total sample size than Holm-Bonferroni correction because it assigns\n    the most conservative $\\\\alpha^{*}$ to all variants.\n\n    Arguments:\n        n_variants: number of variants in the experiment\n        minimum_effect: minimum (relative) effect that we find meaningful to detect\n        mean: estimate of the current population mean, also known as rate in case of Bernoulli distribution\n        std: estimate of the current population standard deviation\n        std_2: estimate of the treatment population standard deviation\n        confidence_level: confidence level of the test\n        power: power of the test\n\n    Returns:\n        required sample size\n    \"\"\"\n\n    if minimum_effect &lt; 0:\n        raise ValueError(\"minimum_effect must be greater than zero.\")\n\n    if n_variants &lt; 2:\n        raise ValueError(\"There must be at least two variants.\")\n\n    two_vars = 2 * (std**2) if std_2 is None else (std**2 + std_2**2)\n    delta = np.float64(mean * minimum_effect)\n\n    alpha = 1 - confidence_level\n    m = n_variants - 1\n    alpha = alpha / m  # Bonferroni correction\n    # 7.84 for 80% power and 95% confidence, alpha / 2 for two-sided hypothesis\n    confidence_and_power = (st.norm.ppf(1 - alpha / 2) + st.norm.ppf(power)) ** 2\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        samples_size_per_variant = confidence_and_power * (two_vars / delta**2)\n    return np.round(samples_size_per_variant)\n</code></pre>"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.required_sample_size_per_variant_bernoulli","title":"<code>required_sample_size_per_variant_bernoulli(n_variants, minimum_effect, mean, confidence_level=DEFAULT_CONFIDENCE_LEVEL, power=DEFAULT_POWER, **unused_kwargs)</code>  <code>classmethod</code>","text":"<p>Computes the sample size required to reach the defined <code>confidence_level</code> and <code>power</code> when the data follow Bernoulli distribution.</p> <p>Uses <code>Statistics.required_sample_size_per_variant</code> with <code>std_2</code> defined as</p> \\[ p_2 = p_1(1 + \\mathrm{MEI}) \\\\ s_2^2 = p_2(1 - p_2) \\\\ \\] <p>Parameters:</p> Name Type Description Default <code>n_variants</code> <code>int</code> <p>number of variants in the experiment</p> required <code>minimum_effect</code> <code>float</code> <p>minimum (relative) effect that we find meaningful to detect</p> required <code>mean</code> <code>float</code> <p>estimate of the current population mean,   also known as rate in case of Bernoulli distribution</p> required <code>confidence_level</code> <code>float</code> <p>confidence level of the test</p> <code>DEFAULT_CONFIDENCE_LEVEL</code> <code>power</code> <code>float</code> <p>power of the test</p> <code>DEFAULT_POWER</code> <p>Returns:</p> Type Description <code>Union[int, float]</code> <p>required sample size</p> Source code in <code>src/epstats/toolkit/statistics.py</code> <pre><code>@classmethod\ndef required_sample_size_per_variant_bernoulli(\n    cls,\n    n_variants: int,\n    minimum_effect: float,\n    mean: float,\n    confidence_level: float = DEFAULT_CONFIDENCE_LEVEL,\n    power: float = DEFAULT_POWER,\n    **unused_kwargs,\n) -&gt; Union[int, float]:\n    \"\"\"\n    Computes the sample size required to reach the defined `confidence_level`\n    and `power` when the data follow Bernoulli distribution.\n\n    Uses `Statistics.required_sample_size_per_variant` with `std_2` defined as\n\n    $$\n    p_2 = p_1(1 + \\\\mathrm{MEI}) \\\\\\\\\n    s_2^2 = p_2(1 - p_2) \\\\\\\\\n    $$\n\n    Arguments:\n        n_variants: number of variants in the experiment\n        minimum_effect: minimum (relative) effect that we find meaningful to detect\n        mean: estimate of the current population mean,\n              also known as rate in case of Bernoulli distribution\n        confidence_level: confidence level of the test\n        power: power of the test\n\n    Returns:\n        required sample size\n    \"\"\"\n\n    if mean &gt; 1 or mean &lt; 0:\n        raise ValueError(f\"mean must be between zero and one, received {mean}.\")\n\n    # if we know minimum effect, we know treatment mean and treatment variance\n    # see https://github.com/bookingcom/powercalculator/blob/master/src/js/math.js#L113\n\n    def get_std(mean):\n        return np.sqrt(mean * (1 - mean))\n\n    mean_2 = mean * (1 + minimum_effect)\n\n    return cls.required_sample_size_per_variant(\n        n_variants=n_variants,\n        minimum_effect=minimum_effect,\n        mean=mean,\n        std=get_std(mean),\n        std_2=get_std(mean_2),\n        confidence_level=confidence_level,\n        power=power,\n    )\n</code></pre>"},{"location":"api/statistics.html#epstats.toolkit.statistics.Statistics.ttest_evaluation","title":"<code>ttest_evaluation(stats, control_variant)</code>  <code>classmethod</code>","text":"<p>Testing statistical significance of relative difference in means of treatment and control variant.</p> <p>This is inspired by scipy.stats.ttest_ind_from_stats method that returns many more statistics than p-value and test statistic.</p> <p>Statistics used:</p> <ol> <li>Welch's t-test</li> <li>Welch\u2013Satterthwaite equation approximation of degrees of freedom.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>array</code> <p>array with dimensions (metrics, variants, stats)</p> required <code>control_variant</code> <code>str</code> <p>string with the name of control variant</p> required <p><code>stats</code> array values:</p> <ol> <li><code>metric_id</code></li> <li><code>metric_name</code></li> <li><code>exp_variant_id</code></li> <li><code>count</code></li> <li><code>mean</code></li> <li><code>std</code></li> <li><code>sum_value</code></li> <li><code>sum_sqr_value</code></li> </ol> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dataframe containing statistics from the t-test</p> <p>Schema of returned dataframe:</p> <ol> <li><code>metric_id</code> - metric id as in <code>Experiment</code> definition</li> <li><code>metric_name</code> - metric name as in <code>Experiment</code> definition</li> <li><code>exp_variant_id</code> - variant id</li> <li><code>count</code> - number of exposures, value of metric denominator</li> <li><code>mean</code> - <code>sum_value</code> / <code>count</code></li> <li><code>std</code> - sample standard deviation</li> <li><code>sum_value</code> - value of goals, value of metric nominator</li> <li><code>confidence_level</code> - current confidence level used to calculate <code>p_value</code> and <code>confidence_interval</code></li> <li><code>diff</code> - relative diff between sample means of this and control variant</li> <li><code>test_stat</code> - value of test statistic of the relative difference in means</li> <li><code>p_value</code> - p-value of the test statistic under current <code>confidence_level</code></li> <li><code>confidence_interval</code> - confidence interval of the <code>diff</code> under current <code>confidence_level</code></li> <li><code>standard_error</code> - standard error of the <code>diff</code></li> <li><code>degrees_of_freedom</code> - degrees of freedom of this variant mean</li> </ol> Source code in <code>src/epstats/toolkit/statistics.py</code> <pre><code>@classmethod\ndef ttest_evaluation(cls, stats: np.array, control_variant: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Testing statistical significance of relative difference in means of treatment and control variant.\n\n    This is inspired by [scipy.stats.ttest_ind_from_stats](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html)\n    method that returns many more statistics than p-value and test statistic.\n\n    Statistics used:\n\n    1. [Welch's t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test)\n    1. [Welch\u2013Satterthwaite equation](https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation)\n    approximation of degrees of freedom.\n\n    Arguments:\n        stats: array with dimensions (metrics, variants, stats)\n        control_variant: string with the name of control variant\n\n    `stats` array values:\n\n    0. `metric_id`\n    1. `metric_name`\n    1. `exp_variant_id`\n    1. `count`\n    1. `mean`\n    1. `std`\n    1. `sum_value`\n    1. `sum_sqr_value`\n\n    Returns:\n        dataframe containing statistics from the t-test\n\n    Schema of returned dataframe:\n\n    1. `metric_id` - metric id as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n    1. `metric_name` - metric name as in [`Experiment`][epstats.toolkit.experiment.Experiment] definition\n    1. `exp_variant_id` - variant id\n    1. `count` - number of exposures, value of metric denominator\n    1. `mean` - `sum_value` / `count`\n    1. `std` - sample standard deviation\n    1. `sum_value` - value of goals, value of metric nominator\n    1. `confidence_level` - current confidence level used to calculate `p_value` and `confidence_interval`\n    1. `diff` - relative diff between sample means of this and control variant\n    1. `test_stat` - value of test statistic of the relative difference in means\n    1. `p_value` - p-value of the test statistic under current `confidence_level`\n    1. `confidence_interval` - confidence interval of the `diff` under current `confidence_level`\n    1. `standard_error` - standard error of the `diff`\n    1. `degrees_of_freedom` - degrees of freedom of this variant mean\n    \"\"\"\n    stats = stats.transpose(1, 2, 0)\n\n    stat_res = []  # semiresults\n    variants_count = stats.shape[0]  # number of variants\n\n    # get only stats (not metric_id, metric_name, exp_variant_id) from the stats array as floats\n    stats_values = stats[:, 3:8, :].astype(float)\n\n    # select stats data for control variant\n    for s in stats:\n        if s[2][0] == control_variant:\n            stats_values_control_variant = s[3:8, :].astype(float)\n            break\n\n    # control variant values\n    count_cont = stats_values_control_variant[0]  # number of observations\n    mean_cont = stats_values_control_variant[1]  # mean\n    std_cont = stats_values_control_variant[2]  # standard deviation\n    conf_level = stats_values_control_variant[4]  # confidence level\n\n    # this for loop goes over variants and compares one variant values against control variant values for\n    # all metrics at once. Similar to scipy.stats.ttest_ind_from_stats\n    for i in range(variants_count):\n        # treatment variant data\n        s = stats_values[i]\n        count_treat = s[0]  # number of observations\n        mean_treat = s[1]  # mean\n        std_treat = s[2]  # standard deviation\n        sum_value = s[3]  # sum of observations\n\n        # degrees of freedom\n        num = (std_cont**2 / count_cont + std_treat**2 / count_treat) ** 2\n        den = (std_cont**4 / (count_cont**2 * (count_cont - 1))) + (\n            std_treat**4 / (count_treat**2 * (count_treat - 1))\n        )\n\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            # We fill in zeros, when goal data are missing for some variant.\n            # There could be division by zero here which is expected as we return\n            # nan or inf values to the caller.\n            # np.round() in case of roundoff errors, e.g. f = 9.999999998 =&gt; trunc(round(f, 5)) = 10\n            f = np.trunc(np.round(num / den, 5))  # (rounded &amp; truncated) degrees of freedom\n\n        # t-quantile\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n            t_quantile = st.t.ppf(conf_level + (1 - conf_level) / 2, f)  # right quantile\n\n        # relative difference and test statistics\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            # We fill in zeros, when goal data are missing for some variant.\n            # There could be division by zero here which is expected as we return\n            # nan or inf values to the caller.\n            rel_diff = (mean_treat - mean_cont) / np.abs(mean_cont)\n            # standard error for relative difference\n            rel_se = (\n                np.sqrt((mean_treat * std_cont) ** 2 / (mean_cont**2 * count_cont) + (std_treat**2 / count_treat))\n                / mean_cont\n            )\n            test_stat = rel_diff / rel_se\n\n            # If test_stat is inf and sum of non-zero observations is low,\n            # set test_stat to 0 to prevent p-value from being 0.\n            if test_stat.shape == sum_value.shape:\n                mask = np.isinf(test_stat) &amp; (sum_value &lt;= 10)\n                test_stat[mask] = 0.0\n\n        # p-value\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n            pval = 2 * (1 - st.t.cdf(np.abs(test_stat), f))\n\n        # confidence interval\n        conf_int = rel_se * t_quantile\n\n        # save results\n        stat_res.append((rel_diff, test_stat, pval, conf_int, rel_se, f))\n\n    # Tune up results\n    s = np.hstack([stats, stat_res])\n    s = s.transpose(2, 0, 1)  # move back to metrics, variants, stats order\n    x, y, z = s.shape\n    arr = s.reshape(x * y, z)\n\n    # Output dataframe\n    col = [\n        \"metric_id\",\n        \"metric_name\",\n        \"exp_variant_id\",\n        \"count\",\n        \"mean\",\n        \"std\",\n        \"sum_value\",\n        \"confidence_level\",\n        \"diff\",\n        \"test_stat\",\n        \"p_value\",\n        \"confidence_interval\",\n        \"standard_error\",\n        \"degrees_of_freedom\",\n    ]\n    r = pd.DataFrame(arr, columns=col)\n    return r\n</code></pre>"},{"location":"api/test_data.html","title":"Test Data","text":"<p>Utility methods to load sample (test) data that are used in unit tests through this project.</p> Source code in <code>src/epstats/toolkit/testing/test_data.py</code> <pre><code>class TestData:\n    \"\"\"\n    Utility methods to load sample (test) data that are used in unit tests through this\n    project.\n    \"\"\"\n\n    @classmethod\n    def load_goals_agg(cls, exp_id: str = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Load sample of aggregated test data to evaluate metrics. We use this dataset\n        in unit testing and we are making it available here for other possible use-cases too.\n\n        See `load_evaluations` set of functions to load corresponding evaluation results.\n\n        Arguments:\n            exp_id: experiment id\n        \"\"\"\n        df_file = files(resources).joinpath(\"goals_agg.csv\")\n        goals_df = pd.read_csv(df_file)\n        return goals_df[goals_df.exp_id == exp_id] if exp_id is not None else goals_df\n\n    @classmethod\n    def load_goals_simple_agg(cls) -&gt; pd.DataFrame:\n        \"\"\"\n        Load sample of aggregated test data in simple wide format. File `goals_simple_agg.csv` contains only one\n        experiment, so it is sufficient to just open it.\n\n        We use this dataset in unit testing and we are making it available here for other possible use-cases too.\n\n        See `load_evaluations` set of functions to load corresponding evaluation results.\n        \"\"\"\n        df_file = files(resources).joinpath(\"goals_simple_agg.csv\")\n        goals_df = pd.read_csv(df_file)\n        return goals_df\n\n    @classmethod\n    def load_goals_by_unit(cls, exp_id: str = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Load sample of test data by unit to evaluate metrics. We use this dataset\n        in unit testing and we are making it available here for other possible use-cases too.\n\n        See `load_evaluations` set of functions to load corresponding evaluation results.\n\n        Arguments:\n            exp_id: experiment id\n        \"\"\"\n        df_file = files(resources).joinpath(\"goals_by_unit.csv\")\n        goals_df = pd.read_csv(df_file)\n        return goals_df[goals_df.exp_id == exp_id] if exp_id is not None else goals_df\n\n    @classmethod\n    def load_evaluations_checks(cls, exp_id: str = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Load checks (SRM) evaluations results. This data can be used to do asserts against\n        after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg]\n        or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data.\n\n        Arguments:\n            exp_id: experiment id\n        \"\"\"\n        df_file = files(resources).joinpath(\"evaluations_checks.csv\")\n        goals_df = pd.read_csv(df_file)\n        return goals_df[goals_df.exp_id == exp_id] if exp_id is not None else goals_df\n\n    @classmethod\n    def load_evaluations_exposures(cls, exp_id: str = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Load exposures evaluations results. This data can be used to do asserts against\n        after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg]\n        or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data.\n\n        Arguments:\n            exp_id: experiment id\n        \"\"\"\n        df_file = files(resources).joinpath(\"evaluations_exposures.csv\")\n        exposures_df = pd.read_csv(df_file)\n        return exposures_df[exposures_df.exp_id == exp_id] if exp_id is not None else exposures_df\n\n    @classmethod\n    def load_evaluations_metrics(cls, exp_id: str = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Load metric evaluations results. This data can be used to do asserts against\n        after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg]\n        or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data.\n\n        Arguments:\n            exp_id: experiment id\n        \"\"\"\n        df_file = files(resources).joinpath(\"evaluations_metrics.csv\")\n        goals_df = pd.read_csv(df_file)\n        return goals_df[goals_df.exp_id == exp_id] if exp_id is not None else goals_df\n</code></pre>"},{"location":"api/test_data.html#epstats.toolkit.testing.TestData.load_evaluations_checks","title":"<code>load_evaluations_checks(exp_id=None)</code>  <code>classmethod</code>","text":"<p>Load checks (SRM) evaluations results. This data can be used to do asserts against after running evaluation on pre-aggregated or by-unit test data.</p> <p>Parameters:</p> Name Type Description Default <code>exp_id</code> <code>str</code> <p>experiment id</p> <code>None</code> Source code in <code>src/epstats/toolkit/testing/test_data.py</code> <pre><code>@classmethod\ndef load_evaluations_checks(cls, exp_id: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Load checks (SRM) evaluations results. This data can be used to do asserts against\n    after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg]\n    or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data.\n\n    Arguments:\n        exp_id: experiment id\n    \"\"\"\n    df_file = files(resources).joinpath(\"evaluations_checks.csv\")\n    goals_df = pd.read_csv(df_file)\n    return goals_df[goals_df.exp_id == exp_id] if exp_id is not None else goals_df\n</code></pre>"},{"location":"api/test_data.html#epstats.toolkit.testing.TestData.load_evaluations_exposures","title":"<code>load_evaluations_exposures(exp_id=None)</code>  <code>classmethod</code>","text":"<p>Load exposures evaluations results. This data can be used to do asserts against after running evaluation on pre-aggregated or by-unit test data.</p> <p>Parameters:</p> Name Type Description Default <code>exp_id</code> <code>str</code> <p>experiment id</p> <code>None</code> Source code in <code>src/epstats/toolkit/testing/test_data.py</code> <pre><code>@classmethod\ndef load_evaluations_exposures(cls, exp_id: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Load exposures evaluations results. This data can be used to do asserts against\n    after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg]\n    or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data.\n\n    Arguments:\n        exp_id: experiment id\n    \"\"\"\n    df_file = files(resources).joinpath(\"evaluations_exposures.csv\")\n    exposures_df = pd.read_csv(df_file)\n    return exposures_df[exposures_df.exp_id == exp_id] if exp_id is not None else exposures_df\n</code></pre>"},{"location":"api/test_data.html#epstats.toolkit.testing.TestData.load_evaluations_metrics","title":"<code>load_evaluations_metrics(exp_id=None)</code>  <code>classmethod</code>","text":"<p>Load metric evaluations results. This data can be used to do asserts against after running evaluation on pre-aggregated or by-unit test data.</p> <p>Parameters:</p> Name Type Description Default <code>exp_id</code> <code>str</code> <p>experiment id</p> <code>None</code> Source code in <code>src/epstats/toolkit/testing/test_data.py</code> <pre><code>@classmethod\ndef load_evaluations_metrics(cls, exp_id: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Load metric evaluations results. This data can be used to do asserts against\n    after running evaluation on [pre-aggregated][epstats.toolkit.testing.test_data.TestData.load_goals_agg]\n    or [by-unit][epstats.toolkit.testing.test_data.TestData.load_goals_by_unit] test data.\n\n    Arguments:\n        exp_id: experiment id\n    \"\"\"\n    df_file = files(resources).joinpath(\"evaluations_metrics.csv\")\n    goals_df = pd.read_csv(df_file)\n    return goals_df[goals_df.exp_id == exp_id] if exp_id is not None else goals_df\n</code></pre>"},{"location":"api/test_data.html#epstats.toolkit.testing.TestData.load_goals_agg","title":"<code>load_goals_agg(exp_id=None)</code>  <code>classmethod</code>","text":"<p>Load sample of aggregated test data to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too.</p> <p>See <code>load_evaluations</code> set of functions to load corresponding evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>exp_id</code> <code>str</code> <p>experiment id</p> <code>None</code> Source code in <code>src/epstats/toolkit/testing/test_data.py</code> <pre><code>@classmethod\ndef load_goals_agg(cls, exp_id: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Load sample of aggregated test data to evaluate metrics. We use this dataset\n    in unit testing and we are making it available here for other possible use-cases too.\n\n    See `load_evaluations` set of functions to load corresponding evaluation results.\n\n    Arguments:\n        exp_id: experiment id\n    \"\"\"\n    df_file = files(resources).joinpath(\"goals_agg.csv\")\n    goals_df = pd.read_csv(df_file)\n    return goals_df[goals_df.exp_id == exp_id] if exp_id is not None else goals_df\n</code></pre>"},{"location":"api/test_data.html#epstats.toolkit.testing.TestData.load_goals_by_unit","title":"<code>load_goals_by_unit(exp_id=None)</code>  <code>classmethod</code>","text":"<p>Load sample of test data by unit to evaluate metrics. We use this dataset in unit testing and we are making it available here for other possible use-cases too.</p> <p>See <code>load_evaluations</code> set of functions to load corresponding evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>exp_id</code> <code>str</code> <p>experiment id</p> <code>None</code> Source code in <code>src/epstats/toolkit/testing/test_data.py</code> <pre><code>@classmethod\ndef load_goals_by_unit(cls, exp_id: str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Load sample of test data by unit to evaluate metrics. We use this dataset\n    in unit testing and we are making it available here for other possible use-cases too.\n\n    See `load_evaluations` set of functions to load corresponding evaluation results.\n\n    Arguments:\n        exp_id: experiment id\n    \"\"\"\n    df_file = files(resources).joinpath(\"goals_by_unit.csv\")\n    goals_df = pd.read_csv(df_file)\n    return goals_df[goals_df.exp_id == exp_id] if exp_id is not None else goals_df\n</code></pre>"},{"location":"api/test_data.html#epstats.toolkit.testing.TestData.load_goals_simple_agg","title":"<code>load_goals_simple_agg()</code>  <code>classmethod</code>","text":"<p>Load sample of aggregated test data in simple wide format. File <code>goals_simple_agg.csv</code> contains only one experiment, so it is sufficient to just open it.</p> <p>We use this dataset in unit testing and we are making it available here for other possible use-cases too.</p> <p>See <code>load_evaluations</code> set of functions to load corresponding evaluation results.</p> Source code in <code>src/epstats/toolkit/testing/test_data.py</code> <pre><code>@classmethod\ndef load_goals_simple_agg(cls) -&gt; pd.DataFrame:\n    \"\"\"\n    Load sample of aggregated test data in simple wide format. File `goals_simple_agg.csv` contains only one\n    experiment, so it is sufficient to just open it.\n\n    We use this dataset in unit testing and we are making it available here for other possible use-cases too.\n\n    See `load_evaluations` set of functions to load corresponding evaluation results.\n    \"\"\"\n    df_file = files(resources).joinpath(\"goals_simple_agg.csv\")\n    goals_df = pd.read_csv(df_file)\n    return goals_df\n</code></pre>"},{"location":"presentations/architecture/architecture.html","title":"Architecture","text":""},{"location":"stats/basics.html","title":"Statistics","text":"<p>We want to test, whether the difference between treatment and control variant in chosen metric is statistically significant or not. From statistical point of view, we deal with point estimates, confidence intervals and hypothesis testing. Confidence level is set by default to 95%, i.e. we talk about 95%-confidence intervals and hypothesis testing on 5% level of significance.</p> <p>We also run various data quality checks to guarantee trustworthiness of presented data.</p>"},{"location":"stats/basics.html#point-estimates-confidence-intervals-and-hypothesis-testing","title":"Point Estimates, Confidence Intervals and Hypothesis Testing","text":"<p>Assume we have one control variant (usually denoted by \\(A\\)) and one or more treatment variants (usually denoted by \\(B\\), \\(C\\), \\(D\\), ...). We calculate point estimate for relative difference between treatment and control variant in chosen metric. Next we calculate confidence interval for the relative difference. Finally, we want to test whether estimated relative difference is significantly significant.</p> <p>Formula for relative difference is straightforward: \\((B - A) / A * 100\\). To be more precise, point estimate is only the estimate of true relative difference between treatment and control variant. The true relative difference is unknown! The point estimate is the best possible estimate of the true (unknown) relative difference using available data from the experiment.</p> <p>We implemented Welch's t-test<sup>1</sup> to test the significance of point estimate in EP Stats engine. Delta method is necessary, since standard Welch's test works well only for absolute differences, i.e. \\(B - A\\). We use quantiles from Student's t-distribution which approximate normal distribution.</p>"},{"location":"stats/basics.html#pitfalls-and-corrections","title":"Pitfalls and Corrections","text":"<p>We summarize all corrections implemented next to Welch's t-test in this part. Generally Welch's t-test assumes we have two samples, i.e. we have two sequences of independent and identically distributed random variables with finite variance. The variances between the variants may not be equal.</p> <p>Unfortunately assumptions above are often violated in practice and appropriate corrections are necessary to guarantee desired level of significance.</p>"},{"location":"stats/basics.html#absolute-vs-relative-difference","title":"Absolute vs. Relative Difference","text":"<p>Two-sample t-test is only correct when we deal with absolute difference, i.e. \\(B - A\\). But it does not hold any more when we deal with relative difference. In order to be statistically correct, delta method for relative difference is necessary.</p>"},{"location":"stats/basics.html#independent-and-identically-distributed-observations","title":"Independent and Identically Distributed Observations","text":"<p>Welch's t-test assumes that observations are independent and identically distributed (i.i.d.). Unfortunately this assumption does not hold always.</p> <p>Let's assume Click-through rate metric (i.e. clicks / views). Since multiple views (and clicks) from the same user (user being randomization unit here) are allowed, the assumption of independence is violated. Multiple observations from the same user are not independent. Delta method for iid is necessary (has not been implemented yet).</p>"},{"location":"stats/basics.html#multiple-comparisons-problem","title":"Multiple Comparisons Problem","text":"<p>If we have only one control \\(A\\) and one treatment \\(B\\) variant, we need to run just one Welch's t-test, i.e. relative difference between \\(B\\) and \\(A\\). If we have multiple treatment variants, e.g. \\(B\\), \\(C\\) and \\(D\\), we need to run three Welch's t-tests, i.e. relative difference between \\(B\\) and \\(A\\), \\(C\\) and \\(A\\), \\(D\\) and \\(A\\). If we run every single test on 5% level of significance, the overall level of significance is lower. The probability of false-positive error (i.e. we wrongly reject at least one null hypothesis) is higher than required 5% level.</p> <p>In ep-stats engine, we implemented HolmBonferroni p-value correction. Single tests are more conservative and so overall level of confidence is satisfied.</p>"},{"location":"stats/basics.html#real-time-experiment-evaluation","title":"Real-time Experiment Evaluation","text":"<p>Time is Money</p> <p>One of the main goals in Experimentation Platform was to develop real-time data pipelines. Standard Welch's t-test assumes you evaluate experiment only once, after you collect all data. If you evaluate experiment more than once, than the false-positive error (wrongly rejecting null hypothesis) grows enormously<sup>2</sup>.</p> <p>We implemented Sequential testing procedure to tackle this issue. It allows us to evaluate experiments (hypothesis) real-time during the experiment period without exceeding false-positive errors.</p> <p>The solution itself is pretty simple. In the beginning (circa first half of the experiment period) the decision rule is very conservative and only great differences can be called statistically significant. As we approach the end of the experiment the decision rule is less and less stringent. If we have enough evidence, the difference can be statistically significant and we can end up the experiment earlier.</p> <p>Main disadvantage is that the length of the experiment must be set in advance - before starting the experiment. This is very annoying for the experimenters (test owners) but right now this is the only way, how to deal with this issue. If we do not use Sequential testing, our false-positive errors could be somewhere between 20-30%, instead of required below 5%. It means one third of all presented results are wrong and without any chance to fix them. In other words, one third of decisions are wrong.</p>"},{"location":"stats/basics.html#data-quality-checks","title":"Data Quality Checks","text":"<p>Experimentation Platform is very complex. From data collection to statistical evaluation, there are many intermediate steps. All of these steps must be checked regularly in order to guarantee trustworthiness of presented results.</p>"},{"location":"stats/basics.html#sample-ratio-mismatch-check","title":"Sample Ratio Mismatch Check","text":"<p>Sample Ratio Mismatch check<sup>3</sup> (SRM Check) checks the quality of randomization. Randomization is absolutely crucial. Wrong randomization can have fatal consequences and results might be highly misleading. SRM check checks whether we have the same number of users in each variant. Chi-square[^6] test is implemented.</p> <p>In this check we require high reliability. Therefore the confidence level is set to 99.9%. If SRM check fails, presented results should not be taken into account and any decisions based on this experiment should not be done!</p>"},{"location":"stats/basics.html#sum-ratio-check","title":"Sum Ratio Check","text":"<p>Computes ratios of two goal counts summed across all variants.</p> <p>In EP this is used for the Experiment Variant Assignment Consistency Check (EVA Check), which evaluates how many unit ids are assigned to more than one experiment variant.</p>"},{"location":"stats/basics.html#data-quality","title":"Data Quality","text":"<p>We have not implemented any automatic data quality checks yet. The plan is to check all parts of data pipelines.</p> <ol> <li> <p>Welch's t-test \u21a9</p> </li> <li> <p>Ronny Kohavi, Sample Ratio Mismatch \u21a9</p> </li> <li> <p>Wikipedia, Chi-square test \u21a9</p> </li> </ol>"},{"location":"stats/compound_metrics.html","title":"Compound Metrics","text":"<p>There are metrics in Experimentation Platform, which are compounded, i.e. the nominator does not consist of value or count from a single goal, but it is composed from multiple goals. However, when we combine multiple goals into one, there might occur issues with standard deviation (variance).</p> <p>In data pipeline we save goals in multiple stages of aggregation:</p> <ol> <li> <p>Tracking table <code>tracking</code>: Basically raw data, no aggregation applied. Usually one line, one goal.</p> </li> <li> <p>Aggregated Unit Goals table <code>agg_unit_goals</code>: Goals are aggregated (grouped by) with respect to <code>experiment_id</code>, <code>variant_id</code>, <code>goal</code> and <code>guid</code>.</p> </li> <li> <p>Aggregated Goals table <code>agg_goals</code>: Goals are aggregated (grouped by) with respect to <code>experiment_id</code>, <code>variant_id</code> and <code>goal</code>.</p> </li> </ol> <p>In statistical evaluation in EP we use tables <code>agg_unit_goals</code> and <code>agg_goals</code>, which contain pre-aggregated goals. We do not use table <code>tracking</code> with raw goals. This is mainly due to optimality of SQL queries.</p>"},{"location":"stats/compound_metrics.html#example","title":"Example","text":"<p>We illustrate differences between tables on the following example. Assume we have two users, Sam and Mike. Sam bought three products for $10, $20 and $50. Than he refunded two of them - the one for $10 and the one for $20. Mike bought two products for $20 and $30. Later he refunded the cheaper one. They are both in A variant of our imaginary experiment.</p> <p>Tracking table <code>tracking</code></p> Variant User Goal Count Value A  Sam  purchase 1 $10 A  Sam  purchase 1 $20 A  Sam  purchase 1 $50 A  Sam  refund 1 $10 A  Sam  refund 1 $20 A  Mike  purchase 1 $20 A  Mike  purchase 1 $30 A  Mike  refund 1 $20 <p>Aggregated Unit Goals table <code>agg_unit_goals</code></p> Variant User Goal Count Value Value Squared A  Sam  purchase 3 $80 6,400 (80 * 80) A  Sam  refund 2 $30 900 (30 * 30) A  Mike  purchase 2 $50 2,500 (50 * 50) A  Mike  refund 1 $20 400 (20 * 20) <p>Aggregated Goals table <code>agg_goals</code></p> Variant Goal Count Value Value Squared A  purchase 5 $130 8,900 (6,400 + 2,500) A  refund 3 $50 1,300 (900 + 400)"},{"location":"stats/compound_metrics.html#simple-vs-compound-metrics","title":"Simple vs Compound Metrics","text":"<p>From table <code>agg_goals</code> it is easy to compute simple \"per User\" metrics such as Bookings per User or Refunds per User. For example total bookings equals to $130 ($80 Sam + $50 Mike) with sample variance 8,900 (6,400 Sam + 2,500 Mike). Analogously total refunds equals to $50 ($30 Sam + $20 Mike) with sample variance 1,300 (900 Sam + 400 Mike).</p> <p>The issues might arise when we think about compound metrics. Let assume we are interested in metric Net Bookings per User. In this metric we do not want to count purchases which were later refunded. Ideal situation would be if we had goal <code>net purchases</code>. This goal would be defined as <code>purchases</code> - <code>refunds</code>. If we compute <code>net purchases</code> from already aggregated goals, we overestimate the sample variance (<code>Value Squared</code>).</p> <p>Aggregated Unit Goals table <code>agg_unit_goals</code> (Overestimating sample variance - the way how we compute it now)</p> Variant User Goal Count Value Value Squared A  Sam  net purchases 1 $50 5,500 (6,400 - 900) A  Mike  net purchases 1 $30 2,100 (2,500 - 400) <p>This is exactly what we get if we use <code>agg_unit_goals</code> table. Now look closer. Sam bought three items for $10, $20 and $50. He refunded two of them for $10 and $20. In summary, Sam made one valid purchase for $50. Mike bought two items for $20 and $30. He refunded the first one. So do Mike made only one valid purchase for $30. Therefore, the <code>agg_unit_goals</code> table for (non-existing) goal <code>net purchases</code> should looks like this.</p> <p>Aggregated Unit Goals table <code>agg_unit_goals</code> (True sample variance - the way how we should compute it)</p> Variant User Goal Count Value Value Squared A  Sam  net purchases 1 $50 2,500 (50 * 50) A  Mike  net purchases 1 $30 900 (30 * 30) <p>Now when we compute metric Net Bookings per User from the first table, we end up with two purchases with total value of $80 and total sample variance 7,600.</p> <p>When we compute the same metric from the second table, we end up with two purchases with total value of $80 and total sample variance 3,400.</p> <p>In this specific case we overestimated sample variance by 4,200 - by 124%!</p>"},{"location":"stats/compound_metrics.html#solutions","title":"Solutions","text":"<p>There exists three possible solutions of this issue.</p>"},{"location":"stats/compound_metrics.html#let-it-be-as-it-is","title":"Let it be as it is","text":"<p>We can just let it be as it is now. Actually, there are two scenarios what can happen. We can either overestimate true sample variance or underestimate it.</p> <p>Overestimating sample variance is not as harmful as underestimating. Overestimating causes we end up with higher p-value and wider confidence intervals. In a nutshell, we are more conservative. Underestimating is much worse. In this case we have no guarantee our results are trustworthy anymore.</p> <p>We overestimate the true sample variance if compound metric is subtraction of goals. We underestimate the true sample variance if compound metric is summation of goals. We can both underestimate or overestimate the true sample variance if compound metric is both subtraction and summation of goals.</p>"},{"location":"stats/compound_metrics.html#compute-value-squared-correctly","title":"Compute Value Squared correctly","text":"<p>We can compute <code>Value Squared</code> correctly and avoid all these problems. Unfortunately, this straightforward solution is pretty tricky. It demands non-trivial implementation in EP SQL queries. The interim table <code>agg_unit_goals</code> must join itself as many times as many goals compound metric contains.</p> <p>On the other hand, this is a general solution, and it would solve all problems connected with compound metrics and their sample variances.</p>"},{"location":"stats/compound_metrics.html#correct-the-error","title":"Correct the error","text":"<p>In the next sections we will explicitly derive the error in sample variance. We can adjust the overestimated (underestimated) sample variance using this error. Then we end up with correct value of sample variance. However, this might be messy for compound metrics with more than two goals.</p>"},{"location":"stats/compound_metrics.html#error-adjustment-subtraction","title":"Error adjustment - subtraction","text":"<p>When the compound metric consists of subtraction of two goals, we overestimate the true sample variance. Firstly, we derive the error for one user. Secondly, we derive the error form multiple users.</p>"},{"location":"stats/compound_metrics.html#only-one-user","title":"Only one user","text":"<p>Let assume we have only one user. The user made \\(P\\) purchases and \\(R\\) refunds, where \\(R \\leq P\\). We denote values of purchases by \\(p_{i}, \\, i = 1, \\dots, P\\) and values of refunds by \\(r_{j}, \\, j = 1, \\dots, R\\).</p> <p>We want to prove that if we compute sample variance of <code>net purchases</code> from <code>agg_unit_goals</code> table, we will overestimate the true sample variance (right side of inequality). The true sample variance is on the left side of the inequality.</p> \\[\\begin{split} \\big( \\sum_{i = 1}^{P} p_{i} - \\sum_{j = 1}^{R} r_{j} \\big)^{2} &amp; \\leq \\big( \\sum_{i = 1}^{P} p_{i} \\big)^{2} - \\big( \\sum_{j = 1}^{R} r_{j} \\big)^{2}, \\\\ \\big( \\sum_{i = 1}^{P} p_{i} \\big)^{2} + \\big( \\sum_{j = 1}^{R} r_{j} \\big)^{2} - 2 \\sum_{i = 1}^{P} p_{i} \\sum_{j = 1}^{R} r_{j} &amp; \\leq \\big( \\sum_{i = 1}^{P} p_{i} \\big)^{2} - \\big( \\sum_{j = 1}^{R} r_{j} \\big)^{2}, \\\\ \\big( \\sum_{j = 1}^{R} r_{j} \\big)^{2} - 2 \\sum_{i = 1}^{P} p_{i} \\sum_{j = 1}^{R} r_{j} &amp; \\leq - \\big( \\sum_{j = 1}^{R} r_{j} \\big)^{2}, \\\\ 0 &amp; \\leq 2 \\sum_{j = 1}^{R} r_{j} \\big( \\sum_{i = 1}^{P} p_{i} - \\sum_{j = 1}^{R} r_{j} \\big). \\end{split}\\] <p>Since \\(p_{i} \\geq 0\\) for \\(i = 1, \\dots, P\\) and \\(r_{j} \\geq 0\\) for \\(j = 1, \\dots, R\\), the right side on the last line is greater or equal to zero. We have proved that the original inequality holds - we overestimate the true sample variance. Next, we have explicitly derived the error term</p> \\[ 2 \\sum_{j = 1}^{R} r_{j} \\big( \\sum_{i = 1}^{P} p_{i} - \\sum_{j = 1}^{R} r_{j} \\big). \\]"},{"location":"stats/compound_metrics.html#multiple-users","title":"Multiple users","text":"<p>In case of multiple users the problem complicates only slightly. Let assume we have \\(N\\) users. So the inequality is following</p> \\[ \\sum_{n = 1}^{N} \\Big( \\sum_{i = 1}^{P_{n}} p_{i,n} - \\sum_{j = 1}^{R_{n}} r_{j,n} \\Big)^{2} \\leq \\sum_{n = 1}^{N} \\Big( \\big( \\sum_{i = 1}^{P_{n}} p_{i,n} \\big)^{2} - \\big( \\sum_{j = 1}^{R_{n}} r_{j,n} \\big)^{2} \\Big). \\] <p>Since the inequality holds for every single user, it must also holds if we sum them up.</p> <p>Also the error by how much do we differ is the sum of single errors</p> \\[ \\sum_{n = 1}^{N} \\Big( 2 \\sum_{j = 1}^{R_{n}} r_{j,n} \\big( \\sum_{i = 1}^{P_{n}} p_{i,n} - \\sum_{j = 1}^{R_{n}} r_{j,n} \\big) \\Big). \\]"},{"location":"stats/compound_metrics.html#error-adjustment-summation","title":"Error adjustment - summation","text":"<p>This time we derive error term when we sum two goals. Let assume we want to compute compound metric All bookings per User which is defined as purchases plus trial conversions. The compound goal <code>all purchases</code> equals to <code>purchases</code> + <code>trails</code>. In this case we show by how much we underestimate the true sample variance.</p>"},{"location":"stats/compound_metrics.html#only-one-user_1","title":"Only one user","text":"<p>Again let assume we have only one user. The user made \\(P\\) purchases and \\(T\\) trial conversions. We denote values of purchases by \\(p_{i}, \\, i = 1, \\dots, P\\) and values of trial conversions by \\(t_{k}, \\, k = 1, \\dots, T\\).</p> <p>We want to prove that if we compute sample variance of <code>all purchases</code> from <code>agg_unit_goals</code> table, we will underestimate the true sample variance (right side of inequality). The true sample variance is on the left side of the inequality.</p> \\[\\begin{split} \\big( \\sum_{i = 1}^{P} p_{i} + \\sum_{k = 1}^{T} t_{k} \\big)^{2} &amp; \\geq \\big( \\sum_{i = 1}^{P} p_{i} \\big)^{2} + \\big( \\sum_{k = 1}^{T} t_{k} \\big)^{2}, \\\\ \\big( \\sum_{i = 1}^{P} p_{i} \\big)^{2} + \\big( \\sum_{k = 1}^{T} t_{k} \\big)^{2} + 2 \\sum_{i = 1}^{P} p_{i} \\sum_{k = 1}^{T} t_{k} &amp; \\geq \\big( \\sum_{i = 1}^{P} p_{i} \\big)^{2} + \\big( \\sum_{k = 1}^{T} t_{k} \\big)^{2}, \\\\ 2 \\sum_{i = 1}^{P} p_{i} \\sum_{k = 1}^{T} t_{k} &amp; \\geq 0. \\end{split}\\] <p>Since \\(p_{i} \\geq 0\\) for \\(i = 1, \\dots, P\\) and \\(t_{k} \\geq 0\\) for \\(k = 1, \\dots, T\\), the left side on the last line is greater or equal to zero. We have proved that the original inequality holds - we underestimate the true sample variance. Next, we have explicitly derived the error term</p> \\[ 2 \\sum_{i = 1}^{P} p_{i} \\sum_{k = 1}^{T} t_{k}. \\]"},{"location":"stats/compound_metrics.html#multiple-users_1","title":"Multiple users","text":"<p>In case of multiple users the problem complicates only slightly. Let assume we have \\(N\\) users. So the inequality is following</p> \\[ \\sum_{n = 1}^{N} \\Big( \\sum_{i = 1}^{P_{n}} p_{i,n} + \\sum_{k = 1}^{T_{n}} t_{k,n} \\Big)^{2} \\geq \\sum_{n = 1}^{N} \\Big( \\big( \\sum_{i = 1}^{P_{n}} p_{i,n} \\big)^{2} + \\big( \\sum_{k = 1}^{T_{n}} t_{k, n} \\big)^{2} \\Big). \\] <p>Since the inequality holds for every single user, it must also holds if we sum them up.</p> <p>Also the error by how much do we differ is the sum of single errors</p> \\[ \\sum_{n = 1}^{N} \\Big( 2 \\sum_{i = 1}^{P_{n}} p_{i,n} \\sum_{k = 1}^{T_{n}} t_{k,n} \\Big). \\]"},{"location":"stats/ctr.html","title":"CTR Metric","text":"<p>The goal of this post is to provide complete manual for deriving asymptotic distribution of Click-through rate (CTR). We are aimed at correct theoretical derivations, including verification of all assumptions. Since CTR is one of the primary metrics in A/B testing, we derive asymptotic distribution for absolute and relative difference in CTR between two variants - control and treatment.</p>"},{"location":"stats/ctr.html#theory","title":"Theory","text":"<p>We will use two core statistical tools - Central limit theorem and Delta method. There exists a great YouTube video from the Khan Academy explaining CLT, we greatly recommend it!</p>"},{"location":"stats/ctr.html#central-limit-theorem","title":"Central limit theorem","text":"<p>Let \\(X_{1}, \\dots, X_{n}\\) be a random sample of size \\(n\\) - a sequence of \\(n\\) independent and identically distributed random variables drawn from a distribution of expected value given by \\(\\mu\\) and finite variance given by \\(\\sigma^{2}\\). Let denote sample average as \\(\\bar{X_{n}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\). Then holds</p> \\[ \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} (0, \\sigma^{2}), \\] \\[ \\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma^{2}), \\,\\,\\, n \\rightarrow \\, \\infty \\] <p>i.e. as \\(n\\) approaches infinity, the random variables \\(\\sqrt{n} \\, \\big( \\bar{X_{n}} - \\mu \\big)\\) converge in distribution to a normal \\(\\mathcal{N} (0, \\sigma^{2})\\). Another acceptable, but slightly vague formulations are</p> \\[ \\bar{X_{n}} \\stackrel{as}{\\sim} \\mathcal{N} (\\mu, \\frac{\\sigma^{2}}{n}), \\] <p>or</p> \\[ \\sqrt{n} \\, \\frac{\\bar{X_{n}} - \\mu}{\\sigma} \\stackrel{as}{\\sim} \\mathcal{N} (0, 1).\\]"},{"location":"stats/ctr.html#delta-method-univariate","title":"Delta Method - Univariate","text":"<p>Let assume any sequence of random variables \\(\\{T_{n}\\}_{n=1}^{\\infty}\\) satisfying</p> \\[ \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big) \\] \\[ \\sqrt{n} \\, \\big( T_{n} - \\mu \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty \\] <p>and function \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\) which has continuous derivative around a point \\(\\mu\\), i.e. \\(g^{'}(\\mu)\\) is continuous. Then holds</p> \\[ \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big).\\] \\[ \\sqrt{n} \\, \\big( g(T_{n}) - g(\\mu) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, [g^{'}(\\mu)]^{2}\\sigma^{2}\\big), \\,\\,\\, n \\rightarrow \\, \\infty.\\]"},{"location":"stats/ctr.html#delta-method-multivariate","title":"Delta Method - Multivariate","text":"<p>Let assume any sequence of random vectors \\(\\{ \\pmb{T}_{n} \\}_{n=1}^{\\infty}\\) satisfying</p> \\[ \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big) \\] \\[ \\sqrt{n} \\, \\big( \\pmb{T}_{n} - \\pmb{\\mu} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{k} \\big(\\pmb{0}, \\, \\Sigma\\big), \\,\\,\\, n \\rightarrow \\, \\infty \\] <p>and function \\(g: \\mathbb{R^{k}} \\rightarrow \\mathbb{R^{p}}\\) which is continuously differentiable around point \\(\\pmb{\\mu}\\). Denote \\(\\mathbb{D}(x) = \\frac{\\partial \\, g(x)}{\\partial \\, x}\\). Then holds</p> \\[ \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\xrightarrow[\\text{n $\\rightarrow \\, \\infty$}]{\\text{d}} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big). \\] \\[ \\sqrt{n} \\, \\big( g(\\pmb{T}_{n}) - g(\\pmb{\\mu}) \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{p} \\big(\\pmb{0}, \\, \\mathbb{D}(\\mu) \\, \\Sigma \\, \\mathbb{D}(\\mu)^{T} \\big), \\,\\,\\, n \\rightarrow \\, \\infty. \\]"},{"location":"stats/ctr.html#ctr-definition","title":"CTR Definition","text":"<p>Without loss of generality, we can only focus on the control group with \\(K\\) users. Every user can see test screen multiple times, denoted by \\(N_{i}, \\, i = 1, \\dots, K\\). \\(N_{i} \\in \\mathbb{N}\\) is a discrete random variable with unknown probability distribution and finite variance. Next user can click on the screen. This action is denoted by binomial random variable \\(Y_{i, j}, \\, i = 1, \\dots, K, \\, j = 1, \\dots, N_{i}\\)</p> \\[  Y_{i, j}=\\begin{cases}     1, &amp; \\text{if $i-th$ user clicks in his $j-th$ view},\\\\     0, &amp; \\text{otherwise}.   \\end{cases} \\] <p>Click-through rate (CTR) is then defined as sum of all clicks devided by sum of all views</p> \\[ CTR = \\frac{\\sum_{i=1}^{K} \\sum_{j=1}^{N_{i}} Y_{i, j}}{\\sum_{i=1}^{K} N_{i}}.\\] <p>We want to derive asymptotic distribution for CTR. But we can not directly use central limit theorem since assumptions are violated. Random variables \\(Y_{i, j}\\) are not independent, nor identically distributed. We can use a little trick<sup>1</sup> and simply reformulate CTR definition without any change:</p> \\[ CTR = \\frac{\\sum_{i=1}^{K} \\big( \\sum_{j=1}^{N_{i}} Y_{i, j} \\big)}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i}}{\\sum_{i=1}^{K} N_{i}} = \\frac{\\sum_{i=1}^{K} S_{i} \\big/ K}{\\sum_{i=1}^{K} N_{i} \\big/ K} = \\frac{\\bar{S}}{\\bar{N}} = \\bar{Y}, \\] <p>where \\(\\bar{S} = \\frac{1}{K} \\sum_{i=1}^{K} S_{i}\\) stands for average clicks per user and \\(\\bar{N} = \\frac{1}{K} \\sum_{i=1}^{K} N_{i}\\) stands for average views per user. Users are independent of each other, random variables \\(N_{i}, \\, i = 1, \\dots, K\\) are independent and indetically distributed. For simplification we will assume that also random variables \\(S_{i}, \\, i = 1, \\dots, K\\) are independent and identically distributed, but it is only half true. \\(S_{i}\\) are independent, since users are independent of each other, but they are not identically distributed - \\(S_{1}\\) has some unknown discrete distribution on closed interval \\([0, N_{1}]\\), \\(S_{2}\\) has some unknown discrete distribution on closed interval \\([0, N_{2}]\\) and so on. Since \\(N_{i} \\in \\mathbb{N}, \\, i = 1, \\dots, K\\) are random variables and so \\(P \\big(N_{i} = N_{j} \\big) \\neq 1\\) for \\(i \\neq j\\). There exist other versions of central limit theorem which only assume independence, e.g. Lyapunov CLT.</p>"},{"location":"stats/ctr.html#asymptotic-distribution-of-ctr","title":"Asymptotic Distribution of CTR","text":"<p>In this part we will derive asymptotic distributon for CTR. CTR is defined as fraction of two random variables - \\(\\bar{S}\\) and \\(\\bar{N}\\). We will proceed in three steps:</p> <ol> <li>We will use CLT and derive asymptotic distributions for both \\(\\bar{S}\\) and \\(\\bar{N}\\).</li> <li>We will use delta method - multivariate and derive asymptotic distribution for CTR.</li> </ol>"},{"location":"stats/ctr.html#step-1","title":"Step 1","text":"<p>Since \\(S_{1}, \\dots, S_{K}\\) is a random sample, from CLT we have</p> \\[ \\sqrt{K} \\, \\big( \\bar{S} - \\mu_{S} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{S}^{2}). \\] <p>Since \\(N_{1}, \\dots, N_{K}\\) is a random sample, from CLT we similary have</p> \\[ \\sqrt{K} \\, \\big( \\bar{N} - \\mu_{N} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} (0, \\sigma_{N}^{2}). \\] <p>We can join both asymptotic normal distributions into two dimensional normal distribution</p> \\[ \\sqrt{K} \\, \\Bigg( \\begin{pmatrix} \\bar{S} \\\\ \\bar{N} \\end{pmatrix} - \\begin{pmatrix} \\mu_{S} \\\\ \\mu_{N} \\end{pmatrix} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{S}^{2} &amp; \\sigma_{SN} \\\\ \\sigma_{SN} &amp; \\sigma_{N}^2 \\end{pmatrix} \\Bigg), \\] <p>where \\(\\sigma_{SN}\\) is covariance between random variables \\(S\\) and \\(N\\) defined as \\(\\sigma_{SN} = \\mathrm{cov}(S,N) = \\mathbb{E} \\big[(S - \\mu_{S})(N - \\mu_{n})\\big]\\). Unknown covariance \\(\\sigma_{SN}\\) can be easily estimated using following formula</p> \\[ \\hat{\\sigma_{SN}} = \\sum_{i=1}^{K} (S_{i} - \\bar{S}_{n}) (N_{i} - \\bar{N}_{n}) = \\sum_{i=1}^{K} S_{i} N_{i} - K \\bar{S}_{n} \\bar{N}_{n}. \\]"},{"location":"stats/ctr.html#step-2","title":"Step 2","text":"<p>Now we apply multivariate delta method with a link function \\(g: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) defined as \\(g(x, y) = \\frac{x}{y}\\). Gradient in point \\((\\mu_{S}, \\mu_{N})\\) equals to</p> \\[\\nabla g (\\mu_{S}, \\mu_{N}) = (\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}}).\\] <p>Hence we have</p> \\[ \\sqrt{K} \\, \\Bigg( \\frac{\\bar{S}}{\\bar{N}}  - \\frac{\\mu_{S}}{\\mu_{N}} \\Bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\Bigg(0, \\, \\big(\\frac{1}{\\mu_{N}}, -\\frac{\\mu_{S}}{\\mu_{N}} \\big) \\begin{pmatrix} \\sigma_{S}^{2} &amp; \\sigma_{SN} \\\\ \\sigma_{SN} &amp; \\sigma_{N}^2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\mu_{N}} \\\\ -\\frac{\\mu_{S}}{\\mu_{N}} \\end{pmatrix} \\Bigg) \\] <p>Asymptotic distribution for CTR in treatment group with \\(K\\) observations equals to</p> <p>$$ \\sqrt{K} \\, \\bigg( \\bar{Y} - \\mu_{Y} \\bigg) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\bigg(0, \\, \\frac{1}{\\mu_{N}^2} \\big(\\sigma_{S}^2 - 2\\frac{\\mu_{S}}{\\mu_{N}}\\sigma_{SN} + \\frac{\\mu_{S}^2}{\\mu_{N}^2} \\sigma_{N}^2 \\big) \\bigg),$$ as \\(K\\) approaches infinity.</p>"},{"location":"stats/ctr.html#difference-between-control-and-treatment-group","title":"Difference Between Control and Treatment Group","text":"<p>We have derived asymptotic distribution fot control group. Analogously we would have derived asymptotic distribution for treatment group. Let's write them both once again</p> \\[\\sqrt{K} \\, \\big( \\bar{Y}_{A} - \\mu_{A} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{A}^2 \\big),$$ $$\\sqrt{L} \\, \\big( \\bar{Y}_{B} - \\mu_{B} \\big) \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, \\sigma_{B}^2 \\big),\\] <p>where \\(\\sigma_{A}^2\\) and \\(\\sigma_{B}^2\\) follows derivations right above (the complicated fomula) and \\(K\\) and \\(L\\) are number of observations in control and treatment group respectively.</p> <p>Since we have again two asymptotic normal distributions, we can join them into two dimensional normal distribution</p> \\[\\Bigg( \\begin{pmatrix} \\bar{Y}_{A} \\\\ \\bar{Y}_{B} \\end{pmatrix} - \\begin{pmatrix} \\mu_{A} \\\\ \\mu_{B} \\end{pmatrix} \\Bigg) \\stackrel{as}{\\sim} \\mathcal{N}_{2} \\Bigg( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma_{A}^{2} \\, / \\, K &amp; 0 \\\\ 0 &amp; \\sigma_{B}^2 \\, / \\, {L} \\end{pmatrix} \\Bigg).\\] <p>This time we used slightly different notation. We do need to be careful now. In general, we have different sample size (\\(K \\neq L\\)). But on the other hand, in this case we assume there is no correlation between those two distributions, see zeros in covariance matrix.</p> <p>In A/B testing we are usually interested in whether the difference between treatment and control group is statistically significant. We derive asymptotic distribution for both absolute and relative difference.</p>"},{"location":"stats/ctr.html#absolute-difference","title":"Absolute Difference","text":"<p>Absolute difference is easier. We will use multivariate delta method with simple link function \\(g: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) defined as \\(g(x, y) = y - x\\). Be aware of order \\(x\\) and \\(y\\) - it is \\(y - x\\), not \\(x - y\\). Gradient in point \\((\\mu_{A}, \\mu_{B})\\) equals to</p> \\[ \\nabla g (\\mu_{A}, \\mu_{B}) = (-1, 1) \\] <p>and hence the result is</p> \\[ \\big( (\\bar{Y}_{B} - \\bar{Y}_{A}) - (\\mu_{B} - \\mu_{A}) \\big) \\stackrel{as}{\\sim} \\mathcal{N} \\big(0, \\, (\\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L}) \\big).\\] <p>It can be written in following form</p> \\[ Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), \\] \\[ Z_{K, L} = \\frac{\\bar{Y}_{B} - \\bar{Y}_{A} - \\delta_{0}}{\\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}}} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), \\] <p>if \\(K, L \\rightarrow \\infty\\) and \\(\\frac{K}{L} \\rightarrow q \\in (0, \\infty)\\). \\(S_{A}^2\\) and \\(S_{B}^2\\) are sample variances.</p> <p>Two sided asymptotic confidence interval for absolute difference equals to</p> \\[ \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + u_{1 - \\alpha / 2} \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big), \\] <p>where \\(u_{1 - \\alpha / 2}\\) is \\((1 - \\alpha / 2)-\\)quantile of normal distribution \\(\\mathcal{N}(0, 1)\\).</p> <p>P-value equals to</p> <p>$$ p = 2 \\big(1 - \\Phi(|z|) \\big), $$ where \\(\\Phi\\) is distribution function of \\(\\mathcal{N}(0, 1)\\) and \\(z\\) is observed value of test statistics \\(Z_{K,L}\\).</p> <p>In practice is usually used Welch test, which uses t-distribution instead of normal distribution, with \\(f\\) degrees of freedom given as</p> \\[ f = \\frac{\\big( \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} \\big)^2}{\\frac{S_{A}^4}{K^2 (K-1)} + \\frac{S_{B}^4}{L^2 (L-1)}} .\\] <p>Then two sided asymptotic confidence interval (with t-quantiles) for absolute difference equals to</p> \\[ \\Big( \\bar{Y}_{B} - \\bar{Y}_{A} - t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} ; \\,\\, \\bar{Y}_{B} - \\bar{Y}_{A} + t_{f}(1 - \\alpha / 2) \\, \\sqrt{\\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L}} \\Big). \\] <p>P-value equals to</p> <p>$$ p = 2 \\big(1 - \\text{CDF}_{t, f}(|z|) \\big), $$ where \\(\\text{CDF}_{t,f}\\) is cumulative distribution function of t-distribution with \\(f\\) degrees of freedom and \\(z\\) is observed value of test statistics \\(Z_{K,L}\\).</p>"},{"location":"stats/ctr.html#relative-difference","title":"Relative Difference","text":"<p>To derive asymptotic distribution for relative difference we will again use multivariate delta method with a link function \\(g: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) defined as \\(g(x, y) = \\frac{y - x}{x}\\). Be aware of order \\(x\\) and \\(y\\). Gradient in point \\((\\mu_{A}, \\mu_{B})\\) equals to</p> \\[ \\nabla g (\\mu_{A}, \\mu_{B}) = \\big( -\\frac{\\mu_{B}}{\\mu_{A}^2}, \\frac{1}{\\mu_{A}} \\big).\\] <p>The result is</p> \\[ \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}} \\Big) \\stackrel{as}{\\sim} \\mathcal{N} \\Big(0, \\, \\frac{1}{\\mu_{A}^2} \\big(\\frac{\\mu_{B}^2}{\\mu_{A}^2} \\frac{\\sigma_{A}^2}{K} + \\frac{\\sigma_{B}^2}{L} \\big)\\Big).\\] <p>This can be rewritten in following form</p> \\[ Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\xrightarrow{\\text{d}} \\mathcal{N} \\big(0, \\, 1 \\big), \\] \\[ Z_{K, L} = \\frac{\\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\delta_{0}^{*}}{\\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }} \\stackrel{d}{\\longrightarrow} \\mathcal{N} \\big(0, \\, 1 \\big), \\] <p>if \\(K, L \\rightarrow \\infty\\) and \\(\\frac{K}{L} \\rightarrow q \\in (0, \\infty)\\). \\(S_{A}^2\\) and \\(S_{B}^2\\) are sample variances.</p> <p>For unknown true relative difference \\(\\frac{\\mu_{B} - \\mu_{A}}{\\mu_{A}}\\) we can derive confidence interval. For simplicity let's denote the sample variance as \\(\\tilde{S}^2\\), i.e.:</p> \\[\\tilde{S}^2 = \\frac{1}{\\bar{Y}_{A}} \\sqrt{ \\frac{\\bar{Y}_{B}^2}{\\bar{Y}_{A}^2} \\frac{S_{A}^2}{K} + \\frac{S_{B}^2}{L} }. \\] <p>Finaly, the two sided asymptotic confidence interval for relative difference equals to</p> \\[ \\Big( \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} - \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 ; \\frac{\\bar{Y}_{B} - \\bar{Y}_{A}}{\\bar{Y}_{A}} + \\mu_{1 - \\alpha / 2} \\, \\tilde{S}^2 \\Big). \\] <p>P-value equals to</p> <p>$$ p = 2 \\big(1 - \\Phi(|z|) \\big), $$ where \\(\\Phi\\) is distribution function of \\(\\mathcal{N}(0, 1)\\) and \\(z\\) is observed value of test statistics \\(Z_{K,L}\\).</p> <p>Since we know, there is no straightforward approximation using t-quantiles, because there is no formula for degrees of freedom. In practise, we have huge amount of observations and both quantiles (normal and t-quantile) are very close to each other for large \\(n\\).</p> <ol> <li> <p>A. Deng et al., Applying the Delta Method in Metrics Analytics: A Practical Guide with Novel Ideas \u21a9</p> </li> </ol>"},{"location":"stats/multiple.html","title":"Multiple Comparison Correction","text":"<p>Multiple comparisons problem is a problem, when we perform instead of one statistical test (i.e. difference between A and B variant) multiple statistical tests (i.e. difference between A and B, A and C, A and D). Some of them may have p-values less than 0.05 purely by chance, even if all our null hypotheses are really true.</p> <p>There exist various solutions. They differ mainly in power and implementation difficulty. We present one-by-one Bonferroni correction, HolmBonferroni method and Sidak correction. We summarize pros and cons of methods and provide step-by-step algorithms. Finally we present simple example in order to compare suggested methods.</p> <p>Generally HolmBonferroni method and the idk correction are universally more powerful procedures than the Bonferroni correction. Bonferroni correction is the most conservative one and suitable for independent tests.</p>"},{"location":"stats/multiple.html#summary","title":"Summary","text":"<p>After taking into account computational difficulty and results from a simple, but quite real example, we suggest to use Holm-Bonferroni method.</p>"},{"location":"stats/multiple.html#multiple-comparisons-problem","title":"Multiple Comparisons Problem","text":"<p>Source Multiple comparisons problem (Wikipedia).</p> <p>Let assume we want to test \\(m\\) null hypotheses \\(H_{0}^{1}, \\dots, H_{0}^{m}\\). Let denote by \\(\\alpha\\) our overall level of significance. Typically we set \\(\\alpha = 0.05\\). For each hull hypotheses we calculate p-values \\(p_{1}, \\dots, p_{m}\\). We need to adjust these p-values for individual hypotheses \\(H_{0}^{i}\\), \\(i = 1, \\dots, m\\) in order to satisfy overall level of significance \\(\\alpha\\).</p>"},{"location":"stats/multiple.html#bonferroni-correction","title":"Bonferroni correction","text":"<p>Source Bonferroni correction (Wikipedia).</p> <p>This is the simplest way how to deal with Multiple comparisons problem. It is easy to implement, on the other hand, it is too conservative and tests loose too much power. This correction works in the worst-case scenario that all tests are independent. Since we test difference between A and B, A and C, A and D variant, these tests are dependent and therefore this correction is too stringent.</p>"},{"location":"stats/multiple.html#algorithm","title":"Algorithm","text":"<ol> <li>Compute p-values \\(p_{1}, \\dots, p_{m}\\) for all \\(m\\) hypotheses \\(H_{0}^{1}, \\dots, H_{0}^{m}\\).</li> <li>Compute adjusted p-values using formula</li> </ol> \\[\\tilde{p}_{i} = \\min \\big\\{ m \\, p_{i}, 1 \\big\\}, \\,\\,\\, i = 1, \\dots, m\\] <ol> <li>Reject null hypothesis \\(H_{0}^{i}\\) if and only if \\(\\tilde{p}_{i} \\leq \\alpha\\).</li> </ol>"},{"location":"stats/multiple.html#example","title":"Example","text":"<pre><code>import statsmodels.stats.multitest\npvals=[0.01, 0.04, 0.03]\n\ndecision, adj_pvals, sidak_aplha, bonf_alpha = statsmodels.stats.multitest.multipletests(pvals=pvals, alpha=0.05, method='bonferroni')\n\nprint(f'Original p-values: \\t {pvals}')\nprint(f'Adjusted p-values: \\t {adj_pvals}')\n</code></pre> <pre><code>Original p-values:   [0.01, 0.04, 0.03]\nAdjusted p-values:   [0.03 0.12 0.09]\n</code></pre>"},{"location":"stats/multiple.html#holmbonferroni-method","title":"HolmBonferroni method","text":"<p>Source HolmBonferroni method (Wikipedia).</p> <p>Holm-Bonferroni method is more powerful than Bonferroni and it is valid under the same assumptions.</p>"},{"location":"stats/multiple.html#algorithm_1","title":"Algorithm","text":"<ol> <li>Compute p-values \\(p_{1}, \\dots, p_{m}\\) for all \\(m\\) hypotheses \\(H_{0}^{1}, \\dots, H_{0}^{m}\\).</li> <li>Order p-values from lowest to highest \\(p_{(1)} \\leq \\dots \\leq p_{(m)}\\)</li> <li>Let \\(k\\) be the minimal index such that \\(p_{k} (m + 1 - k) &gt; \\alpha\\).</li> <li>Reject the null hypotheses \\(H_{0}^{(1)}, \\dots, H_{0}^{(k-1)}\\) and do not reject \\(H_{0}^{(k)}, \\dots, H_{0}^{(m)}\\).</li> <li>If \\(k = 1\\) then do not reject any of the null hypotheses and if no such \\(k\\) index exist then reject all of the null hypotheses.</li> </ol>"},{"location":"stats/multiple.html#example_1","title":"Example","text":"<p><pre><code>decision, adj_pvals, sidak_aplha, bonf_alpha = statsmodels.stats.multitest.multipletests(pvals=pvals, alpha=0.05, method='holm')\n\nprint(f'Original p-values: \\t {pvals}')\nprint(f'Adjusted p-values: \\t {adj_pvals}')\n</code></pre>     Original p-values:   [0.01, 0.04, 0.03]     Adjusted p-values:   [0.03 0.06 0.06]</p>"},{"location":"stats/multiple.html#idk-correction","title":"idk correction","text":"<p>Source idk correction (Wikipedia).</p> <p>idk correction keeps Type I error rate of exactly \\(\\alpha\\) when the tests are independent from each other and all null hypotheses are true. It is less stringent than the Bonferroni correction, but only slightly.</p>"},{"location":"stats/multiple.html#algorithm-for-alpha","title":"Algorithm (for \\(\\alpha\\))","text":"<ol> <li>Compute p-values \\(p_{1}, \\dots, p_{m}\\) for all \\(m\\) hypotheses \\(H_{0}^{1}, \\dots, H_{0}^{m}\\).</li> <li>Reject null hypothesis \\(H_{0}^{i}\\) if and only if associated p-value \\(p_{i}\\) is lower or equal than \\(\\alpha_{SID} = 1 - (1 - \\alpha)^{\\frac{1}{m}}\\).</li> </ol>"},{"location":"stats/multiple.html#algorithm-for-p-values","title":"Algorithm (for p-values)","text":"<ol> <li>Compute p-values \\(p_{1}, \\dots, p_{m}\\) for all \\(m\\) hypotheses \\(H_{0}^{1}, \\dots, H_{0}^{m}\\).</li> <li>Order p-values from lowest to highest \\(p_{(1)} \\leq \\dots \\leq p_{(m)}\\)</li> <li>Calculate adjusted p-values \\(\\tilde{p}_{(i)}\\) using formula $$  \\tilde{p}{(i)}=      \\begin{cases}         1 - (1 - p &amp; \\text{for })^{m\\(i = 1\\)},\\         \\max \\big{ \\tilde{p}{(i-1)}, 1 - (1 - p \\big} &amp; \\text{for })^{m-i+1\\(i = 2, \\dots, m\\)}.      \\end{cases} $$</li> </ol>"},{"location":"stats/multiple.html#example_2","title":"Example","text":"<p><pre><code>decision, adj_pvals, sidak_aplha, bonf_alpha = statsmodels.stats.multitest.multipletests(pvals=pvals, alpha=0.05, method='sidak')\n\nprint(f'Original p-values: \\t {pvals}')\nprint(f'Adjusted p-values: \\t {adj_pvals}')\n</code></pre>     Original p-values:   [0.01, 0.04, 0.03]     Adjusted p-values:   [0.029701 0.115264 0.087327]</p>"},{"location":"stats/multiple.html#confidence-interval-adjustment","title":"Confidence Interval Adjustment","text":"<p>After p-values adjustment for Multiple comparisons problem it is also necessary to appropriately adjust confidence intervals. Fortunately this is an easy problem and it does not depend on chosen method. We use duality between p-values and confidence intervals.</p>"},{"location":"stats/multiple.html#algorithm_2","title":"Algorithm","text":"<ol> <li>Compute (original) p-values \\(p_{1}, \\dots, p_{m}\\) for all \\(m\\) hypotheses \\(H_{0}^{1}, \\dots, H_{0}^{m}\\).</li> <li>Based on chosen method compute adjusted p-values \\(\\tilde{p}_{1}, \\dots, \\tilde{p}_{m}\\).</li> <li>Compute adjustment ratios \\(r_{1}, \\dots, r_{m}\\) such that</li> </ol> \\[r_{i} = \\frac{p_{i}}{\\tilde{p}_{i}}, \\,\\,\\, i = 1, \\dots, m.\\] <ol> <li>Let denote desired confidence level(s) as usual by \\(\\alpha\\), e.g. \\(\\alpha = 0.05\\). More precisely let assume \\(\\alpha_{1}, \\dots, \\alpha_{m}\\).</li> <li>Compute adjusted confidence levels \\(\\tilde{\\alpha}_{1}, \\dots, \\tilde{\\alpha}_{m}\\) such that</li> </ol> \\[\\tilde{\\alpha_{i}} = r \\, \\alpha_{i}.\\] <ol> <li>Compute \\((1 - \\alpha_{i})\\)-confidence intervals such that you use \\(\\tilde{\\alpha}_{i}\\) instead of \\(\\alpha_{i}\\).</li> </ol> <p>Note</p> <p>Computed confidence intervals using \\(\\tilde{\\alpha}_{i}\\) instead of \\(\\alpha_{i}\\) are actually \\((1 - \\alpha_{i})\\)-confidence intervals. Levels of significance DO NOT change, they are still \\(1 - \\alpha_{i}\\), not \\(1 - \\tilde{\\alpha}_{i}\\).</p>"},{"location":"stats/multiple.html#example_3","title":"Example","text":"<p>Unadjusted p-value is \\(p_{i} = 0.01\\). Adjusted p-value is \\(\\tilde{p}_{i} = 0.03\\). Adjustment ratio is \\(r = 1/3\\). Desired level of significance is \\(\\alpha = 0.05\\). Adjusted level of significance is \\(\\tilde{\\alpha} = 0.05 \\cdot 1/3 = 0.01667\\). So we need to compute \\(1 - 0.01667 = 0.983 = 98.3\\%\\)-confidence interval in order to get \\(95\\%\\)-confidence interval.</p>"},{"location":"stats/multiple.html#python-package","title":"Python package","text":"<p>Link to documentation of Statsmodels package.</p>"},{"location":"stats/sample_size.html","title":"Sample Size","text":"<p>Setting a correct sample size is a key step when launching successful experiments. This page discuss how to calculate it and how to proceed if the required sample is too large.</p> <p>There is no free lunch in experimenting. Every experiment owner needs to navigate the trade-off between the selected metric, sample size, and ability to detect an impact of certain size.</p>"},{"location":"stats/sample_size.html#why-you-should-know-the-correct-sample-size","title":"Why You Should Know the Correct Sample Size","text":"<p>When your sample size is too small, your metrics will be noisy and you will not be able to replicate the experiment. You will be at increased risk of encountering two types of errors:</p> <ul> <li>Failure to detect an existing effect</li> <li>Measured effect is greater than in reality</li> </ul>"},{"location":"stats/sample_size.html#what-impacts-the-sample-size","title":"What Impacts the Sample Size","text":"<p>Sample size depends on the mean and variance of the selected metric and on the minimum effect of interest (MEI). MEI is the smallest relative difference between the control and treatment that is meaningful for the experiment owner to detect.</p> <p>While it can be insightful to run an experiment that is able to detect +1% difference as statistically significant, it does not always make sense to full-scale such an experiment. In many bussiness cases, the effect of +1% is just too small to offset the cost of a full-scale rollout.</p> <p>Compromising on the size of the effect we can measure by using higher MEI helps us limit the number of users we need to expose to an experiment. Smaller sample of users means smaller negative impact in case of an experiment going wrong.</p> <p>MEI is the smallest relative difference between the control and treatment that is meaningful for the experiment owner to detect.</p>"},{"location":"stats/sample_size.html#calculating-the-sample-size","title":"Calculating the Sample Size","text":"<p>Sample size per variant \\(n\\) can be computed as</p> \\[ n = \\frac{(Z_{1-\\alpha/2} + Z_{1-\\beta})^2(\\sigma_1^2 + \\sigma_2^2)}{\\Delta^2} \\] <ul> <li>\\(\\sigma_1\\), \\(\\sigma_2\\) are the standard deviations of the control and treatment respectively.</li> <li>\\(\\Delta = |\\mu_2 - \\mu_1| = \\mu_1\\mathrm{MEI}\\) is the difference between the control and treatment mean.</li> <li>\\(Z_{x} = \\Phi^{-1}(x)\\) where \\(\\Phi^{-1}\\) is the inverse CDF of the standard normal distribution \\(\\mathcal{N}(0, 1)\\).</li> <li>\\(\\alpha\\) is the false positive rate, \\(1-\\frac{\\alpha}{2}\\) is the confidence level of a two-sided test.</li> <li>\\(\\beta\\) is the false negative rate, \\(1 - \\beta\\) is the power.</li> </ul> <p>When the treatment standard deviation is unknown, we assume \\(\\sigma_1 = \\sigma_2\\). This gives us a simplified formula for power \\(1 - \\beta = 0.8\\) and \\(\\alpha = 0.05\\).</p> \\[ n \\approx \\frac{(Z_{0.975} + Z_{0.8})^2 2\\sigma_1^2}{\\Delta^2} \\approx \\frac{15.7\\sigma_1^2}{\\Delta^2} \\] <p>In case of conversion metrics such as CTR (click-through rate) that follow Bernoulli distribution, the treatment standard deviation is known because \\(\\sigma^2 = p(1-p)\\) where \\(p=\\mu\\) is the conversion rate.</p> \\[ \\mu_2 = \\mu_1(1 + \\mathrm{MEI}) \\\\ \\sigma^2_2 = p_2(1-p_2) = \\mu_2(1-\\mu_2) \\] <p>We can see that:</p> <ol> <li>Sample size increases as MEI decreases (because \\(\\Delta\\) is in the denominator).</li> <li>Conversion rates \\(p = 0.5\\) result in the largest required sample size (because of large \\(\\sigma^2\\)).</li> </ol>"},{"location":"stats/sample_size.html#low-conversion-rate","title":"Low Conversion Rate","text":"<p>The baseline rate of sales metrics such as conversions per view is usually below 1%. Low baseline metrics require large sample sizes to measure an experiment impact.</p> <p>The following figure shows how the required experiment size varies depending on the metric baseline. For example, we would need over 20,000 samples per variant to measure an impact of 20% when using a metric with 1% baseline conversion.</p> <p></p>"},{"location":"stats/sample_size.html#high-conversion-rate","title":"High Conversion Rate","text":"<p>CTR and other engagement metrics with higher baseline rates usually requires smaller experiment sizes.</p> <p>A metric with a baseline of 50%, needs less than 1000 samples to measure 20% impact. Why? Because when the MEI stays constant, high baseline \\(\\mu_1\\) translates to a large absolute difference \\(\\Delta = \\mu_1\\mathrm{MEI}\\).</p> <p></p>"},{"location":"stats/sample_size.html#required-sample-size-is-too-large","title":"Required Sample Size Is Too Large","text":"<p>Let's say we want to run an experiment that introduces a new feature to our product. While the ultimate goal is to increase subscription sales, metrics such as bookings per user might not be the right choice because of their low baseline requiring unreasonable large experiment sizes.</p> <p>One of the ways around this problem is to use a high baseline proxy metric (for example number of feature enagements) that is able to reflect the business impact of the new feature while needing significantly smaller samples.</p>"},{"location":"stats/sequential.html","title":"Sequential Evaluation","text":"<p>There is a strong demand from experimenters to statisticians for support of online decision making in online controlled experiments (OCE). Given big volume of observations and real-time nature of modern data pipelines, this demand is validated.</p> <p>This brings new issues to statistical evaluations. We cannot use standard Student\u2019s t-test, because it assumes that we evaluate data only once - after all experiment data has been collected.</p> <p>It has been known for a while, that evaluating experiments sequentially using standard t-tests increases dramatically type I errors (False-positive errors) that platforms try to keep below desired 5%. Using standard t-tests sequentially increases false-positive errors to a range of 20-30% or even higher making whole statistical inference useless.</p> <p>We verified this statement using simulations of A/A tests.</p> <p>We can evaluate experiments sequentially without exceeding desired false-positive errors using more sophisticated methods. Unfortunately, there is no unique approach how to deal with this problem. Multiple approaches differs mainly in mathematical complexity. Therefore we present a very basic one, so-called Alpha Spending Function<sup>1</sup>. Despite its simplicity, it had been used in clinical trials for decades, so we have a good reason to use it too.</p>"},{"location":"stats/sequential.html#peeking-at-temporary-results-is-wrong","title":"Peeking at Temporary Results is Wrong","text":"<p>We can say that any A/A experiment when ran long enough will show significant difference in the primary metric regardless of the validity of the null hypothesis. So there's 100% chance that we get false-positive result if we are not careful about this fact. We want to keep this error below 5% any-time. This is not possible without setting experiment duration before starting it. Knowing where we are in the period of experiment helps EP to provide results with controlled false-positive error rate any time.</p> <p>Following graph illustrates this problem. Test statistic measures difference in value of treatment variant metric to the value of control variant on unit scale. If we call the experiment off once the value of the test statistics crosses one of two constant boundaries (dashed lines) for the first time at day 6, we might be doing false-positive error compared to if we evaluated the experiment once completed at day 14.</p> <p></p>"},{"location":"stats/sequential.html#alpha-spending-function","title":"Alpha Spending Function","text":"<p>EP allows for safe early-stopping when observed difference is big enough. It requires the difference to be bigger early in the experiment than at the end of the experiment. Intuition is that if the experiment is doing really great or really poor, we do not need to wait full experiment duration to make a decision.</p> <p>Following graph shows decreasing decision boundary that requires observed difference to be big at the beginning of the experiment and decreases along the time. The decision boundary is called Alpha Spending Function and we implemented O'Brien-Fleming<sup>2</sup> version of it.</p> <p></p> <p>This process ensures below 5% false-positive errors at any time during the experiment but requires setting experiment duration upfront to be able to calculate decreasing boundaries. This requirement is covered in implementation of the experiment protocol.</p>"},{"location":"stats/sequential.html#simulation","title":"Simulation","text":"<p>We simulate 2000 A/A experiments for Click-through rate metric. We want to compare three possible solutions to sequential evaluation:</p> <ol> <li>Old-school - we evaluate the experiment once and after all data has been collected.</li> <li>p-Value hacking or peeking - we evaluate the experiment once a day during whole experiment duration using constant decision boundaries.</li> <li>Sequential experimenting - we use alpha spending function and evaluate once a day during whole experiment duration.</li> </ol> <p>We measure the quality of the solution by false-positive error rate.</p> <p>Following chart depicts development of A/A experiments in 14 days. We see that old-school and sequential experimenting methods keep false-positives below 5% while p-value hacking method shows false-positive errors around 20%.</p> <p>The longer we run the experiment the greater false-positive error rate we can expect. It can even be 29% for 30-day experiment.</p> <p></p> <p>We can conclude that sequential experimenting is the best method for our needs and simple enough to implement.</p> <ol> <li> <p>David L. DeMents, K. K. Gordon Lan, Interim analysis: The alpha spending function approach \u21a9</p> </li> <li> <p>P. C. O\u2019Brien, T.R. Fleming - A Multiple Testing Procedure for Clinical Trials \u21a9</p> </li> </ol>"},{"location":"user_guide/ab_test_simple_evaluation.html","title":"Ad-hoc A/B test evaluation using Ep-Stats","text":"In\u00a0[2]: Copied! <pre># This is only example to show required format of the input DataFrame\n# You have to prepare aggregated data on your own, e.g. using SQL\n\nfrom epstats.toolkit.testing import TestData\n\ngoals = TestData.load_goals_simple_agg()\ngoals\n</pre> # This is only example to show required format of the input DataFrame # You have to prepare aggregated data on your own, e.g. using SQL  from epstats.toolkit.testing import TestData  goals = TestData.load_goals_simple_agg() goals  Out[2]: experiment variant views clicks conversions bookings bookings_squared 0 test-simple-metric a 473661 48194 413 17152 803105 1 test-simple-metric b 471485 47184 360 14503 677178 <p>For continous metrics like RPM (RPM = bookings / views * 1000) it is necessary to prepare squared values - in this case we have columns <code>bookings</code> and <code>bookings_squared</code>.</p> <p>Lets assume we have $K$ purchases. Hence the exact definition of columns <code>bookings</code> and <code>bookings_squared</code> is following</p> <p>$$\\text{bookings} = \\sum_{i=1}^{K} \\text{purchase_value}_{i}$$ $$\\text{bookings_squared} = \\sum_{i=1}^{K} (\\text{purchase_value}_{i})^2$$</p> <p>This is not necessary for binary metrics like Click-through Rate or Conversion Rate.</p> In\u00a0[3]: Copied! <pre>from epstats.toolkit import Experiment, SimpleMetric, SimpleSrmCheck\n\nunit_type='test_unit_type'  # this is only technical detail; it has no impact on the results\n\n# Experiment Definition\nexperiment = Experiment(\n    'test-simple-metric',\n    'a',\n    [\n        SimpleMetric(1, 'Click-through Rate (CTR)', 'clicks', 'views', unit_type),\n        SimpleMetric(2, 'Conversion Rate', 'conversions', 'views', unit_type),\n        SimpleMetric(3, 'Revenue per Mille (RPM)', 'bookings', 'views', unit_type, metric_format='${:,.2f}', metric_value_multiplier=1000),\n    ],\n    [SimpleSrmCheck(1, 'SRM', 'views')],\n    unit_type=unit_type)\n\n# Experiment Evaluation\n# `goals` is the DataFrame you have prepared on your own, e.g. using SQL\nev = experiment.evaluate_wide_agg(goals)\n\n# Resluts\nev.checks\nev.metrics\n</pre> from epstats.toolkit import Experiment, SimpleMetric, SimpleSrmCheck  unit_type='test_unit_type'  # this is only technical detail; it has no impact on the results  # Experiment Definition experiment = Experiment(     'test-simple-metric',     'a',     [         SimpleMetric(1, 'Click-through Rate (CTR)', 'clicks', 'views', unit_type),         SimpleMetric(2, 'Conversion Rate', 'conversions', 'views', unit_type),         SimpleMetric(3, 'Revenue per Mille (RPM)', 'bookings', 'views', unit_type, metric_format='${:,.2f}', metric_value_multiplier=1000),     ],     [SimpleSrmCheck(1, 'SRM', 'views')],     unit_type=unit_type)  # Experiment Evaluation # `goals` is the DataFrame you have prepared on your own, e.g. using SQL ev = experiment.evaluate_wide_agg(goals)  # Resluts ev.checks ev.metrics  Out[3]: timestamp exp_id metric_id metric_name exp_variant_id count mean std sum_value confidence_level diff test_stat p_value confidence_interval standard_error degrees_of_freedom 0 1615928648 test-simple-metric 1 Click-through Rate (CTR) a 473661 0.101748 0.302317 48194 0.95 0 0 1 0.0119665 0.00610546 947320 1 1615928648 test-simple-metric 1 Click-through Rate (CTR) b 471485 0.100075 0.300101 47184 0.95 -0.0164385 -2.72161 0.00649657 0.0118382 0.00603998 945136 2 1615928648 test-simple-metric 2 Conversion Rate a 473661 0.000871932 0.0295156 413 0.95 0 0 1 0.136333 0.0695586 947320 3 1615928648 test-simple-metric 2 Conversion Rate b 471485 0.000763545 0.0276218 360 0.95 -0.124306 -1.96949 0.048897 0.123705 0.063116 941568 4 1615928648 test-simple-metric 3 Revenue per Mille (RPM) a 473661 0.0362116 1.30162 17152 0.95 0 0 1 0.144766 0.0738616 947320 5 1615928648 test-simple-metric 3 Revenue per Mille (RPM) b 471485 0.0307603 1.19805 14503 0.95 -0.15054 -2.29841 0.0215384 0.128373 0.0654974 939408 In\u00a0[4]: Copied! <pre>from epstats.toolkit.results import results_long_to_wide, format_results\n\nev.metrics.pipe(results_long_to_wide)\n</pre> from epstats.toolkit.results import results_long_to_wide, format_results  ev.metrics.pipe(results_long_to_wide) Out[4]: metric_name Click-through Rate (CTR) Conversion Rate Revenue per Mille (RPM) statistic mean diff conf_int_lower conf_int_upper p_value mean diff conf_int_lower conf_int_upper p_value mean diff conf_int_lower conf_int_upper p_value exp_id exp_variant_id test-simple-metric A 0.101748 0 -0.0119665 0.0119665 1 0.000871932 0 -0.136333 0.136333 1 0.0362116 0 -0.144766 0.144766 1 B 0.100075 -0.0164385 -0.0282766 -0.00460032 0.00649657 0.000763545 -0.124306 -0.248012 -0.000601163 0.048897 0.0307603 -0.15054 -0.278913 -0.0221675 0.0215384 In\u00a0[5]: Copied! <pre>ev.metrics.pipe(results_long_to_wide).pipe(format_results, experiment, format_pct='{:.1%}', format_pval='{:.3f}')\n</pre> ev.metrics.pipe(results_long_to_wide).pipe(format_results, experiment, format_pct='{:.1%}', format_pval='{:.3f}') Out[5]: Metric Click-through Rate (CTR) Conversion Rate Revenue per Mille (RPM) Statistics Mean Impact Conf. interval lower bound Conf. interval upper bound p-value Mean Impact Conf. interval lower bound Conf. interval upper bound p-value Mean Impact Conf. interval lower bound Conf. interval upper bound p-value Experiment Id Variant test-simple-metric A 10.17% 0.0% -1.2% 1.2% 1.000 0.09% 0.0% -13.6% 13.6% 1.000 $36.21 0.0% -14.5% 14.5% 1.000 B 10.01% -1.6% -2.8% -0.5% 0.006 0.08% -12.4% -24.8% -0.1% 0.049 $30.76 -15.1% -27.9% -2.2% 0.022"},{"location":"user_guide/ab_test_simple_evaluation.html#ad-hoc-ab-test-evaluation-using-ep-stats","title":"Ad-hoc A/B test evaluation using Ep-Stats\u00b6","text":"<p>This is a simplified version of general manual Using Ep-Stats in Jupyter. In this case we assume simple DataFrame at the input. It should contain aggregated data of an A/B test in a wide format.</p> <p>Next we define metrics and checks we are interested in. Finally, we evaluate the experiment and nicely formate results.</p>"},{"location":"user_guide/ab_test_simple_evaluation.html#input-dataframe-example","title":"Input DataFrame Example\u00b6","text":"<p>Mind that you need to prepare experiment data on your own. Following example is only illustrative.</p> <p>You should be aware of following assumptions:</p> <ol> <li>First two columns must contain name of the experiment and variants. Names of the columns may vary.</li> <li>It is necessary to download squared values for continuous metrics like Revenue per Mille (RPM). If you forget to do it, results will be wrong and misleading. Ep-Stats will not warn you about this issue!</li> </ol>"},{"location":"user_guide/ab_test_simple_evaluation.html#experiment-definition-and-evaluation","title":"Experiment Definition and Evaluation\u00b6","text":"<p>Firstly, you need to define metrics you want to evaluate. You can define as many metrics as you want. While creating the instance of the class <code>SimpleMetric</code> you need to specify parameters <code>id</code>, <code>name</code>, <code>numerator</code> and <code>denominator</code>. Further you can specify optional parameters <code>metric_format</code>, e.g. '${:,.1f}' for RPM, and parameter <code>metric_value_multiplier</code>, e.g. 1000 for RPM. The last optional parameter <code>unit_type</code> is preset only for technical reason. Be aware there can be used only one <code>unit_type</code> within one experiment. The value of this parameter has no impact on the evaluation.</p> <p>Secondly, you can define checks as well by creating the instance of the class <code>SimpleSrmCheck</code>. It is not mandatory to define checks - keep it empty if you do not need one.</p> <p>You wrap both metrics and checks definitions inside the Experiment definition. For more details see <code>Experiment</code>.</p> <p>Finally, you evaluate the experiment calling method <code>evaluate_wide_agg</code> method, for details see <code>Experiment.evaluate_wide_agg()</code>. The results for metrics and checks are separated.</p>"},{"location":"user_guide/ab_test_simple_evaluation.html#formatting-results","title":"Formatting Results\u00b6","text":"<p>You may find useful two methods for nice presentation of results - <code>results_long_to_wide</code> and <code>format_results</code>.</p> <p>The former simply convert results from long format to wide one. The later then provide extra tuning. You can set number of decimals defining parameters <code>format_pct</code> and <code>format_pval</code> respectively.</p>"},{"location":"user_guide/aggregation.html","title":"Aggregation","text":""},{"location":"user_guide/aggregation.html#goals-in-metric-definition","title":"Goals in Metric Definition","text":"<p>Goals in the metric definition look like</p> <p><code>value(test_unit_type.unit.conversion)</code></p> <p>where</p> <ul> <li><code>count</code>, <code>value</code>, or <code>unique</code> determines we need number of goals recorded or their value (e.g. USD bookings of <code>conversion</code> goal) or that we need only info if at least 1 goal has been collected.</li> <li><code>test_unit_type</code> is a type of the unit the goal has been recorded for.</li> <li><code>unit</code> or <code>global</code> are types of aggregation.</li> <li><code>conversion</code> is a name of the goal.</li> </ul>"},{"location":"user_guide/aggregation.html#supported-unit-types","title":"Supported Unit Types","text":"<p>Ep-stats support any type of randomization unit. It is a responsibility of an integrator to correctly query for the data.</p>"},{"location":"user_guide/aggregation.html#note-on-randomization","title":"Note on Randomization","text":"<p>It is necessary for the statistics to work correctly that unit exposures are randomly (independently and identically a.k.a. IID) distributed within one experiment into its variants. This is usually the case when we randomize at page view or event session unit types.</p> <p>In general, one unit can experience the experiment variant multiple times.</p> <p>Violation of IID leads to uncontrolled false-positive errors in metric evaluation. Ep-stats remedies this IID violation by using delta method for IID.</p>"},{"location":"user_guide/aggregation.html#aggregation-types","title":"Aggregation Types","text":"<p>There are 2 types or levels of aggregation of goals available:</p> <ol> <li><code>global</code> aggregates goals as if one goal is one observation.</li> <li><code>unit</code> aggregates goals first per unit (e.g. count of <code>conversion</code> goals per unit id <code>unit_1</code>).</li> </ol>"},{"location":"user_guide/aggregation.html#unit-aggregation-type","title":"Unit Aggregation Type","text":"<p>We need to use unit aggregation type when we calculate any \"per User\" or generally any \"per exposure\" metrics. It is \"per User\" metric so we need all goals that happened for one user represented as one observation of one unit. This is required for correct calculation of sample standard deviation which is a basic block in all statistical evaluation methods.</p> <p>For example, if there are 2 <code>conversion</code> goals for unit id <code>unit_1</code>, we want to have 2 as a count of <code>conversion</code> goals for this unit id making it 1 observation rather than having 2 separate observations of <code>conversion</code> goals.</p>"},{"location":"user_guide/aggregation.html#global-aggregation-type","title":"Global Aggregation Type","text":"<p>Global aggregation type skips \"per User\" aggregation step described in previous section and treats every goal as one observation. This is now enough for all metrics that are not based directly on exposures of the experiment randomization unit type.</p> <p>For example Refund Rate metric defined as <code>count(test_unit_type.global.refund) / count(test_unit_type.global.conversion)</code> does not need to use unit aggregation type because 1 observation is 1 <code>conversion</code> that can have only zero or one <code>refund</code> goal.</p> <p>This kind of metrics needs application of delta method or bootstrapping which has not been implemented yet.</p>"},{"location":"user_guide/aggregation.html#dimensional-goals","title":"Dimensional Goals","text":"<p>Metric goal definition allows to filter goals by supported dimensions. For example we can specify that we want transactional metrics to be calculated only for transactions involving VPN products. Or we want Revenue per Mile or CTR metrics calculated for particular screen.</p> <p>For example Average Bookings of product <code>p_1</code> can be defined as</p> <pre><code>value(test_unit_type.unit.conversion(product=p_1)) / count(test_unit_type.global.exposure)\n</code></pre> <p>Different goals may allow filtering by different dimensions.</p> <p>Multiple dimensions per goal are allowed.</p> <pre><code>value(test_unit_type.unit.conversion(product=p_1, country=A)) / count(test_unit_type.global.exposure)\n</code></pre> <p>Note</p> <p>Dimensions are not supported yet.</p>"},{"location":"user_guide/aggregation.html#example","title":"Example","text":"<p>Following SQL snippet shows how top-level aggregation should be made to obtain <code>goals</code> for <code>Experiment.evaluate_agg</code>.</p> <pre><code>SELECT\n    exp_id,\n    exp_variant_id,\n    unit_type,\n    agg_type,\n    goal,\n    SUM(sum_cnt) count,\n    SUM(sum_cnt * sum_cnt) sum_sqr_count,\n    SUM(value) sum_value,\n    SUM(value * value) sum_sqr_value,\n    CAST(SUM(unique) AS Int64) count_unique\n    FROM (\n        SELECT\n            exp_id,\n            exp_variant_id,\n            unit_type,\n            agg_type,\n            goal,\n            unit_id,\n            SUM(cnt) sum_cnt,\n            SUM(value) value,\n            IF(SUM(cnt) &gt; 0, 1, 0) unique\n            FROM events.table\n            GROUP BY\n                exp_id,\n                exp_variant_id,\n                unit_type,\n                agg_type,\n                goal,\n                unit_id\n    ) u\n    GROUP BY\n        exp_id,\n        exp_variant_id,\n        unit_type,\n        agg_type,\n        goal,\n</code></pre> <p>See Test Data for examples of pre-aggregated goals that make input to statistical evaluation using <code>Experiment.evaluate_agg</code> or per unit goals that make input to statistical evaluation using <code>Experiment.evaluate_by_unit</code>.</p>"},{"location":"user_guide/configuring_api.html","title":"Integration","text":"<p>This short integration guide assumes you are familiar with Basic Principles.</p>"},{"location":"user_guide/configuring_api.html#access-to-data","title":"Access to Data","text":"<p>To evaluate a metric in experiment, we have to compile metric definition that comes in form of nominator and denominator expressions into some underlying data source in the form that is vastly company or use-case specific. We use class <code>DAO</code> to interface underlying data source. <code>DAO</code> gets all the information contained in <code>Experiment</code> and needs to compile it into SQL or something else understandable by company's data systems to provide pre-aggregated or by-unit goals.</p> <p>Following snippet shows one way how to aggregate data to provide input in form of pre-aggregated goals in some implementation of <code>DAO</code> class.</p> <pre><code>SELECT\n    -- we aggregate secondly by all dims required by ep-stats and omit `unit_id`\n    -- this way we get correct $\\sum x^2$ values in `sum_sqr_value` to calculate\n    -- correct sample standard deviation of real-valued metrics.\n    exp_id,\n    exp_variant_id,\n    unit_type,\n    agg_type,\n    goal,\n    SUM(sum_cnt) count,\n    SUM(sum_cnt * sum_cnt) sum_sqr_count,\n    SUM(value) sum_value,\n    SUM(value * value) sum_sqr_value,\n    SUM(unique) count_unique\n    FROM (\n        -- we aggregate firstly by all dims required by ep-stats and by `unit_id`\n        SELECT\n            exp_id,\n            exp_variant_id,\n            unit_type,\n            agg_type,\n            goal,\n            unit_id,\n            SUM(cnt) sum_cnt,\n            SUM(value) value,\n            IF(SUM(cnt) &gt; 0, 1, 0) unique\n            FROM events\n            GROUP BY\n                exp_id,\n                exp_variant_id,\n                unit_type,\n                agg_type,\n                goal,\n                unit_id\n    ) u\n    GROUP BY\n        exp_id,\n        exp_variant_id,\n        unit_type,\n        agg_type,\n        goal\n</code></pre>"},{"location":"user_guide/configuring_api.html#configuring-rest-api","title":"Configuring REST API","text":"<p>After having access to our data in custom implementation of <code>Dao</code> class e.g. <code>CustomDao</code>, we can follow up an example in <code>main.py</code> to configure the REST API with our <code>CustomDao</code>. We need to implement <code>CustomDaoFactory</code> that creates instances of our <code>CustomDao</code> for every request served. We can then customize <code>get_dao_factory()</code> method in <code>main.py</code> and to launch the server.</p> <pre><code>def get_dao_factory():\n    return CustomDaoFactory(...)\n\n\ndef main():\n    from .config import config\n\n    logging.config.dictConfig(config['logging'])\n    serve('my_package:api', settings.api, config['logging'])\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"user_guide/ep_in_python.html","title":"Using Ep-Stats from Python","text":"In\u00a0[1]: Copied! <pre>from epstats.toolkit import Experiment, Metric, SrmCheck\nexperiment = Experiment(\n    'test-conversion',\n    'a',\n    [Metric(\n        1,\n        'Click-through Rate',\n        'count(test_unit_type.unit.click)',\n        'count(test_unit_type.global.exposure)'),\n    ],\n    [SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')],\n    unit_type='test_unit_type')\n\n# This gets testing data, use other Dao or get aggregated goals in some other way.\nfrom epstats.toolkit.testing import TestData\ngoals = TestData.load_goals_agg(experiment.id)\n\n# evaluate experiment\nev = experiment.evaluate_agg(goals)\n</pre> from epstats.toolkit import Experiment, Metric, SrmCheck experiment = Experiment(     'test-conversion',     'a',     [Metric(         1,         'Click-through Rate',         'count(test_unit_type.unit.click)',         'count(test_unit_type.global.exposure)'),     ],     [SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')],     unit_type='test_unit_type')  # This gets testing data, use other Dao or get aggregated goals in some other way. from epstats.toolkit.testing import TestData goals = TestData.load_goals_agg(experiment.id)  # evaluate experiment ev = experiment.evaluate_agg(goals) <p>Number of exposures per variant.</p> In\u00a0[2]: Copied! <pre>ev.exposures\n</pre> ev.exposures Out[2]: exp_variant_id exposures exp_id 0 a 21.0 test-conversion 1 b 26.0 test-conversion 2 c 30.0 test-conversion <p>Metrics evaluations, see <code>Evaluation.metric_columns</code> for column value meanings.</p> In\u00a0[3]: Copied! <pre>ev.metrics\n</pre> ev.metrics Out[3]: timestamp exp_id metric_id metric_name exp_variant_id count mean std sum_value confidence_level diff test_stat p_value confidence_interval standard_error degrees_of_freedom 0 1607977256 test-conversion 1 Click-through Rate a 21 0.238095 0.436436 5 0.95 0 0 1 1.14329 0.565685 40 1 1607977256 test-conversion 1 Click-through Rate b 26 0.269231 0.452344 7 0.95 0.130769 0.223152 1 1.23275 0.586008 43.5401 2 1607977256 test-conversion 1 Click-through Rate c 30 0.3 0.466092 9 0.95 0.26 0.420806 1 1.35281 0.617862 44.9314 <p>SRM check results, p-value &lt; 0.001 signals problem in experiment randomization. See Sample Ratio Mismatch Check for details.</p> In\u00a0[4]: Copied! <pre>ev.checks\n</pre> ev.checks Out[4]: timestamp exp_id check_id check_name variable_id value 0 1607977256 test-conversion 1 SRM p_value 0.452844 1 1607977256 test-conversion 1 SRM test_stat 1.584416 2 1607977256 test-conversion 1 SRM confidence_level 0.999000 In\u00a0[5]: Copied! <pre>goals['date'] = '2020-08-01'\ngoals['count_unique'] = goals['count']\ngoals\n</pre> goals['date'] = '2020-08-01' goals['count_unique'] = goals['count'] goals Out[5]: exp_id date exp_variant_id unit_type agg_type goal dimension dimension_value element count sum_sqr_count sum_value sum_sqr_value count_unique 0 test-conversion 2020-08-01 a test_unit_type unit click NaN 5 5 5 5 5 1 test-conversion 2020-08-01 b test_unit_type unit click NaN 7 7 7 7 7 2 test-conversion 2020-08-01 c test_unit_type unit click NaN 9 9 9 9 9 3 test-conversion 2020-08-01 a test_unit_type global exposure NaN 21 21 21 21 21 4 test-conversion 2020-08-01 b test_unit_type global exposure NaN 26 26 26 26 26 5 test-conversion 2020-08-01 c test_unit_type global exposure NaN 30 30 30 30 30 <p>Following SQL pseudo code shows how we first aggregate data per experiment unit id (to get aggregates per-user) and then how we aggregate without unit id to get pre-aggregated goals dataframe.</p> In\u00a0[11]: Copied! <pre>\"\"\"\nSELECT\n    exp_id,\n    exp_variant_id,\n    unit_type,\n    agg_type,\n    goal,\n    dimension,\n    dimension_value,\n    SUM(sum_cnt) count,\n    SUM(sum_cnt * sum_cnt) sum_sqr_count,\n    SUM(value) sum_value,\n    SUM(value * value) sum_sqr_value,\n    CAST(SUM(unique) AS Int64) count_unique\n    FROM (\n        SELECT\n            exp_id,\n            exp_variant_id,\n            unit_type,\n            agg_type,\n            goal,\n            dimension,\n            dimension_value,\n            unit_id,\n            SUM(cnt) sum_cnt,\n            SUM(value) value,\n            IF(SUM(cnt) &gt; 0, 1, 0) unique\n            FROM events.table\n            GROUP BY\n                exp_id,\n                exp_variant_id,\n                unit_type,\n                agg_type,\n                goal,\n                dimension,\n                dimension_value,\n                unit_id\n    ) u\n    GROUP BY\n        exp_id,\n        exp_variant_id,\n        unit_type,\n        agg_type,\n        goal,\n        dimension,\n        dimension_value\n\"\"\";\n</pre> \"\"\" SELECT     exp_id,     exp_variant_id,     unit_type,     agg_type,     goal,     dimension,     dimension_value,     SUM(sum_cnt) count,     SUM(sum_cnt * sum_cnt) sum_sqr_count,     SUM(value) sum_value,     SUM(value * value) sum_sqr_value,     CAST(SUM(unique) AS Int64) count_unique     FROM (         SELECT             exp_id,             exp_variant_id,             unit_type,             agg_type,             goal,             dimension,             dimension_value,             unit_id,             SUM(cnt) sum_cnt,             SUM(value) value,             IF(SUM(cnt) &gt; 0, 1, 0) unique             FROM events.table             GROUP BY                 exp_id,                 exp_variant_id,                 unit_type,                 agg_type,                 goal,                 dimension,                 dimension_value,                 unit_id     ) u     GROUP BY         exp_id,         exp_variant_id,         unit_type,         agg_type,         goal,         dimension,         dimension_value \"\"\";"},{"location":"user_guide/ep_in_python.html#using-ep-stats-from-python","title":"Using Ep-Stats from Python\u00b6","text":"<p>We can call ep-stats as regular python package to evaluate any experiment from any data. We can define arbitrary goals and metrics if we are able to select goals from our primary data store.</p> <p>Make sure please to read and understand basic Principles of EP before using this notebook.</p>"},{"location":"user_guide/ep_in_python.html#evaluate","title":"Evaluate\u00b6","text":"<p>We define experiment with one Click-through Rate metric to evaluate.</p> <p>We load testing pre-aggregated goals data using <code>TestData.load_goals_agg</code>.</p> <p>See <code>Experiment.evaluate_agg</code> for details.</p>"},{"location":"user_guide/ep_in_python.html#how-to-prepare-goals-dataframe","title":"How to Prepare Goals Dataframe\u00b6","text":"<p>You have to prepare the goals input dataframe from your data and follow description at either <code>Experiment.evaluate_agg</code> or <code>Experiment.evaluate_by_unit</code>.</p> <p>The goals dataframe must contain data to evaluate all metrics. Per-user metrics require that you first group by including some experiment randomization unit id (<code>unit_id</code>) column to get correct value for <code>sum_sqr_count</code> and <code>sum_sqr_value</code>, then you group by without it to get pre-aggregated data.</p> <p>This is an example of goals dataframe used to evaluate experiment <code>test-conversion</code> above.</p>"},{"location":"user_guide/protocol.html","title":"Experimentation Protocol","text":"<p>We try to be well aware that implementation of experimentation protocol is what all of us experimenters will find the most difficult to understand and get done correctly when running experiments in teams and scaling the experimentation up to the whole companies.</p> <p>The experimentation protocol we are implementing is here to rather enable us to safely run and evaluate more experiments than to limit us from expanding our experimentation.</p> <p>Experimentation is a work with uncertainty where errors and erroneous decisions were and will be a necessary part of it. When experimenting on a larger scale, errors happen. It\u2019s no longer a \u201csmall chance of error\u201d in case when I am running one or two experiments. Errors happen with a specific chance. This chance must be monitored and controlled for.</p> <p>It will always be true that we will be making wrong decisions based on experiment data. There is no way out of it. We can only control it. It is not about being statistically correct, it is that we can and should know that we are making the right decision 95 times and the wrong one 5 times out of every 100 decisions we make. Not knowing and controlling for this, we are playing a game where we do not know our odds of a good or a bad decision.</p> <p>Only by joint effort among systems, data, and experimenters, we can guarantee to keep  erroneous decisions at bay.</p> <p></p> <p>Decisions we make are the most important outcome from experimenting, we need to keep bad decisions at bay. To do that, we need to set experiment duration upfront and follow the experimentation protocol. Following experimentation protocol is not special to us, it is a proven concept from clinical trials and also from online controlled experiments, it ensures that we are making the right decisions most of the time.</p> <p>Setting experiment duration upfront is certainly the most difficult point. We are not sure if current implementation (still in development) is the best one, we will need to iterate based on your feedback. We will have a calculator that helps to set duration almost in a conversational manner. In future iterations, we can run automated pre-tests to determine audience sizes and baseline conversion rates, we can improve the statistics, we can make it a lot easier and relax experimenters from filling up what we have to do today.</p> <p>There is one more immediate benefit from setting the duration upfront, we are able to stop experiments early much before the set duration if results are good (or bad) enough. And we can do early stopping safely with again keeping errors at bay below 5%. I personally believe that this possibility of early stopping without introduction of further error into our decision making is worth it.</p>"},{"location":"user_guide/protocol.html#experimentation-protocol-in-depth","title":"Experimentation Protocol in Depth","text":"<p>General aim of experimenting is to test a new idea (feature) on a fraction of users. Based on observed behavior of users exposed to the experiment, we want to infer what will happen if we full-scale the change to the whole user base. We are looking for the best option as well as we are limiting a risk of full-scaling harmful ideas (features). The experimenting is a work with uncertainty and thus it includes inevitable errors.</p> <p>We distinguish two types of errors in experimenting:</p> <ol> <li>Type I error (False-positive error)<ul> <li>We DO detect falsely significant result as significant</li> <li>Null hypothesis is true and we reject it</li> </ul> </li> <li>Type II error (False-negative error)<ul> <li>We DO NOT detect truly significant result as significant</li> <li>Null hypothesis is false and we do not reject it</li> </ul> </li> </ol> <p>The Experimentation Platform keeps false-positive errors below 5% by</p> <ol> <li>Implementation of sound statistical evaluation (engineers and data people).</li> <li>Asking experimenters to (dully) follow Experimentation Protocol.</li> </ol> <p>By ignoring Experimentation Protocol you can drastically increase false-positive errors (even multiple times) making results of the experiment highly misleading! There should not be done any business decision based on such results.</p> <p>When implementing the experimentation protocol, we followed a meta-study Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale that collects best-practices from the leading experimentation platforms in Booking.com, Microsoft, Intuit and others. We also studied how online platforms like Optimizely, Google Optimize, VWO, AB Smartly implement experimentation protocols to come up with the meaningful yet powerful minimum.</p> <p>The experimentation protocol requires the experimenter to adhere to following steps:</p> <ol> <li>Formulate Hypothesis</li> <li>Pick Primary Metric</li> <li>Set Experiment Duration</li> <li>Check Guardrail Metrics</li> </ol>"},{"location":"user_guide/protocol.html#formulate-hypothesis","title":"Formulate Hypothesis","text":"<p>Experiment hypothesis should be defined and should be falsifiable. Experiment analysis begins with the transformation of an idea for testing into an experiment. The first step in this process is the impact analysis. Every change for testing should be introduced with a description of what the change that will be evaluated is, who will see the change, what the expected impact is, and how this impact is connected to the top-level business goals (increase in revenue). Most importantly, an impact analysis should contain a line of reasoning \u2013 belief - that explains why a change is expected to have an impact. Common questions that help in evaluating this item are \u201cWhy are we improving this?\u201d, \u201cWhy is existing state not good enough?\u201d, \u201cWhy will the change make a difference?\u201d, \u201cWill it help the users, the business, or both?\u201d<sup>1</sup></p>"},{"location":"user_guide/protocol.html#pick-primary-metric","title":"Pick Primary Metric","text":"<p>Picking primary metric helps to frame the experiment by determining what is the most important to measure. It also helps to think about possible next steps after the experiment will be over. Think about what you will do in all 3 possible experiment results:</p> <ol> <li>Significant positive impact</li> <li>Significant negative impact</li> <li>Inconclusive results</li> </ol>"},{"location":"user_guide/protocol.html#set-experiment-duration-before-starting-it","title":"Set Experiment Duration Before Starting It","text":"<p>The length of the experiment must be set upfront (before starting the experiment). If we do not set it up, then when should we stop? Immediately when the difference in the primary metric is significant? Or should we wait two more days and stop it then?</p> <p>Imagine you are developing a new version of a drug (medicament) and now you want to test a new version versus an old version. How will you proceed? Probably you will select a group of people and test it on them. If the difference between old and new versions is not statistically significant in a selected group of people, you end up experimenting with the result that the new drug is not outperforming the old one. But definitely you will not be adding more and more people to the experiment just to prove the new one is better. The same holds for online experiments.</p> <p>The experimenter must estimate the necessary duration of the experiment carefully. The best way to do it is to use the Sample Size Calculator implemented right in the Experimentation Platform, or ask colleagues, or ping EP Support team.</p> <p>The Experimentation Platform supports safe early-stopping of the experiment. If the experiment is performing really great or really poor, we can stop it right now and make a decision. We do not need to wait until the end of the experiment. For the right and safe set up of early-stopping criteria, we need experimenters to set up the appropriate duration of the experiment upfront. This is alpha and omega of controlling false-positive errors - falsely claiming untrue effect to be a true effect.</p> <p>See Sequential Analysis for detailed explanation.</p>"},{"location":"user_guide/protocol.html#check-guardrail-metrics","title":"Check Guardrail Metrics","text":"<p>Many things can go wrong during the experiment running phase. We might not get IID exposures in variants, data pipelines could fail, there could be client-side bugs in the experiment implementation. It is necessary to check for possible bugs before evaluating the experiment.</p> <p>While EP is not a platform to check all possible errors, it does the best to highlight and warn about them. EP does Sample Ratio Mismatch check for all experiments by default and it offers several guardrail metrics that could be part of the secondary metric definition.</p>"},{"location":"user_guide/protocol.html#sample-ratio-mismatch","title":"Sample Ratio Mismatch","text":"<p>Checking for SRM-Sample Ratio Mismatch, is an important test everyone doing experimenting should do. If you designed your experiment with equal percentages and got 821,588 vs. 815,482 users (50.2% instead of 50%), stop and find the bug<sup>2</sup>.</p> <p>Failing SRM check tells us there is some underlying problem in the experiment randomization. Experiments with failed SRM check should not be evaluated at all.</p> <p>See SRM for details about EP implementation.</p>"},{"location":"user_guide/protocol.html#pros-and-cons","title":"Pros and Cons","text":"<p>On one hand we require experimenters to set experiment duration upfront, on the other hand we allow for safely calling the experiment off any time when measured differences are big enough.</p> <ol> <li> <p>Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale \u21a9</p> </li> <li> <p>R. Kohavi, Sample Ratio Mismatch \u21a9</p> </li> </ol>"},{"location":"user_guide/quick_start.html","title":"Quick Start","text":""},{"location":"user_guide/quick_start.html#installation","title":"Installation","text":"<p>You can install this package via <code>pip</code>.</p> <pre><code>pip install ep-stats\n</code></pre>"},{"location":"user_guide/quick_start.html#running","title":"Running","text":"<p>You can run a testing version of ep-stats via</p> <pre><code>python -m epstats\n</code></pre> <p>Then see Swagger on http://localhost:8080/docs for API documentation.</p>"},{"location":"user_guide/quick_start.html#contributing","title":"Contributing","text":"<p>To get started locally, you can clone the repo and quickly get started using the <code>Makefile</code>.</p> <pre><code>git clone https://github.com/avast/ep-stats.git\ncd ep-stats\nmake install-dev\n</code></pre> <p>It sets a new virtual environment <code>venv</code> in <code>./venv</code> using venv, installs all development dependencies, and sets pre-commit git hooks to keep the code neatly formatted with flake8 and brunette.</p> <p>To run tests, you can use <code>Makefile</code> as well.</p> <pre><code>source venv/bin/activate  # activate python environment\nmake check\n</code></pre> <p>To run a development version of ep-stats do</p> <pre><code>source venv/bin/activate\ncd src\npython -m epstats\n</code></pre>"},{"location":"user_guide/quick_start.html#base-example","title":"Base Example","text":"<p>Ep-stats allows for quick experiment evaluation. We are using provided testing data to evaluate metric <code>Click-through Rate</code> in experiment <code>test-conversion</code>.</p> <pre><code>from epstats.toolkit import Experiment, Metric, SrmCheck\nexperiment = Experiment(\n    'test-conversion',\n    'a',\n    [Metric(\n        1,\n        'Click-through Rate',\n        'count(test_unit_type.unit.click)',\n        'count(test_unit_type.global.exposure)'),\n    ],\n    [SrmCheck(1, 'SRM', 'count(test_unit_type.global.exposure)')],\n    unit_type='test_unit_type')\n\n# This gets testing data, use other Dao or get aggregated goals in some other way.\nfrom epstats.toolkit.testing import TestData\ngoals = TestData.load_goals_agg(experiment.id)\n\n# evaluate experiment\nev = experiment.evaluate_agg(goals)\n</code></pre> <p><code>ev</code> contains evaluations of exposures, metrics and checks. This will have following output.</p> <p><code>ev.exposures</code>:</p> exp_id exp_variant_id exposures test-conversion a 21 test-conversion b 26 <p><code>ev.metrics</code>:</p> exp_id metric_id metric_name exp_variant_id count mean std sum_value confidence_level diff test_stat p_value confidence_interval standard_error degrees_of_freedom test-conversion 1 Click-through Rate a 21 0.238095 0.436436 5 0.95 0 0 1 1.14329 0.565685 40 test-conversion 1 Click-through Rate b 26 0.269231 0.452344 7 0.95 0.130769 0.223152 0.82446 1.18137 0.586008 43.5401 <p><code>ev.checks</code>:</p> exp_id check_id check_name variable_id value test-conversion 1 SRM p_value 0.465803 test-conversion 1 SRM test_stat 0.531915 test-conversion 1 SRM confidence_level 0.999000"},{"location":"user_guide/test_data.html","title":"Test Data","text":"<p>We made testing data part of the <code>epstats</code> python package to simplify development and to provide real example of input data and formats required by <code>epstats</code>.</p> <p>There are test goal data in both pre-aggregated and by-unit forms. See <code>TestData</code> for various access methods.</p> <p>Test data itself are saved as csv files in src/epstats/toolkit/testing/resources. They include pre-aggregated and by-unit goals together with pre-computed evaluations of metrics, checks, and exposures that are used to assert our unit-tests against (e.g. in <code>test_experiment.py</code>).</p>"},{"location":"user_guide/test_data.html#how-to-update-test-data","title":"How to Update Test Data","text":"<p>We keep master of test data in google spreadsheet because we implemented statistical procedure in the sheet itself to pre-calculate experiment evaluations we then use in unit test asserts.</p>"}]}